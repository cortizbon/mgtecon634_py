
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>2. Introduction to Machine Learning &#8212; MGTECON 634 at Stanford (Python scripts)</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3. ATE I: Binary treatment" href="3_average_treatment_effect_1.html" />
    <link rel="prev" title="1. Introduction" href="1_introduction_1.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">MGTECON 634 at Stanford (Python scripts)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../md/intro.html">
                    Machine Learning-Based Causal Inference
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Topics
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1_introduction_1.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   2. Introduction to Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3_average_treatment_effect_1.html">
   3. ATE I: Binary treatment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4_heterogeneous_treatment_effect_1.html">
   4. HTE I: Binary treatment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5_policy_evaluation_1.html">
   5. Policy Evaluation I - Binary Treatment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="6_policy_learning_1.html">
   6. Policy Learning I - Binary Treatment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="8_WGANs_for_simulation.html">
   7. Tutorial to simulate data using WGANs
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/d2cml-ai/mgtecon634_python/master?urlpath=tree/_build/jupyter_execute/notebooks/2_introduction_to_machine_learning.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/d2cml-ai/mgtecon634_python/blob/master/_build/jupyter_execute/notebooks/2_introduction_to_machine_learning.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
      <li>
        
<button onclick="initThebeSBT()"
  class="headerbtn headerbtn-launch-thebe"
  data-toggle="tooltip"
data-placement="left"
title="Launch Thebe"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="headerbtn__text-container">Live Code</span>
</button>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/d2cml-ai/mgtecon634_python"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/d2cml-ai/mgtecon634_python/issues/new?title=Issue%20on%20page%20%2Fnotebooks/2_introduction_to_machine_learning.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/d2cml-ai/mgtecon634_python/edit/master/_build/jupyter_execute/notebooks/2_introduction_to_machine_learning.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/notebooks/2_introduction_to_machine_learning.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#key-concepts">
   2.1. Key concepts
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#common-machine-learning-algorithms">
   2.2. Common machine learning algorithms
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generalized-linear-models">
     2.2.1. Generalized linear models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decision-tree">
     2.2.2. Decision Tree
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#forest">
     2.2.3. Forest
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-reading">
   2.3. Further reading
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Introduction to Machine Learning</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#key-concepts">
   2.1. Key concepts
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#common-machine-learning-algorithms">
   2.2. Common machine learning algorithms
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generalized-linear-models">
     2.2.1. Generalized linear models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decision-tree">
     2.2.2. Decision Tree
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#forest">
     2.2.3. Forest
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-reading">
   2.3. Further reading
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="introduction-to-machine-learning">
<h1><span class="section-number">2. </span>Introduction to Machine Learning<a class="headerlink" href="#introduction-to-machine-learning" title="Permalink to this headline">#</a></h1>
<p>In this chapter, we’ll briefly review machine learning concepts that will be relevant later. We’ll focus in particular on the problem of <strong>prediction</strong>, that is, to model some output variable as a function of observed input covariates.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># importing the packages</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span> <span class="nn">SyncRNG</span> <span class="kn">import</span> <span class="n">SyncRNG</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ImportError</span><span class="g g-Whitespace">                               </span>Traceback (most recent call last)
<span class="o">~</span>\<span class="n">AppData</span>\<span class="n">Local</span>\<span class="n">Temp</span><span class="o">/</span><span class="n">ipykernel_19520</span><span class="o">/</span><span class="mf">4231609786.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="ne">----&gt; </span><span class="mi">4</span> <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="kn">import</span> <span class="nn">random</span>

<span class="o">~</span>\<span class="n">anaconda3</span>\<span class="n">lib</span>\<span class="n">site</span><span class="o">-</span><span class="n">packages</span>\<span class="n">matplotlib</span>\<span class="fm">__init__</span><span class="o">.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">    </span><span class="mi">105</span> <span class="c1"># cbook must import matplotlib only within function</span>
<span class="g g-Whitespace">    </span><span class="mi">106</span> <span class="c1"># definitions, so it is safe to import from it here.</span>
<span class="ne">--&gt; </span><span class="mi">107</span> <span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">_api</span><span class="p">,</span> <span class="n">cbook</span><span class="p">,</span> <span class="n">docstring</span><span class="p">,</span> <span class="n">rcsetup</span>
<span class="g g-Whitespace">    </span><span class="mi">108</span> <span class="kn">from</span> <span class="nn">matplotlib.cbook</span> <span class="kn">import</span> <span class="n">MatplotlibDeprecationWarning</span><span class="p">,</span> <span class="n">sanitize_sequence</span>
<span class="g g-Whitespace">    </span><span class="mi">109</span> <span class="kn">from</span> <span class="nn">matplotlib.cbook</span> <span class="kn">import</span> <span class="n">mplDeprecation</span>  <span class="c1"># deprecated</span>

<span class="o">~</span>\<span class="n">anaconda3</span>\<span class="n">lib</span>\<span class="n">site</span><span class="o">-</span><span class="n">packages</span>\<span class="n">matplotlib</span>\<span class="n">rcsetup</span><span class="o">.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">     </span><span class="mi">22</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="g g-Whitespace">     </span><span class="mi">23</span> 
<span class="ne">---&gt; </span><span class="mi">24</span> <span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">_api</span><span class="p">,</span> <span class="n">animation</span><span class="p">,</span> <span class="n">cbook</span>
<span class="g g-Whitespace">     </span><span class="mi">25</span> <span class="kn">from</span> <span class="nn">matplotlib.cbook</span> <span class="kn">import</span> <span class="n">ls_mapper</span>
<span class="g g-Whitespace">     </span><span class="mi">26</span> <span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">Colormap</span><span class="p">,</span> <span class="n">is_color_like</span>

<span class="ne">ImportError</span>: cannot import name &#39;animation&#39; from partially initialized module &#39;matplotlib&#39; (most likely due to a circular import) (C:\Users\sandr\anaconda3\lib\site-packages\matplotlib\__init__.py)
</pre></div>
</div>
</div>
</div>
<p>In this section, we will use simulated data. In the next section we’ll load a real dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Simulating data</span>

<span class="c1"># Sample size</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">500</span>

<span class="c1"># Generating covariate X ~ Unif[-4, 4]</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="c1">#with linspace we can generate a vector of &quot;n&quot; numbers between a range of numbers</span>

<span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> 
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">),</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">+</span> <span class="mi">1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span> <span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># collecting observations in a data.frame object</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>The following shows how the two variables <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> relate. Note that the relationship is nonlinear.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Data&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">mu</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;Ground truth E[Y|X=x]&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Outcome y&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Outcome y&#39;)
</pre></div>
</div>
<img alt="../_images/2_introduction_to_machine_learning_6_1.png" src="../_images/2_introduction_to_machine_learning_6_1.png" />
</div>
</div>
<p>Note: If you’d like to run the code below on a different dataset, you can replace the dataset above with another <code class="docutils literal notranslate"><span class="pre">data.frame</span></code> of your choice, and redefine the key variable identifiers (<code class="docutils literal notranslate"><span class="pre">outcome</span></code>, <code class="docutils literal notranslate"><span class="pre">covariates</span></code>) accordingly. Although we try to make the code as general as possible, you may also need to make a few minor changes to the code below; read the comments carefully.</p>
<section id="key-concepts">
<h2><span class="section-number">2.1. </span>Key concepts<a class="headerlink" href="#key-concepts" title="Permalink to this headline">#</a></h2>
<p>The prediction problem is to accurately guess the value of some output variable <span class="math notranslate nohighlight">\(Y_i\)</span> from input variables <span class="math notranslate nohighlight">\(X_i\)</span>. For example, we might want to predict “house prices given house characteristics such as the number of rooms, age of the building, and so on. The relationship between input and output is modeled in very general terms by some function</p>
<div class="math notranslate nohighlight" id="equation-true-model">
<span class="eqno">(2.1)<a class="headerlink" href="#equation-true-model" title="Permalink to this equation">#</a></span>\[
  Y_i = f(X_i) + \epsilon_i
\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon_i\)</span> represents all that is not captured by information obtained from <span class="math notranslate nohighlight">\(X_i\)</span> via the mapping <span class="math notranslate nohighlight">\(f\)</span>. We say that error <span class="math notranslate nohighlight">\(\epsilon_i\)</span> is irreducible.</p>
<p>We highlight that <a class="reference internal" href="#equation-true-model">(2.1)</a> is <strong>not modeling a causal relationship</strong> between inputs and outputs. For an extreme example, consider taking <span class="math notranslate nohighlight">\(Y_i\)</span> to be “distance from the equator” and <span class="math notranslate nohighlight">\(X_i\)</span> to be “average temperature.” We can still think of the problem of guessing (“predicting”) “distance from the equator” given some information about “average temperature,” even though one would expect the former to cause the latter.</p>
<p>In general, we can’t know the “ground truth”  <span class="math notranslate nohighlight">\(f\)</span>, so we will approximate it from data. Given <span class="math notranslate nohighlight">\(n\)</span> data points <span class="math notranslate nohighlight">\(\{(X_1, Y_1), \cdots, (X_n, Y_n)\}\)</span>, our goal is to obtain an estimated model  <span class="math notranslate nohighlight">\(\hat{f}\)</span> such that our predictions <span class="math notranslate nohighlight">\(\widehat{Y}_i := \hat{f}(X_i)\)</span> are “close” to the true outcome values <span class="math notranslate nohighlight">\(Y_i\)</span> given some criterion. To formalize this, we’ll follow these three steps:</p>
<ul class="simple">
<li><p><strong>Modeling:</strong> Decide on some suitable class of functions that our estimated model may belong to. In machine learning applications the class of functions can be very large and complex (e.g., deep decision trees, forests, high-dimensional linear models, etc). Also, we must decide on a loss function that serves as our criterion to evaluate the quality of our predictions (e.g., mean-squared error).</p></li>
<li><p><strong>Fitting:</strong> Find the estimate <span class="math notranslate nohighlight">\(\hat{f}\)</span> that optimizes the loss function chosen in the previous step (e.g., the tree that minimizes the squared deviation between <span class="math notranslate nohighlight">\(\hat{f}(X_i)\)</span> and <span class="math notranslate nohighlight">\(Y_i\)</span> in our data).</p></li>
<li><p><strong>Evaluation:</strong> Evaluate our fitted model <span class="math notranslate nohighlight">\(\hat{f}\)</span>. That is, if we were given a new, yet unseen, input and output pair <span class="math notranslate nohighlight">\((X',Y')\)</span>, we’d like to know if <span class="math notranslate nohighlight">\(Y' \approx \hat{f}(X_i)\)</span> by some metric.</p></li>
</ul>
<p>For concreteness, let’s work through an example. Let’s say that, given the data simulated above, we’d like to predict <span class="math notranslate nohighlight">\(Y_i\)</span> from the first covariate  <span class="math notranslate nohighlight">\(X_{i1}\)</span> only. Also, let’s say that our model class will be polynomials of degree <span class="math notranslate nohighlight">\(q\)</span> in <span class="math notranslate nohighlight">\(X_{i1}\)</span>, and we’ll evaluate fit based on mean squared error. That is, <span class="math notranslate nohighlight">\(\hat{f}(X_{i1}) = \hat{b}_0 + X_{i1}\hat{b}_1 + \cdots + X_{i1}^q \hat{b}_q\)</span>, where the coefficients are obtained by solving the following problem:</p>
<div class="math notranslate nohighlight">
\[
  \hat{b} = \arg\min_b \sum_{i=1}^m
    \left(Y_i - b_0 - X_{i1}b_1 - \cdots - X_{iq}^q b_q \right)^2
\]</div>
<p>An important question is what is <span class="math notranslate nohighlight">\(q\)</span>, the degree of the polynomial. It controls the complexity of the model. One may imagine that more complex models are better, but that is not always true, because a very flexible model may try to simply interpolate over the data at hand, but fail to generalize well for new data points. We call this <strong>overfitting</strong>. The main feature of overfitting is <strong>high variance</strong>, in the sense that, if we were given a different data set of the same size, we’d likely get a very different model.</p>
<p>To illustrate, in the figure below we let the degree be  <span class="math notranslate nohighlight">\(q=10\)</span> but use only the first few data points. The fitted model is shown in green, and the original data points are in red.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="s1">&#39;x&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Note: this code assumes that the first covariate is continuous.</span>
<span class="c1"># Fitting a flexible model on very little data</span>

<span class="c1"># selecting only a few data points</span>
<span class="n">subset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">30</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>


<span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">X_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">poly</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_poly</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">lin2</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lin2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_poly</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">30</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">30</span><span class="p">])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span>
<span class="n">xgrid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">1000</span><span class="p">)</span>

<span class="n">new_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">xgrid</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>

<span class="n">yhat</span> <span class="o">=</span> <span class="n">lin2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">new_data</span><span class="p">))</span>

<span class="c1"># Visualising the Polynomial Regression results</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">subset</span><span class="p">,</span><span class="s1">&#39;x&#39;</span><span class="p">],</span><span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">subset</span><span class="p">,</span><span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xgrid</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Estimate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Example of overfitting&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Outcome y&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Outcome y&#39;)
</pre></div>
</div>
<img alt="../_images/2_introduction_to_machine_learning_10_1.png" src="../_images/2_introduction_to_machine_learning_10_1.png" />
</div>
</div>
<p>On the other hand, when <span class="math notranslate nohighlight">\(q\)</span> is too small relative to our data, we permit only very simple models and may suffer from misspecification bias. We call this <strong>underfitting</strong>. The main feature of underfitting is <strong>high bias</strong> – the selected model just isn’t complex enough to accurately capture the relationship between input and output variables.</p>
<p>To illustrate underfitting, in the figure below we set <span class="math notranslate nohighlight">\(q=1\)</span> (a linear fit).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lin</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="n">lin</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">30</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">30</span><span class="p">])</span>


<span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span>
<span class="n">xgrid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">1000</span><span class="p">)</span>

<span class="n">new_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">xgrid</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>

<span class="n">yhat</span> <span class="o">=</span> <span class="n">lin</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">new_data</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">subset</span><span class="p">,</span><span class="s1">&#39;x&#39;</span><span class="p">],</span><span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">subset</span><span class="p">,</span><span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xgrid</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Estimate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Example of underfitting&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Outcome y&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Outcome y&#39;)
</pre></div>
</div>
<img alt="../_images/2_introduction_to_machine_learning_12_1.png" src="../_images/2_introduction_to_machine_learning_12_1.png" />
</div>
</div>
<p>This tension is called the <strong>bias-variance trade-off</strong>: simpler models underfit and have more bias, more complex models overfit and have more variance.</p>
<p>One data-driven way of deciding an appropriate level of complexity is to divide the available data into a training set (where the model is fit) and the validation set (where the model is evaluated). The next snippet of code uses the first half of the data to fit a polynomial of order <span class="math notranslate nohighlight">\(q\)</span>, and then evaluates that polynomial on the second half. The training MSE estimate decreases monotonically with the polynomial degree, because the model is better able to fit on the training data; the test MSE estimate starts increasing after a while reflecting that the model no longer generalizes well.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">degrees</span> <span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">21</span><span class="p">)</span>
<span class="n">train_mse</span> <span class="o">=</span><span class="p">[]</span>
<span class="n">test_mse</span> <span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">degrees</span><span class="p">:</span>
    <span class="n">poly</span> <span class="o">=</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span> <span class="o">=</span> <span class="n">d</span><span class="p">,</span> <span class="n">include_bias</span> <span class="o">=</span><span class="kc">False</span>  <span class="p">)</span>
    <span class="n">poly_features</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">poly_features</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.5</span> <span class="p">,</span> <span class="n">random_state</span><span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Now since we want the valid and test size to be equal (10% each of overall data). </span>
<span class="c1"># we have to define valid_size=0.5 (that is 50% of remaining data)</span>

    <span class="n">poly_reg_model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
    <span class="n">poly_reg_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    
    
    <span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">poly_reg_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">poly_reg_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

    <span class="n">mse_train</span><span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">)</span>
    <span class="n">mse_test</span><span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">)</span>
    
    <span class="n">train_mse</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mse_train</span><span class="p">)</span>
    <span class="n">test_mse</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mse_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">degrees</span><span class="p">,</span> <span class="n">train_mse</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;Training&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">degrees</span><span class="p">,</span> <span class="n">test_mse</span><span class="p">,</span><span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;Validation&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;MSE Estimates (train test split)&quot;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span> <span class="o">=</span> <span class="s2">&quot;Polynomial degree&quot;</span><span class="p">,</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="s2">&quot;MSE estimate&quot;</span><span class="p">)</span>
    
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">&quot;Low bias </span><span class="se">\n</span><span class="s2"> High Variance&quot;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mf">1.23</span><span class="p">),</span> <span class="n">xycoords</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mf">1.23</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">,</span>
            <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">&quot;-&gt;&quot;</span><span class="p">,</span><span class="n">connectionstyle</span><span class="o">=</span><span class="s2">&quot;arc3&quot;</span><span class="p">),)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">&quot;High bias </span><span class="se">\n</span><span class="s2"> Low Variance&quot;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">5.3</span><span class="p">,</span> <span class="mf">1.30</span><span class="p">),</span> <span class="n">xycoords</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mf">1.30</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">,</span>
            <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">&quot;-&gt;&quot;</span><span class="p">,</span><span class="n">connectionstyle</span><span class="o">=</span><span class="s2">&quot;arc3&quot;</span><span class="p">),)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(7, 1.3, &#39;High bias \n Low Variance&#39;)
</pre></div>
</div>
<img alt="../_images/2_introduction_to_machine_learning_15_1.png" src="../_images/2_introduction_to_machine_learning_15_1.png" />
</div>
</div>
<p>To make better use of the data we will often divide the data into <span class="math notranslate nohighlight">\(K\)</span> subsets, or <em>folds</em>. Then one fits <span class="math notranslate nohighlight">\(K\)</span> models, each using  <span class="math notranslate nohighlight">\(K-1\)</span> folds and then evaluation the fitted model on the remaining fold. This is called <strong>k-fold cross-validation</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">make_scorer</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>
<span class="c1">#cv = KFold(n_splits=10, random_state=1, shuffle=True)</span>
<span class="n">scorer</span> <span class="o">=</span> <span class="n">make_scorer</span>
<span class="n">mse</span> <span class="o">=</span><span class="p">[]</span>

<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">degrees</span><span class="p">:</span> 
    
    <span class="n">poly</span> <span class="o">=</span><span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span> <span class="o">=</span> <span class="n">d</span><span class="p">,</span> <span class="n">include_bias</span> <span class="o">=</span><span class="kc">False</span>  <span class="p">)</span>
    <span class="n">poly_features</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">ols</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
    <span class="n">scorer</span> <span class="o">=</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">)</span>
    <span class="n">mse_test</span><span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">ols</span><span class="p">,</span> <span class="n">poly_features</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scorer</span><span class="p">,</span> <span class="n">cv</span> <span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">mse</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mse_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">degrees</span><span class="p">,</span> <span class="n">mse</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Polynomial degree&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">21</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;MSE estimate&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;MSE estimate (K-fold cross validation)&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="c1">#different to r, the models in python got a better performance with more training cause by the</span>
<span class="c1">#cross validation and the kfold</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;MSE estimate (K-fold cross validation)&#39;)
</pre></div>
</div>
<img alt="../_images/2_introduction_to_machine_learning_18_1.png" src="../_images/2_introduction_to_machine_learning_18_1.png" />
</div>
</div>
<p>A final remark is that, in machine learning applications, the complexity of the model often is allowed to increase with the available data. In the example above, even though we weren’t very successful when fitting a high-dimensional model on very little data, if we had much more data perhaps such a model would be appropriate. The next figure again fits a high order polynomial model, but this time on many data points. Note how, at least in data-rich regions, the model is much better behaved, and tracks the average outcome reasonably well without trying to interpolate wildly of the data points.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="s1">&#39;x&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>


<span class="n">subset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">500</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>


<span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span> <span class="o">=</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">X_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">poly</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_poly</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">lin2</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lin2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_poly</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">500</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">500</span><span class="p">])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span>
<span class="n">xgrid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">1000</span><span class="p">)</span>

<span class="n">new_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">xgrid</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>

<span class="n">yhat</span> <span class="o">=</span> <span class="n">lin2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">new_data</span><span class="p">))</span>

<span class="c1"># Visualising the Polynomial Regression results</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">subset</span><span class="p">,</span><span class="s1">&#39;x&#39;</span><span class="p">],</span><span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">subset</span><span class="p">,</span><span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xgrid</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Estimate&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">mu</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;Ground truth&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Outcome&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Outcome&#39;)
</pre></div>
</div>
<img alt="../_images/2_introduction_to_machine_learning_20_1.png" src="../_images/2_introduction_to_machine_learning_20_1.png" />
</div>
</div>
<p>This is one of the benefits of using machine learning-based models: more data implies more flexible modeling, and therefore potentially better predictive power – provided that we carefully avoid overfitting.</p>
<p>The example above based on polynomial regression was used mostly for illustration. In practice, there are often better-performing algorithms. We’ll see some of them next.</p>
</section>
<section id="common-machine-learning-algorithms">
<h2><span class="section-number">2.2. </span>Common machine learning algorithms<a class="headerlink" href="#common-machine-learning-algorithms" title="Permalink to this headline">#</a></h2>
<p>Next, we’ll introduce three machine learning algorithms: (regularized) linear models, trees, and forests. Although this isn’t an exhaustive list, these algorithms are common enough that every machine learning practitioner should know about them. They also have convenient <code class="docutils literal notranslate"><span class="pre">R</span></code> packages that allow for easy coding.</p>
<p>In this tutorial, we’ll focus heavily on how to <strong>interpret</strong> the output of machine learning models – or, at least, how not to <em>mis</em>-interpret it. However, in this chapter we won’t be making any causal claims about the relationships between variables yet. But please hang tight, as estimating causal effects will be one of the main topics presented in the next chapters.</p>
<p>For the remainder of the chapter we will use a real dataset. Each row in this data set represents the characteristics of a owner-occupied housing unit. Our goal is to predict the (log) price of the housing unit (<code class="docutils literal notranslate"><span class="pre">LOGVALUE</span></code>, our outcome variable) from features such as the size of the lot (<code class="docutils literal notranslate"><span class="pre">LOT</span></code>) and square feet area (<code class="docutils literal notranslate"><span class="pre">UNITSF</span></code>), number of bedrooms (<code class="docutils literal notranslate"><span class="pre">BEDRMS</span></code>) and bathrooms (<code class="docutils literal notranslate"><span class="pre">BATHS</span></code>), year in which it was built (<code class="docutils literal notranslate"><span class="pre">BUILT</span></code>) etc. This dataset comes from the American Housing Survey and was used in <a class="reference external" href="https://www.aeaweb.org/articles?id=10.1257/jep.31.2.87">Mullainathan and Spiess (2017, JEP)</a>. In addition, we will append to this data columns that are pure noise. Ideally, our fitted model should not take them into acccount.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">io</span>

<span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;https://docs.google.com/uc?id=1qHr-6nN7pCbU8JUtbRDtMzUKqS9ZlZcR&amp;export=download&#39;</span>
<span class="n">urlData</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span><span class="o">.</span><span class="n">content</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">io</span><span class="o">.</span><span class="n">StringIO</span><span class="p">(</span><span class="n">urlData</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)))</span>
<span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;Unnamed: 0&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># outcome variable name</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="s1">&#39;LOGVALUE&#39;</span>

<span class="c1"># covariates</span>
<span class="n">true_covariates</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;LOT&#39;</span><span class="p">,</span><span class="s1">&#39;UNITSF&#39;</span><span class="p">,</span><span class="s1">&#39;BUILT&#39;</span><span class="p">,</span><span class="s1">&#39;BATHS&#39;</span><span class="p">,</span><span class="s1">&#39;BEDRMS&#39;</span><span class="p">,</span><span class="s1">&#39;DINING&#39;</span><span class="p">,</span><span class="s1">&#39;METRO&#39;</span><span class="p">,</span><span class="s1">&#39;CRACKS&#39;</span><span class="p">,</span><span class="s1">&#39;REGION&#39;</span><span class="p">,</span><span class="s1">&#39;METRO3&#39;</span><span class="p">,</span><span class="s1">&#39;PHONE&#39;</span><span class="p">,</span><span class="s1">&#39;KITCHEN&#39;</span><span class="p">,</span><span class="s1">&#39;MOBILTYP&#39;</span><span class="p">,</span><span class="s1">&#39;WINTEROVEN&#39;</span><span class="p">,</span><span class="s1">&#39;WINTERKESP&#39;</span><span class="p">,</span><span class="s1">&#39;WINTERELSP&#39;</span><span class="p">,</span><span class="s1">&#39;WINTERWOOD&#39;</span><span class="p">,</span><span class="s1">&#39;WINTERNONE&#39;</span><span class="p">,</span><span class="s1">&#39;NEWC&#39;</span><span class="p">,</span><span class="s1">&#39;DISH&#39;</span><span class="p">,</span><span class="s1">&#39;WASH&#39;</span><span class="p">,</span><span class="s1">&#39;DRY&#39;</span><span class="p">,</span><span class="s1">&#39;NUNIT2&#39;</span><span class="p">,</span><span class="s1">&#39;BURNER&#39;</span><span class="p">,</span><span class="s1">&#39;COOK&#39;</span><span class="p">,</span><span class="s1">&#39;OVEN&#39;</span><span class="p">,</span><span class="s1">&#39;REFR&#39;</span><span class="p">,</span><span class="s1">&#39;DENS&#39;</span><span class="p">,</span><span class="s1">&#39;FAMRM&#39;</span><span class="p">,</span><span class="s1">&#39;HALFB&#39;</span><span class="p">,</span><span class="s1">&#39;KITCH&#39;</span><span class="p">,</span><span class="s1">&#39;LIVING&#39;</span><span class="p">,</span><span class="s1">&#39;OTHFN&#39;</span><span class="p">,</span><span class="s1">&#39;RECRM&#39;</span><span class="p">,</span><span class="s1">&#39;CLIMB&#39;</span><span class="p">,</span><span class="s1">&#39;ELEV&#39;</span><span class="p">,</span><span class="s1">&#39;DIRAC&#39;</span><span class="p">,</span><span class="s1">&#39;PORCH&#39;</span><span class="p">,</span><span class="s1">&#39;AIRSYS&#39;</span><span class="p">,</span><span class="s1">&#39;WELL&#39;</span><span class="p">,</span><span class="s1">&#39;WELDUS&#39;</span><span class="p">,</span><span class="s1">&#39;STEAM&#39;</span><span class="p">,</span><span class="s1">&#39;OARSYS&#39;</span><span class="p">]</span>
<span class="n">p_true</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">true_covariates</span><span class="p">)</span>

<span class="c1"># noise covariates added for didactic reasons</span>

<span class="n">p_noise</span> <span class="o">=</span> <span class="mi">20</span>

<span class="n">noise_covariates</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p_noise</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">noise_covariates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;noise</span><span class="si">{0}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">covariates</span> <span class="o">=</span> <span class="n">true_covariates</span> <span class="o">+</span> <span class="n">noise_covariates</span>

<span class="n">x_noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">p_noise</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28727</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span>
<span class="n">x_noise</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">x_noise</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">noise_covariates</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">data</span><span class="p">,</span> <span class="n">x_noise</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># sample size</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># total number of covariates</span>
<span class="n">p</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">covariates</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here’s the correlation between the first few covariates. Note how, most variables are positively correlated, which is expected since houses with more bedrooms will usually also have more bathrooms, larger area, etc.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">covariates</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">8</span><span class="p">]]</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>LOT</th>
      <th>UNITSF</th>
      <th>BUILT</th>
      <th>BATHS</th>
      <th>BEDRMS</th>
      <th>DINING</th>
      <th>METRO</th>
      <th>CRACKS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>LOT</th>
      <td>1.000000</td>
      <td>0.064841</td>
      <td>0.044639</td>
      <td>0.057325</td>
      <td>0.009626</td>
      <td>-0.015348</td>
      <td>0.136258</td>
      <td>0.016851</td>
    </tr>
    <tr>
      <th>UNITSF</th>
      <td>0.064841</td>
      <td>1.000000</td>
      <td>0.143201</td>
      <td>0.428723</td>
      <td>0.361165</td>
      <td>0.214030</td>
      <td>0.057441</td>
      <td>0.033548</td>
    </tr>
    <tr>
      <th>BUILT</th>
      <td>0.044639</td>
      <td>0.143201</td>
      <td>1.000000</td>
      <td>0.434519</td>
      <td>0.215109</td>
      <td>0.037468</td>
      <td>0.323703</td>
      <td>0.092390</td>
    </tr>
    <tr>
      <th>BATHS</th>
      <td>0.057325</td>
      <td>0.428723</td>
      <td>0.434519</td>
      <td>1.000000</td>
      <td>0.540230</td>
      <td>0.259457</td>
      <td>0.189812</td>
      <td>0.062819</td>
    </tr>
    <tr>
      <th>BEDRMS</th>
      <td>0.009626</td>
      <td>0.361165</td>
      <td>0.215109</td>
      <td>0.540230</td>
      <td>1.000000</td>
      <td>0.281846</td>
      <td>0.121331</td>
      <td>0.026779</td>
    </tr>
    <tr>
      <th>DINING</th>
      <td>-0.015348</td>
      <td>0.214030</td>
      <td>0.037468</td>
      <td>0.259457</td>
      <td>0.281846</td>
      <td>1.000000</td>
      <td>0.022026</td>
      <td>0.021270</td>
    </tr>
    <tr>
      <th>METRO</th>
      <td>0.136258</td>
      <td>0.057441</td>
      <td>0.323703</td>
      <td>0.189812</td>
      <td>0.121331</td>
      <td>0.022026</td>
      <td>1.000000</td>
      <td>0.057545</td>
    </tr>
    <tr>
      <th>CRACKS</th>
      <td>0.016851</td>
      <td>0.033548</td>
      <td>0.092390</td>
      <td>0.062819</td>
      <td>0.026779</td>
      <td>0.021270</td>
      <td>0.057545</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<section id="generalized-linear-models">
<h3><span class="section-number">2.2.1. </span>Generalized linear models<a class="headerlink" href="#generalized-linear-models" title="Permalink to this headline">#</a></h3>
<p>This class of models extends common methods such as linear and logistic regression by adding a penalty to the magnitude of the coefficients. <strong>Lasso</strong> penalizes the absolute value of slope coefficients. For regression problems, it becomes</p>
<div class="math notranslate nohighlight" id="equation-lasso">
<span class="eqno">(2.2)<a class="headerlink" href="#equation-lasso" title="Permalink to this equation">#</a></span>\[
  \hat{b}_{Lasso} = \arg\min_b \sum_{i=1}^m
    \left( Y_i - b_0 - X_{i1}b_1 - \cdots - X_{ip}b_p \right)^2
    - \lambda \sum_{j=1}^p |b_j|
\]</div>
<p>Similarly, in a regression problem <strong>Ridge</strong> penalizes the sum of squares of the slope coefficients,</p>
<div class="math notranslate nohighlight" id="equation-ridge">
<span class="eqno">(2.3)<a class="headerlink" href="#equation-ridge" title="Permalink to this equation">#</a></span>\[
  \hat{b}_{Ridge} = \arg\min_b \sum_{i=1}^m
    \left( Y_i - b_0 - X_{i1}b_1 - \cdots - X_{ip}b_p \right)^2
    - \lambda \sum_{j=1}^p b_j^2
\]</div>
<p>Also, there exists the <strong>Elastic Net</strong> penalization which consists of a convex combination between the other two. In all cases, the scalar parameter
<span class="math notranslate nohighlight">\(\lambda\)</span> controls the complexity of the model. For <span class="math notranslate nohighlight">\(\lambda=0\)</span>, the problem reduces to the “usual” linear regression. As <span class="math notranslate nohighlight">\(\lambda\)</span> increases, we favor simpler models. As we’ll see below, the optimal parameter <span class="math notranslate nohighlight">\(\lambda\)</span> is selected via cross-validation.</p>
<p>An important feature of Lasso-type penalization is that it promotes <strong>sparsity</strong> – that is, it forces many coefficients to be exactly zero. This is different from Ridge-type penalization, which forces coefficients to be small.</p>
<p>Another interesting property of these models is that, even though they are called “linear” models, this should actually be understood as <strong>linear in transformations</strong> of the covariates. For example, we could use polynomials or splines (continuous piecewise polynomials) of the covariates and allow for much more flexible models.</p>
<p>In fact, because of the penalization term, problems <a class="reference internal" href="#equation-lasso">(2.2)</a> and <a class="reference internal" href="#equation-ridge">(2.3)</a> remain well-defined and have a unique solution even in <strong>high-dimensional</strong> problems in which the number of coefficients <span class="math notranslate nohighlight">\(p\)</span> is larger than the sample size <span class="math notranslate nohighlight">\(n\)</span> – that is, our data is “fat” with more columns than rows. These situations can arise either naturally (e.g. genomics problems in which we have hundreds of thousands of gene expression information for a few individuals) or because we are including many transformations of a smaller set of covariates.</p>
<p>Finally, although here we are focusing on regression problems, other generalized linear models such as logistic regression can also be similarly modified by adding a Lasso, Ridge, or Elastic Net-type penalty to similar consequences.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">covariates</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">outcome</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>

<span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">()</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="mf">1e-8</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="mf">1e-1</span><span class="p">),</span> <span class="mi">100</span><span class="p">)</span>

<span class="n">tuned_parameters</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="n">alphas</span><span class="p">}]</span>
<span class="n">n_folds</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">scorer</span> <span class="o">=</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">)</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">lasso</span><span class="p">,</span> <span class="n">tuned_parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">n_folds</span><span class="p">,</span> <span class="n">refit</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scorer</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s2">&quot;mean_test_score&quot;</span><span class="p">]</span>
<span class="n">scores_std</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s2">&quot;std_test_score&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>The next figure plots the average estimated MSE for each lambda. The red dots are the averages across all folds, and the error bars are based on the variability of mse estimates across folds. The vertical dashed lines show the (log) lambda with smallest estimated MSE (left) and the one whose mse is at most one standard error from the first (right).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_lasso</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span> <span class="s2">&quot;alphas&quot;</span><span class="p">),</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;scores&quot;</span><span class="p">)])</span><span class="o">.</span><span class="n">T</span>
<span class="n">best</span> <span class="o">=</span> <span class="n">data_lasso</span><span class="p">[</span><span class="n">data_lasso</span><span class="p">[</span><span class="s2">&quot;scores&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">data_lasso</span><span class="p">[</span><span class="s2">&quot;scores&quot;</span><span class="p">])]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">semilogx</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;red&quot;</span><span class="p">)</span>

<span class="c1"># plot error lines showing +/- std. errors of the scores</span>
<span class="n">std_error</span> <span class="o">=</span> <span class="n">scores_std</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_folds</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">semilogx</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">std_error</span><span class="p">,</span> <span class="s2">&quot;b--&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">semilogx</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">scores</span> <span class="o">-</span> <span class="n">std_error</span><span class="p">,</span> <span class="s2">&quot;b--&quot;</span><span class="p">)</span>

<span class="c1"># alpha=0.2 controls the translucency of the fill color</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">std_error</span><span class="p">,</span> <span class="n">scores</span> <span class="o">-</span> <span class="n">std_error</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;CV score +/- std error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">best</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;.5&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">alphas</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">alphas</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1e-08, 0.1)
</pre></div>
</div>
<img alt="../_images/2_introduction_to_machine_learning_33_1.png" src="../_images/2_introduction_to_machine_learning_33_1.png" />
</div>
</div>
<p>Here are the first few estimated coefficients at the <span class="math notranslate nohighlight">\(\lambda\)</span> value that minimizes cross-validated MSE. Note that many estimated coefficients them are exactly zero.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">best</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
<span class="n">table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">table</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">lasso</span><span class="o">.</span><span class="n">intercept_</span>
<span class="n">table</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">table</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">table</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">table</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;(Intercept)&#39;</span><span class="p">,</span><span class="s1">&#39;LOT&#39;</span><span class="p">,</span><span class="s1">&#39;UNITSF&#39;</span><span class="p">,</span><span class="s1">&#39;BUILT&#39;</span><span class="p">,</span><span class="s1">&#39;BATHS&#39;</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Coef.&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>(Intercept)</th>
      <th>LOT</th>
      <th>UNITSF</th>
      <th>BUILT</th>
      <th>BATHS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Coef.</th>
      <td>11.643421</td>
      <td>3.494443e-07</td>
      <td>0.000023</td>
      <td>0.000229</td>
      <td>0.246402</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of nonzero coefficients at optimal lambda:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">]),</span> <span class="s2">&quot;out of &quot;</span> <span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of nonzero coefficients at optimal lambda: 46 out of  63
</pre></div>
</div>
</div>
</div>
<p>Predictions and estimated MSE for the selected model are retrieved as follows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Retrieve predictions at best lambda regularization parameter</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">lasso</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Get k-fold cross validation</span>
<span class="n">mse_lasso</span>  <span class="o">=</span> <span class="n">best</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;glmnet MSE estimate (k-fold cross-validation):&quot;</span><span class="p">,</span> <span class="n">mse_lasso</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>glmnet MSE estimate (k-fold cross-validation): 0.6156670911339063
</pre></div>
</div>
</div>
</div>
<p>The next command plots estimated coefficients as a function of the regularization parameter <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coefs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
    <span class="n">lasso</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">a</span><span class="p">)</span>
    <span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
    <span class="n">coefs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">import</span> <span class="n">figure</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">coefs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Standardized Coefficients&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Lasso coefficients as a function of alpha&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_introduction_to_machine_learning_41_0.png" src="../_images/2_introduction_to_machine_learning_41_0.png" />
</div>
</div>
<p>It’s tempting to try to interpret the coefficients obtained via Lasso. Unfortunately, that can be very difficult, because by dropping covariates Lasso introduces a form of <strong>omitted variable bias</strong> (<a class="reference external" href="https://en.wikipedia.org/wiki/Omitted-variable_bias">wikipedia</a>). To understand this form of bias, consider the following toy example. We have two positively correlated independent variables, <code class="docutils literal notranslate"><span class="pre">x.1</span></code> and <code class="docutils literal notranslate"><span class="pre">x.2</span></code>, that are linearly related to the outcome <code class="docutils literal notranslate"><span class="pre">y</span></code>. Linear regression of <code class="docutils literal notranslate"><span class="pre">y</span></code> on <code class="docutils literal notranslate"><span class="pre">x1</span></code> and <code class="docutils literal notranslate"><span class="pre">x2</span></code> gives us the correct coefficients. However, if we <em>omit</em> <code class="docutils literal notranslate"><span class="pre">x2</span></code> from the estimation model, the coefficient on <code class="docutils literal notranslate"><span class="pre">x1</span></code> increases. This is because <code class="docutils literal notranslate"><span class="pre">x1</span></code> is now “picking up” the effect of the variable that was left out. In other words, the effect of <code class="docutils literal notranslate"><span class="pre">x1</span></code> seems stronger because we aren’t controlling for some other confounding variable. Note that the second model this still works for prediction, but we cannot interpret the coefficient as a measure of strength of the causal relationship between <code class="docutils literal notranslate"><span class="pre">x1</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">]</span>
<span class="n">cov</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">1.5</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mf">1.5</span><span class="p">]]</span>

<span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="mi">100000</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x1</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100000</span><span class="p">)</span>
<span class="n">data_sim</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">y</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span><span class="s1">&#39;x2&#39;</span><span class="p">,</span><span class="s1">&#39;y&#39;</span><span class="p">]</span> <span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Correct Model&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Correct Model
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;y ~ x1 + x2&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">data_sim</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.997
Model:                            OLS   Adj. R-squared:                  0.997
Method:                 Least Squares   F-statistic:                 1.897e+07
Date:                Wed, 22 Jun 2022   Prob (F-statistic):               0.00
Time:                        20:59:12   Log-Likelihood:                -17706.
No. Observations:              100000   AIC:                         3.542e+04
Df Residuals:                   99997   BIC:                         3.545e+04
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      1.5012      0.001   1643.500      0.000       1.499       1.503
x1             1.9998      0.001   1996.643      0.000       1.998       2.002
x2             3.0011      0.001   3002.007      0.000       2.999       3.003
==============================================================================
Omnibus:                    90005.976   Durbin-Watson:                   2.010
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             6016.746
Skew:                          -0.006   Prob(JB):                         0.00
Kurtosis:                       1.798   Cond. No.                         2.24
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model with omitted variable bias&quot;</span><span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;y ~ x1&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">data_sim</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model with omitted variable bias
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.760
Model:                            OLS   Adj. R-squared:                  0.760
Method:                 Least Squares   F-statistic:                 3.174e+05
Date:                Wed, 22 Jun 2022   Prob (F-statistic):               0.00
Time:                        20:59:21   Log-Likelihood:            -2.4332e+05
No. Observations:              100000   AIC:                         4.866e+05
Df Residuals:                   99998   BIC:                         4.867e+05
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      1.5107      0.009    173.262      0.000       1.494       1.528
x1             4.0084      0.007    563.401      0.000       3.994       4.022
==============================================================================
Omnibus:                        0.159   Durbin-Watson:                   2.003
Prob(Omnibus):                  0.924   Jarque-Bera (JB):                0.158
Skew:                          -0.003   Prob(JB):                        0.924
Kurtosis:                       3.001   Cond. No.                         1.23
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<p>The phenomenon above occurs in Lasso and in any other sparsity-promoting method when correlated covariates are present since, by forcing coefficients to be zero, Lasso is effectively dropping them from the model. And as we have seen, as a variable gets dropped, a different variable that is correlated with it can “pick up” its effect, which in turn can cause bias. Once <span class="math notranslate nohighlight">\(\lambda\)</span> grows sufficiently large, the penalization term overwhelms any benefit of having that variable in the model, so that variable finally decreases to zero too.</p>
<p>One may instead consider using Lasso to select a subset of variables, and then regressing the outcome on the subset of selected variables via OLS (without any penalization). This method is often called <strong>post-lasso</strong>. Although it has desirable properties in terms of model fit (see e.g., <a class="reference external" href="https://arxiv.org/pdf/1001.0188.pdf">Belloni and Chernozhukov, 2013</a>), this procedure does not solve the omitted variable issue we mentioned above.</p>
<p>We illustrate this next. We observe the path of the estimated coefficient on the number of bathroooms (<code class="docutils literal notranslate"><span class="pre">BATHS</span></code>) as we increase <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>

<span class="n">scale_X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">ols</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">ols</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scale_X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
<span class="n">ols_coef</span> <span class="o">=</span> <span class="n">ols</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
<span class="n">lamdas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>


<span class="n">coef_ols</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">ols_coef</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="c1">###############################################</span>

<span class="n">lasso_bath_coef</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">lasso_coefs</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">lamdas</span><span class="p">:</span>
    <span class="n">lasso</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">a</span><span class="p">,</span><span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scale_X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
    <span class="n">lasso_bath_coef</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
    <span class="n">lasso_coefs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="c1">#################################################   </span>

<span class="n">ridge_bath_coef</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">lamdas</span><span class="p">:</span>
    <span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">a</span><span class="p">,</span><span class="n">normalize</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scale_X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
    <span class="n">ridge_bath_coef</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="c1">####################################################</span>

<span class="n">poslasso_coef</span> <span class="o">=</span> <span class="p">[</span> <span class="p">]</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">scale_X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">(</span><span class="n">lasso_coefs</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">!=</span>  <span class="mi">0</span><span class="p">)])</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">(</span><span class="n">lasso_coefs</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">!=</span>  <span class="mi">0</span><span class="p">)])</span>
    <span class="n">ols</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
    <span class="n">ols</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scale_X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>  
    <span class="n">post_coef</span> <span class="o">=</span> <span class="n">ols</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">(</span><span class="n">lasso_coefs</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">!=</span>  <span class="mi">0</span><span class="p">)]</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_loc</span><span class="p">(</span><span class="s1">&#39;BATHS&#39;</span><span class="p">)]</span>                             
    <span class="n">poslasso_coef</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">post_coef</span> <span class="p">)</span>    
    
    
<span class="c1">#################################################</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamdas</span><span class="p">,</span> <span class="n">ridge_bath_coef</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Ridge&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;:&#39;</span><span class="p">,</span><span class="n">markevery</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamdas</span><span class="p">,</span> <span class="n">lasso_bath_coef</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Lasso&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;^&#39;</span><span class="p">,</span><span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;dashed&#39;</span><span class="p">,</span><span class="n">markevery</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamdas</span><span class="p">,</span> <span class="n">coef_ols</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;OLS&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span><span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;dashed&#39;</span><span class="p">,</span><span class="n">markevery</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamdas</span><span class="p">,</span> <span class="n">poslasso_coef</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;postlasso&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span><span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span><span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;dashed&#39;</span><span class="p">,</span><span class="n">markevery</span><span class="o">=</span><span class="mi">8</span> <span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Coefficient estimate on Baths&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Coef&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;lambda&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 0, &#39;lambda&#39;)
</pre></div>
</div>
<img alt="../_images/2_introduction_to_machine_learning_47_1.png" src="../_images/2_introduction_to_machine_learning_47_1.png" />
</div>
</div>
<p>The OLS coefficients are not penalized, so they remain constant. Ridge estimates decrease monotonically as <span class="math notranslate nohighlight">\(\lambda\)</span> grows. Also, for this dataset, Lasso estimates first increase and then decrease. Meanwhile, the post-lasso coefficient estimates seem to behave somewhat erratically with <span class="math notranslate nohighlight">\(lambda\)</span>. To understand this behavior, let’s see what happens to the magnitude of other selected variables that are correlated with <code class="docutils literal notranslate"><span class="pre">BATHS</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scale_X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">UNITSF_coef</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">BEDRMS_coef</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">DINING_coef</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">lamdas</span><span class="p">:</span>
    <span class="n">lasso</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">a</span><span class="p">,</span><span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scale_X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
    <span class="n">UNITSF_coef</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">BEDRMS_coef</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
    <span class="n">DINING_coef</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamdas</span><span class="p">,</span> <span class="n">UNITSF_coef</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;UNITSF&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;black&#39;</span> <span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamdas</span><span class="p">,</span> <span class="n">BEDRMS_coef</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;BEDRMS&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span>  <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamdas</span><span class="p">,</span> <span class="n">DINING_coef</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;DINING&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span><span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;dotted&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Coef&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;lambda&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 0, &#39;lambda&#39;)
</pre></div>
</div>
<img alt="../_images/2_introduction_to_machine_learning_50_1.png" src="../_images/2_introduction_to_machine_learning_50_1.png" />
</div>
</div>
<p>Note how the discrete jumps in magnitude for the <code class="docutils literal notranslate"><span class="pre">BATHS</span></code> coefficient in the first coincide with, for example, variables <code class="docutils literal notranslate"><span class="pre">DINING</span></code> and <code class="docutils literal notranslate"><span class="pre">BEDRMS</span></code> being exactly zero. As these variables got dropped from the model, the coefficient on <code class="docutils literal notranslate"><span class="pre">BATHS</span></code> increased to pick up their effect.</p>
<p>Another problem with Lasso coefficients is their instability. When multiple variables are highly correlated we may spuriously drop several of them. To get a sense of the amount of variability, in the next snippet we fix <span class="math notranslate nohighlight">\(\lambda\)</span> and then look at the lasso coefficients estimated during cross-validation. We see that by simply removing one fold we can get a very different set of coefficients (nonzero coefficients are in black in the heatmap below). This is because there may be many choices of coefficients with similar predictive power, so the set of nonzero coefficients we end up with can be quite unstable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">itertools</span>
<span class="n">nobs</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">nfold</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="c1"># Define folds indices </span>
<span class="n">list_1</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nfold</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span><span class="o">*</span><span class="n">nobs</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">nobs</span><span class="p">,</span><span class="n">nobs</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">foldid</span> <span class="o">=</span> <span class="p">[</span><span class="n">list_1</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">sample</span><span class="p">]</span>

    <span class="c1"># Create split function(similar to R)</span>
<span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
    <span class="n">count</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span> <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">compress</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">el</span> <span class="o">==</span> <span class="n">i</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">f</span><span class="p">)))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">count</span><span class="p">)</span> <span class="p">)</span> 

    <span class="c1"># Split observation indices into folds </span>
<span class="n">list_2</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nobs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
<span class="n">I</span> <span class="o">=</span> <span class="n">split</span><span class="p">(</span><span class="n">list_2</span><span class="p">,</span> <span class="n">foldid</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LassoCV</span>

<span class="n">scale_X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">lasso_coef_fold</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">I</span><span class="p">)):</span>
    
        <span class="c1"># Split data - index to keep are in mask as booleans</span>
        <span class="n">include_idx</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">I</span><span class="p">[</span><span class="n">b</span><span class="p">])</span>  <span class="c1">#Here should go I[b] Set is more efficient, but doesn&#39;t reorder your elements if that is desireable</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([(</span><span class="n">i</span> <span class="ow">in</span> <span class="n">include_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))])</span>

        <span class="c1"># Lasso regression, excluding folds selected </span>
        
        <span class="n">lassocv</span> <span class="o">=</span> <span class="n">LassoCV</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">lassocv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scale_X</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">])</span>
        <span class="n">lasso_coef_fold</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lassocv</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
       
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">index_val</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Fold-1&#39;</span><span class="p">,</span><span class="s1">&#39;Fold-2&#39;</span><span class="p">,</span><span class="s1">&#39;Fold-3&#39;</span><span class="p">,</span><span class="s1">&#39;Fold-4&#39;</span><span class="p">,</span><span class="s1">&#39;Fold-5&#39;</span><span class="p">,</span><span class="s1">&#39;Fold-6&#39;</span><span class="p">,</span><span class="s1">&#39;Fold-7&#39;</span><span class="p">,</span><span class="s1">&#39;Fold-8&#39;</span><span class="p">,</span><span class="s1">&#39;Fold-9&#39;</span><span class="p">,</span><span class="s1">&#39;Fold-10&#39;</span><span class="p">]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span> <span class="n">lasso_coef_fold</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="n">index_val</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="n">df</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">applymap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s2">&quot;background-color: white&quot;</span> <span class="k">if</span> <span class="n">x</span><span class="o">==</span><span class="mi">0</span> <span class="k">else</span> <span class="s2">&quot;background-color: black&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style type="text/css">
#T_0c898_row0_col0, #T_0c898_row0_col1, #T_0c898_row0_col2, #T_0c898_row0_col3, #T_0c898_row0_col4, #T_0c898_row0_col5, #T_0c898_row0_col6, #T_0c898_row0_col7, #T_0c898_row0_col8, #T_0c898_row0_col9, #T_0c898_row1_col0, #T_0c898_row1_col1, #T_0c898_row1_col2, #T_0c898_row1_col3, #T_0c898_row1_col4, #T_0c898_row1_col5, #T_0c898_row1_col6, #T_0c898_row1_col7, #T_0c898_row1_col8, #T_0c898_row1_col9, #T_0c898_row2_col0, #T_0c898_row2_col1, #T_0c898_row2_col2, #T_0c898_row2_col3, #T_0c898_row2_col4, #T_0c898_row2_col5, #T_0c898_row2_col6, #T_0c898_row2_col7, #T_0c898_row2_col8, #T_0c898_row2_col9, #T_0c898_row3_col0, #T_0c898_row3_col1, #T_0c898_row3_col2, #T_0c898_row3_col3, #T_0c898_row3_col4, #T_0c898_row3_col5, #T_0c898_row3_col6, #T_0c898_row3_col7, #T_0c898_row3_col8, #T_0c898_row3_col9, #T_0c898_row4_col0, #T_0c898_row4_col1, #T_0c898_row4_col2, #T_0c898_row4_col3, #T_0c898_row4_col4, #T_0c898_row4_col5, #T_0c898_row4_col6, #T_0c898_row4_col7, #T_0c898_row4_col8, #T_0c898_row4_col9, #T_0c898_row5_col0, #T_0c898_row5_col1, #T_0c898_row5_col2, #T_0c898_row5_col3, #T_0c898_row5_col4, #T_0c898_row5_col5, #T_0c898_row5_col6, #T_0c898_row5_col7, #T_0c898_row5_col8, #T_0c898_row5_col9, #T_0c898_row6_col1, #T_0c898_row6_col3, #T_0c898_row6_col4, #T_0c898_row6_col5, #T_0c898_row6_col7, #T_0c898_row6_col8, #T_0c898_row6_col9, #T_0c898_row7_col0, #T_0c898_row7_col1, #T_0c898_row7_col2, #T_0c898_row7_col3, #T_0c898_row7_col4, #T_0c898_row7_col5, #T_0c898_row7_col6, #T_0c898_row7_col7, #T_0c898_row7_col8, #T_0c898_row7_col9, #T_0c898_row8_col0, #T_0c898_row8_col1, #T_0c898_row8_col2, #T_0c898_row8_col3, #T_0c898_row8_col4, #T_0c898_row8_col5, #T_0c898_row8_col6, #T_0c898_row8_col7, #T_0c898_row8_col8, #T_0c898_row8_col9, #T_0c898_row9_col0, #T_0c898_row9_col1, #T_0c898_row9_col2, #T_0c898_row9_col3, #T_0c898_row9_col4, #T_0c898_row9_col5, #T_0c898_row9_col6, #T_0c898_row9_col7, #T_0c898_row9_col8, #T_0c898_row9_col9, #T_0c898_row10_col0, #T_0c898_row10_col1, #T_0c898_row10_col4, #T_0c898_row10_col5, #T_0c898_row10_col6, #T_0c898_row10_col7, #T_0c898_row10_col8, #T_0c898_row10_col9, #T_0c898_row11_col0, #T_0c898_row11_col2, #T_0c898_row11_col3, #T_0c898_row11_col4, #T_0c898_row11_col5, #T_0c898_row11_col7, #T_0c898_row11_col8, #T_0c898_row11_col9, #T_0c898_row12_col0, #T_0c898_row12_col1, #T_0c898_row12_col2, #T_0c898_row12_col3, #T_0c898_row12_col4, #T_0c898_row12_col5, #T_0c898_row12_col6, #T_0c898_row12_col7, #T_0c898_row12_col8, #T_0c898_row12_col9, #T_0c898_row15_col0, #T_0c898_row15_col1, #T_0c898_row15_col2, #T_0c898_row15_col3, #T_0c898_row15_col4, #T_0c898_row15_col5, #T_0c898_row15_col6, #T_0c898_row15_col7, #T_0c898_row15_col8, #T_0c898_row15_col9, #T_0c898_row17_col0, #T_0c898_row17_col1, #T_0c898_row17_col2, #T_0c898_row17_col3, #T_0c898_row17_col4, #T_0c898_row17_col5, #T_0c898_row17_col6, #T_0c898_row17_col7, #T_0c898_row17_col8, #T_0c898_row17_col9, #T_0c898_row18_col0, #T_0c898_row18_col1, #T_0c898_row18_col2, #T_0c898_row18_col3, #T_0c898_row18_col4, #T_0c898_row18_col5, #T_0c898_row18_col6, #T_0c898_row18_col7, #T_0c898_row18_col8, #T_0c898_row18_col9, #T_0c898_row19_col0, #T_0c898_row19_col1, #T_0c898_row19_col2, #T_0c898_row19_col3, #T_0c898_row19_col4, #T_0c898_row19_col5, #T_0c898_row19_col6, #T_0c898_row19_col7, #T_0c898_row19_col8, #T_0c898_row19_col9, #T_0c898_row20_col0, #T_0c898_row20_col1, #T_0c898_row20_col2, #T_0c898_row20_col3, #T_0c898_row20_col4, #T_0c898_row20_col5, #T_0c898_row20_col6, #T_0c898_row20_col7, #T_0c898_row20_col8, #T_0c898_row20_col9, #T_0c898_row21_col0, #T_0c898_row21_col1, #T_0c898_row21_col2, #T_0c898_row21_col3, #T_0c898_row21_col4, #T_0c898_row21_col5, #T_0c898_row21_col6, #T_0c898_row21_col7, #T_0c898_row21_col8, #T_0c898_row21_col9, #T_0c898_row22_col0, #T_0c898_row22_col1, #T_0c898_row22_col2, #T_0c898_row22_col3, #T_0c898_row22_col4, #T_0c898_row22_col5, #T_0c898_row22_col6, #T_0c898_row22_col7, #T_0c898_row22_col8, #T_0c898_row22_col9, #T_0c898_row27_col0, #T_0c898_row27_col1, #T_0c898_row27_col2, #T_0c898_row27_col3, #T_0c898_row27_col4, #T_0c898_row27_col5, #T_0c898_row27_col6, #T_0c898_row27_col7, #T_0c898_row27_col8, #T_0c898_row27_col9, #T_0c898_row28_col0, #T_0c898_row28_col1, #T_0c898_row28_col2, #T_0c898_row28_col3, #T_0c898_row28_col4, #T_0c898_row28_col5, #T_0c898_row28_col6, #T_0c898_row28_col7, #T_0c898_row28_col8, #T_0c898_row28_col9, #T_0c898_row29_col0, #T_0c898_row29_col1, #T_0c898_row29_col2, #T_0c898_row29_col3, #T_0c898_row29_col4, #T_0c898_row29_col5, #T_0c898_row29_col6, #T_0c898_row29_col7, #T_0c898_row29_col8, #T_0c898_row29_col9, #T_0c898_row30_col0, #T_0c898_row30_col1, #T_0c898_row30_col2, #T_0c898_row30_col3, #T_0c898_row30_col4, #T_0c898_row30_col5, #T_0c898_row30_col6, #T_0c898_row30_col7, #T_0c898_row30_col8, #T_0c898_row30_col9, #T_0c898_row31_col0, #T_0c898_row31_col1, #T_0c898_row31_col2, #T_0c898_row31_col3, #T_0c898_row31_col4, #T_0c898_row31_col5, #T_0c898_row31_col6, #T_0c898_row31_col7, #T_0c898_row31_col8, #T_0c898_row31_col9, #T_0c898_row32_col0, #T_0c898_row32_col1, #T_0c898_row32_col2, #T_0c898_row32_col3, #T_0c898_row32_col4, #T_0c898_row32_col5, #T_0c898_row32_col6, #T_0c898_row32_col7, #T_0c898_row32_col8, #T_0c898_row32_col9, #T_0c898_row33_col0, #T_0c898_row33_col1, #T_0c898_row33_col2, #T_0c898_row33_col3, #T_0c898_row33_col4, #T_0c898_row33_col5, #T_0c898_row33_col6, #T_0c898_row33_col7, #T_0c898_row33_col8, #T_0c898_row33_col9, #T_0c898_row34_col0, #T_0c898_row34_col1, #T_0c898_row34_col2, #T_0c898_row34_col3, #T_0c898_row34_col4, #T_0c898_row34_col5, #T_0c898_row34_col6, #T_0c898_row34_col7, #T_0c898_row34_col8, #T_0c898_row34_col9, #T_0c898_row35_col0, #T_0c898_row35_col1, #T_0c898_row35_col2, #T_0c898_row35_col3, #T_0c898_row35_col4, #T_0c898_row35_col5, #T_0c898_row35_col6, #T_0c898_row35_col7, #T_0c898_row35_col8, #T_0c898_row35_col9, #T_0c898_row36_col0, #T_0c898_row36_col1, #T_0c898_row36_col2, #T_0c898_row36_col3, #T_0c898_row36_col4, #T_0c898_row36_col5, #T_0c898_row36_col6, #T_0c898_row36_col7, #T_0c898_row36_col8, #T_0c898_row36_col9, #T_0c898_row37_col0, #T_0c898_row37_col1, #T_0c898_row37_col2, #T_0c898_row37_col3, #T_0c898_row37_col4, #T_0c898_row37_col5, #T_0c898_row37_col6, #T_0c898_row37_col7, #T_0c898_row37_col8, #T_0c898_row37_col9, #T_0c898_row38_col0, #T_0c898_row38_col1, #T_0c898_row38_col2, #T_0c898_row38_col3, #T_0c898_row38_col4, #T_0c898_row38_col5, #T_0c898_row38_col6, #T_0c898_row38_col7, #T_0c898_row38_col8, #T_0c898_row38_col9, #T_0c898_row40_col0, #T_0c898_row40_col1, #T_0c898_row40_col2, #T_0c898_row40_col3, #T_0c898_row40_col4, #T_0c898_row40_col5, #T_0c898_row40_col6, #T_0c898_row40_col7, #T_0c898_row40_col8, #T_0c898_row40_col9, #T_0c898_row41_col0, #T_0c898_row41_col1, #T_0c898_row41_col4, #T_0c898_row41_col5, #T_0c898_row41_col7, #T_0c898_row41_col8, #T_0c898_row43_col0, #T_0c898_row43_col1, #T_0c898_row43_col2, #T_0c898_row43_col3, #T_0c898_row43_col4, #T_0c898_row43_col5, #T_0c898_row43_col6, #T_0c898_row43_col7, #T_0c898_row43_col8, #T_0c898_row43_col9, #T_0c898_row46_col5, #T_0c898_row46_col7, #T_0c898_row47_col9, #T_0c898_row48_col0, #T_0c898_row48_col1, #T_0c898_row48_col2, #T_0c898_row48_col3, #T_0c898_row48_col4, #T_0c898_row48_col5, #T_0c898_row48_col7, #T_0c898_row48_col8, #T_0c898_row50_col0, #T_0c898_row50_col1, #T_0c898_row50_col2, #T_0c898_row50_col3, #T_0c898_row50_col4, #T_0c898_row50_col5, #T_0c898_row50_col6, #T_0c898_row50_col7, #T_0c898_row50_col8, #T_0c898_row50_col9, #T_0c898_row51_col4, #T_0c898_row52_col8, #T_0c898_row53_col0, #T_0c898_row53_col1, #T_0c898_row53_col2, #T_0c898_row53_col3, #T_0c898_row53_col4, #T_0c898_row53_col5, #T_0c898_row53_col6, #T_0c898_row53_col7, #T_0c898_row53_col8, #T_0c898_row53_col9, #T_0c898_row54_col0, #T_0c898_row54_col1, #T_0c898_row54_col2, #T_0c898_row54_col3, #T_0c898_row54_col4, #T_0c898_row54_col5, #T_0c898_row54_col6, #T_0c898_row54_col7, #T_0c898_row54_col8, #T_0c898_row54_col9, #T_0c898_row55_col4, #T_0c898_row55_col8, #T_0c898_row56_col0, #T_0c898_row57_col0, #T_0c898_row57_col1, #T_0c898_row57_col2, #T_0c898_row57_col3, #T_0c898_row57_col4, #T_0c898_row57_col5, #T_0c898_row57_col6, #T_0c898_row57_col7, #T_0c898_row57_col8, #T_0c898_row57_col9, #T_0c898_row59_col0, #T_0c898_row59_col1, #T_0c898_row59_col2, #T_0c898_row59_col3, #T_0c898_row59_col4, #T_0c898_row59_col6, #T_0c898_row59_col7, #T_0c898_row59_col8, #T_0c898_row59_col9, #T_0c898_row60_col0, #T_0c898_row60_col3, #T_0c898_row60_col8, #T_0c898_row62_col0, #T_0c898_row62_col1, #T_0c898_row62_col2, #T_0c898_row62_col3, #T_0c898_row62_col5, #T_0c898_row62_col6, #T_0c898_row62_col7, #T_0c898_row62_col8, #T_0c898_row63_col0, #T_0c898_row63_col1, #T_0c898_row63_col2, #T_0c898_row63_col3, #T_0c898_row63_col4, #T_0c898_row63_col5, #T_0c898_row63_col6, #T_0c898_row63_col7 {
  background-color: black;
}
#T_0c898_row6_col0, #T_0c898_row6_col2, #T_0c898_row6_col6, #T_0c898_row10_col2, #T_0c898_row10_col3, #T_0c898_row11_col1, #T_0c898_row11_col6, #T_0c898_row13_col0, #T_0c898_row13_col1, #T_0c898_row13_col2, #T_0c898_row13_col3, #T_0c898_row13_col4, #T_0c898_row13_col5, #T_0c898_row13_col6, #T_0c898_row13_col7, #T_0c898_row13_col8, #T_0c898_row13_col9, #T_0c898_row14_col0, #T_0c898_row14_col1, #T_0c898_row14_col2, #T_0c898_row14_col3, #T_0c898_row14_col4, #T_0c898_row14_col5, #T_0c898_row14_col6, #T_0c898_row14_col7, #T_0c898_row14_col8, #T_0c898_row14_col9, #T_0c898_row16_col0, #T_0c898_row16_col1, #T_0c898_row16_col2, #T_0c898_row16_col3, #T_0c898_row16_col4, #T_0c898_row16_col5, #T_0c898_row16_col6, #T_0c898_row16_col7, #T_0c898_row16_col8, #T_0c898_row16_col9, #T_0c898_row23_col0, #T_0c898_row23_col1, #T_0c898_row23_col2, #T_0c898_row23_col3, #T_0c898_row23_col4, #T_0c898_row23_col5, #T_0c898_row23_col6, #T_0c898_row23_col7, #T_0c898_row23_col8, #T_0c898_row23_col9, #T_0c898_row24_col0, #T_0c898_row24_col1, #T_0c898_row24_col2, #T_0c898_row24_col3, #T_0c898_row24_col4, #T_0c898_row24_col5, #T_0c898_row24_col6, #T_0c898_row24_col7, #T_0c898_row24_col8, #T_0c898_row24_col9, #T_0c898_row25_col0, #T_0c898_row25_col1, #T_0c898_row25_col2, #T_0c898_row25_col3, #T_0c898_row25_col4, #T_0c898_row25_col5, #T_0c898_row25_col6, #T_0c898_row25_col7, #T_0c898_row25_col8, #T_0c898_row25_col9, #T_0c898_row26_col0, #T_0c898_row26_col1, #T_0c898_row26_col2, #T_0c898_row26_col3, #T_0c898_row26_col4, #T_0c898_row26_col5, #T_0c898_row26_col6, #T_0c898_row26_col7, #T_0c898_row26_col8, #T_0c898_row26_col9, #T_0c898_row39_col0, #T_0c898_row39_col1, #T_0c898_row39_col2, #T_0c898_row39_col3, #T_0c898_row39_col4, #T_0c898_row39_col5, #T_0c898_row39_col6, #T_0c898_row39_col7, #T_0c898_row39_col8, #T_0c898_row39_col9, #T_0c898_row41_col2, #T_0c898_row41_col3, #T_0c898_row41_col6, #T_0c898_row41_col9, #T_0c898_row42_col0, #T_0c898_row42_col1, #T_0c898_row42_col2, #T_0c898_row42_col3, #T_0c898_row42_col4, #T_0c898_row42_col5, #T_0c898_row42_col6, #T_0c898_row42_col7, #T_0c898_row42_col8, #T_0c898_row42_col9, #T_0c898_row44_col0, #T_0c898_row44_col1, #T_0c898_row44_col2, #T_0c898_row44_col3, #T_0c898_row44_col4, #T_0c898_row44_col5, #T_0c898_row44_col6, #T_0c898_row44_col7, #T_0c898_row44_col8, #T_0c898_row44_col9, #T_0c898_row45_col0, #T_0c898_row45_col1, #T_0c898_row45_col2, #T_0c898_row45_col3, #T_0c898_row45_col4, #T_0c898_row45_col5, #T_0c898_row45_col6, #T_0c898_row45_col7, #T_0c898_row45_col8, #T_0c898_row45_col9, #T_0c898_row46_col0, #T_0c898_row46_col1, #T_0c898_row46_col2, #T_0c898_row46_col3, #T_0c898_row46_col4, #T_0c898_row46_col6, #T_0c898_row46_col8, #T_0c898_row46_col9, #T_0c898_row47_col0, #T_0c898_row47_col1, #T_0c898_row47_col2, #T_0c898_row47_col3, #T_0c898_row47_col4, #T_0c898_row47_col5, #T_0c898_row47_col6, #T_0c898_row47_col7, #T_0c898_row47_col8, #T_0c898_row48_col6, #T_0c898_row48_col9, #T_0c898_row49_col0, #T_0c898_row49_col1, #T_0c898_row49_col2, #T_0c898_row49_col3, #T_0c898_row49_col4, #T_0c898_row49_col5, #T_0c898_row49_col6, #T_0c898_row49_col7, #T_0c898_row49_col8, #T_0c898_row49_col9, #T_0c898_row51_col0, #T_0c898_row51_col1, #T_0c898_row51_col2, #T_0c898_row51_col3, #T_0c898_row51_col5, #T_0c898_row51_col6, #T_0c898_row51_col7, #T_0c898_row51_col8, #T_0c898_row51_col9, #T_0c898_row52_col0, #T_0c898_row52_col1, #T_0c898_row52_col2, #T_0c898_row52_col3, #T_0c898_row52_col4, #T_0c898_row52_col5, #T_0c898_row52_col6, #T_0c898_row52_col7, #T_0c898_row52_col9, #T_0c898_row55_col0, #T_0c898_row55_col1, #T_0c898_row55_col2, #T_0c898_row55_col3, #T_0c898_row55_col5, #T_0c898_row55_col6, #T_0c898_row55_col7, #T_0c898_row55_col9, #T_0c898_row56_col1, #T_0c898_row56_col2, #T_0c898_row56_col3, #T_0c898_row56_col4, #T_0c898_row56_col5, #T_0c898_row56_col6, #T_0c898_row56_col7, #T_0c898_row56_col8, #T_0c898_row56_col9, #T_0c898_row58_col0, #T_0c898_row58_col1, #T_0c898_row58_col2, #T_0c898_row58_col3, #T_0c898_row58_col4, #T_0c898_row58_col5, #T_0c898_row58_col6, #T_0c898_row58_col7, #T_0c898_row58_col8, #T_0c898_row58_col9, #T_0c898_row59_col5, #T_0c898_row60_col1, #T_0c898_row60_col2, #T_0c898_row60_col4, #T_0c898_row60_col5, #T_0c898_row60_col6, #T_0c898_row60_col7, #T_0c898_row60_col9, #T_0c898_row61_col0, #T_0c898_row61_col1, #T_0c898_row61_col2, #T_0c898_row61_col3, #T_0c898_row61_col4, #T_0c898_row61_col5, #T_0c898_row61_col6, #T_0c898_row61_col7, #T_0c898_row61_col8, #T_0c898_row61_col9, #T_0c898_row62_col4, #T_0c898_row62_col9, #T_0c898_row63_col8, #T_0c898_row63_col9 {
  background-color: white;
}
</style>
<table id="T_0c898">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_0c898_level0_col0" class="col_heading level0 col0" >Fold-1</th>
      <th id="T_0c898_level0_col1" class="col_heading level0 col1" >Fold-2</th>
      <th id="T_0c898_level0_col2" class="col_heading level0 col2" >Fold-3</th>
      <th id="T_0c898_level0_col3" class="col_heading level0 col3" >Fold-4</th>
      <th id="T_0c898_level0_col4" class="col_heading level0 col4" >Fold-5</th>
      <th id="T_0c898_level0_col5" class="col_heading level0 col5" >Fold-6</th>
      <th id="T_0c898_level0_col6" class="col_heading level0 col6" >Fold-7</th>
      <th id="T_0c898_level0_col7" class="col_heading level0 col7" >Fold-8</th>
      <th id="T_0c898_level0_col8" class="col_heading level0 col8" >Fold-9</th>
      <th id="T_0c898_level0_col9" class="col_heading level0 col9" >Fold-10</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_0c898_level0_row0" class="row_heading level0 row0" >LOT</th>
      <td id="T_0c898_row0_col0" class="data row0 col0" >0.041050</td>
      <td id="T_0c898_row0_col1" class="data row0 col1" >0.040789</td>
      <td id="T_0c898_row0_col2" class="data row0 col2" >0.039105</td>
      <td id="T_0c898_row0_col3" class="data row0 col3" >0.037300</td>
      <td id="T_0c898_row0_col4" class="data row0 col4" >0.041148</td>
      <td id="T_0c898_row0_col5" class="data row0 col5" >0.043150</td>
      <td id="T_0c898_row0_col6" class="data row0 col6" >0.037104</td>
      <td id="T_0c898_row0_col7" class="data row0 col7" >0.035392</td>
      <td id="T_0c898_row0_col8" class="data row0 col8" >0.037300</td>
      <td id="T_0c898_row0_col9" class="data row0 col9" >0.037464</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row1" class="row_heading level0 row1" >UNITSF</th>
      <td id="T_0c898_row1_col0" class="data row1 col0" >0.044746</td>
      <td id="T_0c898_row1_col1" class="data row1 col1" >0.046055</td>
      <td id="T_0c898_row1_col2" class="data row1 col2" >0.047095</td>
      <td id="T_0c898_row1_col3" class="data row1 col3" >0.045291</td>
      <td id="T_0c898_row1_col4" class="data row1 col4" >0.049540</td>
      <td id="T_0c898_row1_col5" class="data row1 col5" >0.043839</td>
      <td id="T_0c898_row1_col6" class="data row1 col6" >0.043077</td>
      <td id="T_0c898_row1_col7" class="data row1 col7" >0.051535</td>
      <td id="T_0c898_row1_col8" class="data row1 col8" >0.047132</td>
      <td id="T_0c898_row1_col9" class="data row1 col9" >0.046415</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row2" class="row_heading level0 row2" >BUILT</th>
      <td id="T_0c898_row2_col0" class="data row2 col0" >0.001111</td>
      <td id="T_0c898_row2_col1" class="data row2 col1" >0.004845</td>
      <td id="T_0c898_row2_col2" class="data row2 col2" >0.003385</td>
      <td id="T_0c898_row2_col3" class="data row2 col3" >0.003564</td>
      <td id="T_0c898_row2_col4" class="data row2 col4" >0.004757</td>
      <td id="T_0c898_row2_col5" class="data row2 col5" >0.003220</td>
      <td id="T_0c898_row2_col6" class="data row2 col6" >0.003449</td>
      <td id="T_0c898_row2_col7" class="data row2 col7" >0.002987</td>
      <td id="T_0c898_row2_col8" class="data row2 col8" >0.000929</td>
      <td id="T_0c898_row2_col9" class="data row2 col9" >0.004401</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row3" class="row_heading level0 row3" >BATHS</th>
      <td id="T_0c898_row3_col0" class="data row3 col0" >0.200578</td>
      <td id="T_0c898_row3_col1" class="data row3 col1" >0.189623</td>
      <td id="T_0c898_row3_col2" class="data row3 col2" >0.195828</td>
      <td id="T_0c898_row3_col3" class="data row3 col3" >0.200489</td>
      <td id="T_0c898_row3_col4" class="data row3 col4" >0.192490</td>
      <td id="T_0c898_row3_col5" class="data row3 col5" >0.198082</td>
      <td id="T_0c898_row3_col6" class="data row3 col6" >0.203624</td>
      <td id="T_0c898_row3_col7" class="data row3 col7" >0.200081</td>
      <td id="T_0c898_row3_col8" class="data row3 col8" >0.198007</td>
      <td id="T_0c898_row3_col9" class="data row3 col9" >0.198827</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row4" class="row_heading level0 row4" >BEDRMS</th>
      <td id="T_0c898_row4_col0" class="data row4 col0" >0.055605</td>
      <td id="T_0c898_row4_col1" class="data row4 col1" >0.057472</td>
      <td id="T_0c898_row4_col2" class="data row4 col2" >0.055982</td>
      <td id="T_0c898_row4_col3" class="data row4 col3" >0.055394</td>
      <td id="T_0c898_row4_col4" class="data row4 col4" >0.054981</td>
      <td id="T_0c898_row4_col5" class="data row4 col5" >0.056335</td>
      <td id="T_0c898_row4_col6" class="data row4 col6" >0.054475</td>
      <td id="T_0c898_row4_col7" class="data row4 col7" >0.049082</td>
      <td id="T_0c898_row4_col8" class="data row4 col8" >0.055994</td>
      <td id="T_0c898_row4_col9" class="data row4 col9" >0.052763</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row5" class="row_heading level0 row5" >DINING</th>
      <td id="T_0c898_row5_col0" class="data row5 col0" >0.047736</td>
      <td id="T_0c898_row5_col1" class="data row5 col1" >0.046748</td>
      <td id="T_0c898_row5_col2" class="data row5 col2" >0.047269</td>
      <td id="T_0c898_row5_col3" class="data row5 col3" >0.044850</td>
      <td id="T_0c898_row5_col4" class="data row5 col4" >0.044751</td>
      <td id="T_0c898_row5_col5" class="data row5 col5" >0.046515</td>
      <td id="T_0c898_row5_col6" class="data row5 col6" >0.044934</td>
      <td id="T_0c898_row5_col7" class="data row5 col7" >0.048129</td>
      <td id="T_0c898_row5_col8" class="data row5 col8" >0.046415</td>
      <td id="T_0c898_row5_col9" class="data row5 col9" >0.046481</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row6" class="row_heading level0 row6" >METRO</th>
      <td id="T_0c898_row6_col0" class="data row6 col0" >0.000000</td>
      <td id="T_0c898_row6_col1" class="data row6 col1" >0.000356</td>
      <td id="T_0c898_row6_col2" class="data row6 col2" >0.000000</td>
      <td id="T_0c898_row6_col3" class="data row6 col3" >0.001081</td>
      <td id="T_0c898_row6_col4" class="data row6 col4" >0.001190</td>
      <td id="T_0c898_row6_col5" class="data row6 col5" >0.000881</td>
      <td id="T_0c898_row6_col6" class="data row6 col6" >0.000000</td>
      <td id="T_0c898_row6_col7" class="data row6 col7" >0.003189</td>
      <td id="T_0c898_row6_col8" class="data row6 col8" >0.001222</td>
      <td id="T_0c898_row6_col9" class="data row6 col9" >0.002415</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row7" class="row_heading level0 row7" >CRACKS</th>
      <td id="T_0c898_row7_col0" class="data row7 col0" >0.020332</td>
      <td id="T_0c898_row7_col1" class="data row7 col1" >0.020937</td>
      <td id="T_0c898_row7_col2" class="data row7 col2" >0.017848</td>
      <td id="T_0c898_row7_col3" class="data row7 col3" >0.015932</td>
      <td id="T_0c898_row7_col4" class="data row7 col4" >0.019917</td>
      <td id="T_0c898_row7_col5" class="data row7 col5" >0.019677</td>
      <td id="T_0c898_row7_col6" class="data row7 col6" >0.018395</td>
      <td id="T_0c898_row7_col7" class="data row7 col7" >0.023793</td>
      <td id="T_0c898_row7_col8" class="data row7 col8" >0.020314</td>
      <td id="T_0c898_row7_col9" class="data row7 col9" >0.019614</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row8" class="row_heading level0 row8" >REGION</th>
      <td id="T_0c898_row8_col0" class="data row8 col0" >0.083864</td>
      <td id="T_0c898_row8_col1" class="data row8 col1" >0.083337</td>
      <td id="T_0c898_row8_col2" class="data row8 col2" >0.080464</td>
      <td id="T_0c898_row8_col3" class="data row8 col3" >0.081884</td>
      <td id="T_0c898_row8_col4" class="data row8 col4" >0.081064</td>
      <td id="T_0c898_row8_col5" class="data row8 col5" >0.082150</td>
      <td id="T_0c898_row8_col6" class="data row8 col6" >0.078420</td>
      <td id="T_0c898_row8_col7" class="data row8 col7" >0.082237</td>
      <td id="T_0c898_row8_col8" class="data row8 col8" >0.082466</td>
      <td id="T_0c898_row8_col9" class="data row8 col9" >0.082625</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row9" class="row_heading level0 row9" >METRO3</th>
      <td id="T_0c898_row9_col0" class="data row9 col0" >0.007152</td>
      <td id="T_0c898_row9_col1" class="data row9 col1" >0.006738</td>
      <td id="T_0c898_row9_col2" class="data row9 col2" >0.009395</td>
      <td id="T_0c898_row9_col3" class="data row9 col3" >0.009017</td>
      <td id="T_0c898_row9_col4" class="data row9 col4" >0.010476</td>
      <td id="T_0c898_row9_col5" class="data row9 col5" >0.010692</td>
      <td id="T_0c898_row9_col6" class="data row9 col6" >0.007217</td>
      <td id="T_0c898_row9_col7" class="data row9 col7" >0.008143</td>
      <td id="T_0c898_row9_col8" class="data row9 col8" >0.008373</td>
      <td id="T_0c898_row9_col9" class="data row9 col9" >0.007819</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row10" class="row_heading level0 row10" >PHONE</th>
      <td id="T_0c898_row10_col0" class="data row10 col0" >0.003223</td>
      <td id="T_0c898_row10_col1" class="data row10 col1" >0.004145</td>
      <td id="T_0c898_row10_col2" class="data row10 col2" >0.000000</td>
      <td id="T_0c898_row10_col3" class="data row10 col3" >0.000000</td>
      <td id="T_0c898_row10_col4" class="data row10 col4" >0.003644</td>
      <td id="T_0c898_row10_col5" class="data row10 col5" >0.001984</td>
      <td id="T_0c898_row10_col6" class="data row10 col6" >0.001331</td>
      <td id="T_0c898_row10_col7" class="data row10 col7" >0.003200</td>
      <td id="T_0c898_row10_col8" class="data row10 col8" >0.001796</td>
      <td id="T_0c898_row10_col9" class="data row10 col9" >0.001127</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row11" class="row_heading level0 row11" >KITCHEN</th>
      <td id="T_0c898_row11_col0" class="data row11 col0" >-0.003205</td>
      <td id="T_0c898_row11_col1" class="data row11 col1" >-0.000000</td>
      <td id="T_0c898_row11_col2" class="data row11 col2" >-0.000955</td>
      <td id="T_0c898_row11_col3" class="data row11 col3" >-0.002583</td>
      <td id="T_0c898_row11_col4" class="data row11 col4" >-0.007191</td>
      <td id="T_0c898_row11_col5" class="data row11 col5" >-0.002836</td>
      <td id="T_0c898_row11_col6" class="data row11 col6" >-0.000000</td>
      <td id="T_0c898_row11_col7" class="data row11 col7" >-0.003221</td>
      <td id="T_0c898_row11_col8" class="data row11 col8" >-0.005402</td>
      <td id="T_0c898_row11_col9" class="data row11 col9" >-0.000577</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row12" class="row_heading level0 row12" >MOBILTYP</th>
      <td id="T_0c898_row12_col0" class="data row12 col0" >-0.119085</td>
      <td id="T_0c898_row12_col1" class="data row12 col1" >-0.103709</td>
      <td id="T_0c898_row12_col2" class="data row12 col2" >-0.118946</td>
      <td id="T_0c898_row12_col3" class="data row12 col3" >-0.111606</td>
      <td id="T_0c898_row12_col4" class="data row12 col4" >-0.106277</td>
      <td id="T_0c898_row12_col5" class="data row12 col5" >-0.113575</td>
      <td id="T_0c898_row12_col6" class="data row12 col6" >-0.109086</td>
      <td id="T_0c898_row12_col7" class="data row12 col7" >-0.103446</td>
      <td id="T_0c898_row12_col8" class="data row12 col8" >-0.114251</td>
      <td id="T_0c898_row12_col9" class="data row12 col9" >-0.115418</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row13" class="row_heading level0 row13" >WINTEROVEN</th>
      <td id="T_0c898_row13_col0" class="data row13 col0" >0.000000</td>
      <td id="T_0c898_row13_col1" class="data row13 col1" >0.000000</td>
      <td id="T_0c898_row13_col2" class="data row13 col2" >0.000000</td>
      <td id="T_0c898_row13_col3" class="data row13 col3" >0.000000</td>
      <td id="T_0c898_row13_col4" class="data row13 col4" >0.000000</td>
      <td id="T_0c898_row13_col5" class="data row13 col5" >0.000000</td>
      <td id="T_0c898_row13_col6" class="data row13 col6" >0.000000</td>
      <td id="T_0c898_row13_col7" class="data row13 col7" >0.000000</td>
      <td id="T_0c898_row13_col8" class="data row13 col8" >0.000000</td>
      <td id="T_0c898_row13_col9" class="data row13 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row14" class="row_heading level0 row14" >WINTERKESP</th>
      <td id="T_0c898_row14_col0" class="data row14 col0" >0.000000</td>
      <td id="T_0c898_row14_col1" class="data row14 col1" >-0.000000</td>
      <td id="T_0c898_row14_col2" class="data row14 col2" >0.000000</td>
      <td id="T_0c898_row14_col3" class="data row14 col3" >0.000000</td>
      <td id="T_0c898_row14_col4" class="data row14 col4" >-0.000000</td>
      <td id="T_0c898_row14_col5" class="data row14 col5" >0.000000</td>
      <td id="T_0c898_row14_col6" class="data row14 col6" >0.000000</td>
      <td id="T_0c898_row14_col7" class="data row14 col7" >-0.000000</td>
      <td id="T_0c898_row14_col8" class="data row14 col8" >0.000000</td>
      <td id="T_0c898_row14_col9" class="data row14 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row15" class="row_heading level0 row15" >WINTERELSP</th>
      <td id="T_0c898_row15_col0" class="data row15 col0" >0.026793</td>
      <td id="T_0c898_row15_col1" class="data row15 col1" >0.021703</td>
      <td id="T_0c898_row15_col2" class="data row15 col2" >0.025619</td>
      <td id="T_0c898_row15_col3" class="data row15 col3" >0.026638</td>
      <td id="T_0c898_row15_col4" class="data row15 col4" >0.026866</td>
      <td id="T_0c898_row15_col5" class="data row15 col5" >0.024999</td>
      <td id="T_0c898_row15_col6" class="data row15 col6" >0.024933</td>
      <td id="T_0c898_row15_col7" class="data row15 col7" >0.030121</td>
      <td id="T_0c898_row15_col8" class="data row15 col8" >0.026697</td>
      <td id="T_0c898_row15_col9" class="data row15 col9" >0.027365</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row16" class="row_heading level0 row16" >WINTERWOOD</th>
      <td id="T_0c898_row16_col0" class="data row16 col0" >0.000000</td>
      <td id="T_0c898_row16_col1" class="data row16 col1" >-0.000000</td>
      <td id="T_0c898_row16_col2" class="data row16 col2" >0.000000</td>
      <td id="T_0c898_row16_col3" class="data row16 col3" >0.000000</td>
      <td id="T_0c898_row16_col4" class="data row16 col4" >-0.000000</td>
      <td id="T_0c898_row16_col5" class="data row16 col5" >0.000000</td>
      <td id="T_0c898_row16_col6" class="data row16 col6" >0.000000</td>
      <td id="T_0c898_row16_col7" class="data row16 col7" >-0.000000</td>
      <td id="T_0c898_row16_col8" class="data row16 col8" >0.000000</td>
      <td id="T_0c898_row16_col9" class="data row16 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row17" class="row_heading level0 row17" >WINTERNONE</th>
      <td id="T_0c898_row17_col0" class="data row17 col0" >-0.006475</td>
      <td id="T_0c898_row17_col1" class="data row17 col1" >-0.007696</td>
      <td id="T_0c898_row17_col2" class="data row17 col2" >-0.001862</td>
      <td id="T_0c898_row17_col3" class="data row17 col3" >-0.000594</td>
      <td id="T_0c898_row17_col4" class="data row17 col4" >-0.003744</td>
      <td id="T_0c898_row17_col5" class="data row17 col5" >-0.001674</td>
      <td id="T_0c898_row17_col6" class="data row17 col6" >-0.002170</td>
      <td id="T_0c898_row17_col7" class="data row17 col7" >-0.004903</td>
      <td id="T_0c898_row17_col8" class="data row17 col8" >-0.008437</td>
      <td id="T_0c898_row17_col9" class="data row17 col9" >-0.001137</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row18" class="row_heading level0 row18" >NEWC</th>
      <td id="T_0c898_row18_col0" class="data row18 col0" >0.029223</td>
      <td id="T_0c898_row18_col1" class="data row18 col1" >0.027175</td>
      <td id="T_0c898_row18_col2" class="data row18 col2" >0.027914</td>
      <td id="T_0c898_row18_col3" class="data row18 col3" >0.026626</td>
      <td id="T_0c898_row18_col4" class="data row18 col4" >0.027992</td>
      <td id="T_0c898_row18_col5" class="data row18 col5" >0.029549</td>
      <td id="T_0c898_row18_col6" class="data row18 col6" >0.031211</td>
      <td id="T_0c898_row18_col7" class="data row18 col7" >0.027483</td>
      <td id="T_0c898_row18_col8" class="data row18 col8" >0.028221</td>
      <td id="T_0c898_row18_col9" class="data row18 col9" >0.028651</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row19" class="row_heading level0 row19" >DISH</th>
      <td id="T_0c898_row19_col0" class="data row19 col0" >-0.096273</td>
      <td id="T_0c898_row19_col1" class="data row19 col1" >-0.098615</td>
      <td id="T_0c898_row19_col2" class="data row19 col2" >-0.095563</td>
      <td id="T_0c898_row19_col3" class="data row19 col3" >-0.093536</td>
      <td id="T_0c898_row19_col4" class="data row19 col4" >-0.095071</td>
      <td id="T_0c898_row19_col5" class="data row19 col5" >-0.097641</td>
      <td id="T_0c898_row19_col6" class="data row19 col6" >-0.094371</td>
      <td id="T_0c898_row19_col7" class="data row19 col7" >-0.098233</td>
      <td id="T_0c898_row19_col8" class="data row19 col8" >-0.095227</td>
      <td id="T_0c898_row19_col9" class="data row19 col9" >-0.096898</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row20" class="row_heading level0 row20" >WASH</th>
      <td id="T_0c898_row20_col0" class="data row20 col0" >-0.001606</td>
      <td id="T_0c898_row20_col1" class="data row20 col1" >-0.008013</td>
      <td id="T_0c898_row20_col2" class="data row20 col2" >-0.012339</td>
      <td id="T_0c898_row20_col3" class="data row20 col3" >-0.002369</td>
      <td id="T_0c898_row20_col4" class="data row20 col4" >-0.016570</td>
      <td id="T_0c898_row20_col5" class="data row20 col5" >-0.002033</td>
      <td id="T_0c898_row20_col6" class="data row20 col6" >-0.011885</td>
      <td id="T_0c898_row20_col7" class="data row20 col7" >-0.004852</td>
      <td id="T_0c898_row20_col8" class="data row20 col8" >-0.007794</td>
      <td id="T_0c898_row20_col9" class="data row20 col9" >-0.010408</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row21" class="row_heading level0 row21" >DRY</th>
      <td id="T_0c898_row21_col0" class="data row21 col0" >-0.034784</td>
      <td id="T_0c898_row21_col1" class="data row21 col1" >-0.032210</td>
      <td id="T_0c898_row21_col2" class="data row21 col2" >-0.029772</td>
      <td id="T_0c898_row21_col3" class="data row21 col3" >-0.031367</td>
      <td id="T_0c898_row21_col4" class="data row21 col4" >-0.027754</td>
      <td id="T_0c898_row21_col5" class="data row21 col5" >-0.035728</td>
      <td id="T_0c898_row21_col6" class="data row21 col6" >-0.029114</td>
      <td id="T_0c898_row21_col7" class="data row21 col7" >-0.029364</td>
      <td id="T_0c898_row21_col8" class="data row21 col8" >-0.032434</td>
      <td id="T_0c898_row21_col9" class="data row21 col9" >-0.026725</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row22" class="row_heading level0 row22" >NUNIT2</th>
      <td id="T_0c898_row22_col0" class="data row22 col0" >-0.216673</td>
      <td id="T_0c898_row22_col1" class="data row22 col1" >-0.229393</td>
      <td id="T_0c898_row22_col2" class="data row22 col2" >-0.213668</td>
      <td id="T_0c898_row22_col3" class="data row22 col3" >-0.219420</td>
      <td id="T_0c898_row22_col4" class="data row22 col4" >-0.230576</td>
      <td id="T_0c898_row22_col5" class="data row22 col5" >-0.219189</td>
      <td id="T_0c898_row22_col6" class="data row22 col6" >-0.224386</td>
      <td id="T_0c898_row22_col7" class="data row22 col7" >-0.228164</td>
      <td id="T_0c898_row22_col8" class="data row22 col8" >-0.217753</td>
      <td id="T_0c898_row22_col9" class="data row22 col9" >-0.218393</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row23" class="row_heading level0 row23" >BURNER</th>
      <td id="T_0c898_row23_col0" class="data row23 col0" >-0.000000</td>
      <td id="T_0c898_row23_col1" class="data row23 col1" >-0.000000</td>
      <td id="T_0c898_row23_col2" class="data row23 col2" >0.000000</td>
      <td id="T_0c898_row23_col3" class="data row23 col3" >-0.000000</td>
      <td id="T_0c898_row23_col4" class="data row23 col4" >-0.000000</td>
      <td id="T_0c898_row23_col5" class="data row23 col5" >-0.000000</td>
      <td id="T_0c898_row23_col6" class="data row23 col6" >-0.000000</td>
      <td id="T_0c898_row23_col7" class="data row23 col7" >0.000000</td>
      <td id="T_0c898_row23_col8" class="data row23 col8" >0.000000</td>
      <td id="T_0c898_row23_col9" class="data row23 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row24" class="row_heading level0 row24" >COOK</th>
      <td id="T_0c898_row24_col0" class="data row24 col0" >-0.000000</td>
      <td id="T_0c898_row24_col1" class="data row24 col1" >-0.000000</td>
      <td id="T_0c898_row24_col2" class="data row24 col2" >0.000000</td>
      <td id="T_0c898_row24_col3" class="data row24 col3" >-0.000000</td>
      <td id="T_0c898_row24_col4" class="data row24 col4" >-0.000000</td>
      <td id="T_0c898_row24_col5" class="data row24 col5" >-0.000000</td>
      <td id="T_0c898_row24_col6" class="data row24 col6" >-0.000000</td>
      <td id="T_0c898_row24_col7" class="data row24 col7" >0.000000</td>
      <td id="T_0c898_row24_col8" class="data row24 col8" >0.000000</td>
      <td id="T_0c898_row24_col9" class="data row24 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row25" class="row_heading level0 row25" >OVEN</th>
      <td id="T_0c898_row25_col0" class="data row25 col0" >-0.000000</td>
      <td id="T_0c898_row25_col1" class="data row25 col1" >-0.000000</td>
      <td id="T_0c898_row25_col2" class="data row25 col2" >0.000000</td>
      <td id="T_0c898_row25_col3" class="data row25 col3" >-0.000000</td>
      <td id="T_0c898_row25_col4" class="data row25 col4" >-0.000000</td>
      <td id="T_0c898_row25_col5" class="data row25 col5" >-0.000000</td>
      <td id="T_0c898_row25_col6" class="data row25 col6" >-0.000000</td>
      <td id="T_0c898_row25_col7" class="data row25 col7" >0.000000</td>
      <td id="T_0c898_row25_col8" class="data row25 col8" >0.000000</td>
      <td id="T_0c898_row25_col9" class="data row25 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row26" class="row_heading level0 row26" >REFR</th>
      <td id="T_0c898_row26_col0" class="data row26 col0" >-0.000000</td>
      <td id="T_0c898_row26_col1" class="data row26 col1" >-0.000000</td>
      <td id="T_0c898_row26_col2" class="data row26 col2" >-0.000000</td>
      <td id="T_0c898_row26_col3" class="data row26 col3" >-0.000000</td>
      <td id="T_0c898_row26_col4" class="data row26 col4" >-0.000000</td>
      <td id="T_0c898_row26_col5" class="data row26 col5" >-0.000000</td>
      <td id="T_0c898_row26_col6" class="data row26 col6" >-0.000000</td>
      <td id="T_0c898_row26_col7" class="data row26 col7" >-0.000000</td>
      <td id="T_0c898_row26_col8" class="data row26 col8" >-0.000000</td>
      <td id="T_0c898_row26_col9" class="data row26 col9" >-0.000000</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row27" class="row_heading level0 row27" >DENS</th>
      <td id="T_0c898_row27_col0" class="data row27 col0" >0.048246</td>
      <td id="T_0c898_row27_col1" class="data row27 col1" >0.049359</td>
      <td id="T_0c898_row27_col2" class="data row27 col2" >0.046588</td>
      <td id="T_0c898_row27_col3" class="data row27 col3" >0.047767</td>
      <td id="T_0c898_row27_col4" class="data row27 col4" >0.051190</td>
      <td id="T_0c898_row27_col5" class="data row27 col5" >0.046928</td>
      <td id="T_0c898_row27_col6" class="data row27 col6" >0.046455</td>
      <td id="T_0c898_row27_col7" class="data row27 col7" >0.047423</td>
      <td id="T_0c898_row27_col8" class="data row27 col8" >0.049179</td>
      <td id="T_0c898_row27_col9" class="data row27 col9" >0.048865</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row28" class="row_heading level0 row28" >FAMRM</th>
      <td id="T_0c898_row28_col0" class="data row28 col0" >0.057822</td>
      <td id="T_0c898_row28_col1" class="data row28 col1" >0.057013</td>
      <td id="T_0c898_row28_col2" class="data row28 col2" >0.057238</td>
      <td id="T_0c898_row28_col3" class="data row28 col3" >0.059208</td>
      <td id="T_0c898_row28_col4" class="data row28 col4" >0.058518</td>
      <td id="T_0c898_row28_col5" class="data row28 col5" >0.055123</td>
      <td id="T_0c898_row28_col6" class="data row28 col6" >0.057817</td>
      <td id="T_0c898_row28_col7" class="data row28 col7" >0.058604</td>
      <td id="T_0c898_row28_col8" class="data row28 col8" >0.059895</td>
      <td id="T_0c898_row28_col9" class="data row28 col9" >0.057424</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row29" class="row_heading level0 row29" >HALFB</th>
      <td id="T_0c898_row29_col0" class="data row29 col0" >0.103928</td>
      <td id="T_0c898_row29_col1" class="data row29 col1" >0.102791</td>
      <td id="T_0c898_row29_col2" class="data row29 col2" >0.105183</td>
      <td id="T_0c898_row29_col3" class="data row29 col3" >0.104379</td>
      <td id="T_0c898_row29_col4" class="data row29 col4" >0.103671</td>
      <td id="T_0c898_row29_col5" class="data row29 col5" >0.106806</td>
      <td id="T_0c898_row29_col6" class="data row29 col6" >0.112708</td>
      <td id="T_0c898_row29_col7" class="data row29 col7" >0.104332</td>
      <td id="T_0c898_row29_col8" class="data row29 col8" >0.104481</td>
      <td id="T_0c898_row29_col9" class="data row29 col9" >0.108234</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row30" class="row_heading level0 row30" >KITCH</th>
      <td id="T_0c898_row30_col0" class="data row30 col0" >-0.016848</td>
      <td id="T_0c898_row30_col1" class="data row30 col1" >-0.015641</td>
      <td id="T_0c898_row30_col2" class="data row30 col2" >-0.015128</td>
      <td id="T_0c898_row30_col3" class="data row30 col3" >-0.014620</td>
      <td id="T_0c898_row30_col4" class="data row30 col4" >-0.015921</td>
      <td id="T_0c898_row30_col5" class="data row30 col5" >-0.015672</td>
      <td id="T_0c898_row30_col6" class="data row30 col6" >-0.016561</td>
      <td id="T_0c898_row30_col7" class="data row30 col7" >-0.013676</td>
      <td id="T_0c898_row30_col8" class="data row30 col8" >-0.016945</td>
      <td id="T_0c898_row30_col9" class="data row30 col9" >-0.017092</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row31" class="row_heading level0 row31" >LIVING</th>
      <td id="T_0c898_row31_col0" class="data row31 col0" >0.005198</td>
      <td id="T_0c898_row31_col1" class="data row31 col1" >0.002324</td>
      <td id="T_0c898_row31_col2" class="data row31 col2" >0.003951</td>
      <td id="T_0c898_row31_col3" class="data row31 col3" >0.004839</td>
      <td id="T_0c898_row31_col4" class="data row31 col4" >0.006106</td>
      <td id="T_0c898_row31_col5" class="data row31 col5" >0.005630</td>
      <td id="T_0c898_row31_col6" class="data row31 col6" >0.003494</td>
      <td id="T_0c898_row31_col7" class="data row31 col7" >0.003993</td>
      <td id="T_0c898_row31_col8" class="data row31 col8" >0.004532</td>
      <td id="T_0c898_row31_col9" class="data row31 col9" >0.004339</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row32" class="row_heading level0 row32" >OTHFN</th>
      <td id="T_0c898_row32_col0" class="data row32 col0" >0.038355</td>
      <td id="T_0c898_row32_col1" class="data row32 col1" >0.036114</td>
      <td id="T_0c898_row32_col2" class="data row32 col2" >0.039843</td>
      <td id="T_0c898_row32_col3" class="data row32 col3" >0.035012</td>
      <td id="T_0c898_row32_col4" class="data row32 col4" >0.038077</td>
      <td id="T_0c898_row32_col5" class="data row32 col5" >0.037492</td>
      <td id="T_0c898_row32_col6" class="data row32 col6" >0.034321</td>
      <td id="T_0c898_row32_col7" class="data row32 col7" >0.037525</td>
      <td id="T_0c898_row32_col8" class="data row32 col8" >0.037721</td>
      <td id="T_0c898_row32_col9" class="data row32 col9" >0.035186</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row33" class="row_heading level0 row33" >RECRM</th>
      <td id="T_0c898_row33_col0" class="data row33 col0" >0.021484</td>
      <td id="T_0c898_row33_col1" class="data row33 col1" >0.021937</td>
      <td id="T_0c898_row33_col2" class="data row33 col2" >0.019965</td>
      <td id="T_0c898_row33_col3" class="data row33 col3" >0.023502</td>
      <td id="T_0c898_row33_col4" class="data row33 col4" >0.024159</td>
      <td id="T_0c898_row33_col5" class="data row33 col5" >0.020679</td>
      <td id="T_0c898_row33_col6" class="data row33 col6" >0.019380</td>
      <td id="T_0c898_row33_col7" class="data row33 col7" >0.020446</td>
      <td id="T_0c898_row33_col8" class="data row33 col8" >0.022242</td>
      <td id="T_0c898_row33_col9" class="data row33 col9" >0.020969</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row34" class="row_heading level0 row34" >CLIMB</th>
      <td id="T_0c898_row34_col0" class="data row34 col0" >0.012317</td>
      <td id="T_0c898_row34_col1" class="data row34 col1" >0.006384</td>
      <td id="T_0c898_row34_col2" class="data row34 col2" >0.011059</td>
      <td id="T_0c898_row34_col3" class="data row34 col3" >0.011721</td>
      <td id="T_0c898_row34_col4" class="data row34 col4" >0.016332</td>
      <td id="T_0c898_row34_col5" class="data row34 col5" >0.016591</td>
      <td id="T_0c898_row34_col6" class="data row34 col6" >0.011285</td>
      <td id="T_0c898_row34_col7" class="data row34 col7" >0.013526</td>
      <td id="T_0c898_row34_col8" class="data row34 col8" >0.013106</td>
      <td id="T_0c898_row34_col9" class="data row34 col9" >0.010781</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row35" class="row_heading level0 row35" >ELEV</th>
      <td id="T_0c898_row35_col0" class="data row35 col0" >0.076095</td>
      <td id="T_0c898_row35_col1" class="data row35 col1" >0.083937</td>
      <td id="T_0c898_row35_col2" class="data row35 col2" >0.078783</td>
      <td id="T_0c898_row35_col3" class="data row35 col3" >0.079432</td>
      <td id="T_0c898_row35_col4" class="data row35 col4" >0.089403</td>
      <td id="T_0c898_row35_col5" class="data row35 col5" >0.078455</td>
      <td id="T_0c898_row35_col6" class="data row35 col6" >0.084076</td>
      <td id="T_0c898_row35_col7" class="data row35 col7" >0.083452</td>
      <td id="T_0c898_row35_col8" class="data row35 col8" >0.082064</td>
      <td id="T_0c898_row35_col9" class="data row35 col9" >0.078135</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row36" class="row_heading level0 row36" >DIRAC</th>
      <td id="T_0c898_row36_col0" class="data row36 col0" >-0.003499</td>
      <td id="T_0c898_row36_col1" class="data row36 col1" >-0.003454</td>
      <td id="T_0c898_row36_col2" class="data row36 col2" >-0.002993</td>
      <td id="T_0c898_row36_col3" class="data row36 col3" >-0.004058</td>
      <td id="T_0c898_row36_col4" class="data row36 col4" >-0.003754</td>
      <td id="T_0c898_row36_col5" class="data row36 col5" >-0.002351</td>
      <td id="T_0c898_row36_col6" class="data row36 col6" >-0.001929</td>
      <td id="T_0c898_row36_col7" class="data row36 col7" >-0.002463</td>
      <td id="T_0c898_row36_col8" class="data row36 col8" >-0.001677</td>
      <td id="T_0c898_row36_col9" class="data row36 col9" >-0.001690</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row37" class="row_heading level0 row37" >PORCH</th>
      <td id="T_0c898_row37_col0" class="data row37 col0" >-0.018848</td>
      <td id="T_0c898_row37_col1" class="data row37 col1" >-0.015829</td>
      <td id="T_0c898_row37_col2" class="data row37 col2" >-0.016723</td>
      <td id="T_0c898_row37_col3" class="data row37 col3" >-0.014969</td>
      <td id="T_0c898_row37_col4" class="data row37 col4" >-0.013677</td>
      <td id="T_0c898_row37_col5" class="data row37 col5" >-0.014311</td>
      <td id="T_0c898_row37_col6" class="data row37 col6" >-0.015005</td>
      <td id="T_0c898_row37_col7" class="data row37 col7" >-0.015080</td>
      <td id="T_0c898_row37_col8" class="data row37 col8" >-0.016535</td>
      <td id="T_0c898_row37_col9" class="data row37 col9" >-0.013887</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row38" class="row_heading level0 row38" >AIRSYS</th>
      <td id="T_0c898_row38_col0" class="data row38 col0" >-0.049124</td>
      <td id="T_0c898_row38_col1" class="data row38 col1" >-0.052072</td>
      <td id="T_0c898_row38_col2" class="data row38 col2" >-0.052840</td>
      <td id="T_0c898_row38_col3" class="data row38 col3" >-0.053260</td>
      <td id="T_0c898_row38_col4" class="data row38 col4" >-0.051097</td>
      <td id="T_0c898_row38_col5" class="data row38 col5" >-0.050265</td>
      <td id="T_0c898_row38_col6" class="data row38 col6" >-0.053449</td>
      <td id="T_0c898_row38_col7" class="data row38 col7" >-0.053212</td>
      <td id="T_0c898_row38_col8" class="data row38 col8" >-0.052109</td>
      <td id="T_0c898_row38_col9" class="data row38 col9" >-0.051032</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row39" class="row_heading level0 row39" >WELL</th>
      <td id="T_0c898_row39_col0" class="data row39 col0" >-0.000000</td>
      <td id="T_0c898_row39_col1" class="data row39 col1" >0.000000</td>
      <td id="T_0c898_row39_col2" class="data row39 col2" >-0.000000</td>
      <td id="T_0c898_row39_col3" class="data row39 col3" >0.000000</td>
      <td id="T_0c898_row39_col4" class="data row39 col4" >-0.000000</td>
      <td id="T_0c898_row39_col5" class="data row39 col5" >-0.000000</td>
      <td id="T_0c898_row39_col6" class="data row39 col6" >-0.000000</td>
      <td id="T_0c898_row39_col7" class="data row39 col7" >-0.000000</td>
      <td id="T_0c898_row39_col8" class="data row39 col8" >-0.000000</td>
      <td id="T_0c898_row39_col9" class="data row39 col9" >-0.000000</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row40" class="row_heading level0 row40" >WELDUS</th>
      <td id="T_0c898_row40_col0" class="data row40 col0" >-0.024269</td>
      <td id="T_0c898_row40_col1" class="data row40 col1" >-0.024428</td>
      <td id="T_0c898_row40_col2" class="data row40 col2" >-0.025118</td>
      <td id="T_0c898_row40_col3" class="data row40 col3" >-0.022449</td>
      <td id="T_0c898_row40_col4" class="data row40 col4" >-0.024388</td>
      <td id="T_0c898_row40_col5" class="data row40 col5" >-0.023465</td>
      <td id="T_0c898_row40_col6" class="data row40 col6" >-0.022414</td>
      <td id="T_0c898_row40_col7" class="data row40 col7" >-0.023391</td>
      <td id="T_0c898_row40_col8" class="data row40 col8" >-0.023995</td>
      <td id="T_0c898_row40_col9" class="data row40 col9" >-0.026031</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row41" class="row_heading level0 row41" >STEAM</th>
      <td id="T_0c898_row41_col0" class="data row41 col0" >0.002214</td>
      <td id="T_0c898_row41_col1" class="data row41 col1" >0.003292</td>
      <td id="T_0c898_row41_col2" class="data row41 col2" >0.000000</td>
      <td id="T_0c898_row41_col3" class="data row41 col3" >0.000000</td>
      <td id="T_0c898_row41_col4" class="data row41 col4" >0.002270</td>
      <td id="T_0c898_row41_col5" class="data row41 col5" >0.002277</td>
      <td id="T_0c898_row41_col6" class="data row41 col6" >0.000000</td>
      <td id="T_0c898_row41_col7" class="data row41 col7" >0.004752</td>
      <td id="T_0c898_row41_col8" class="data row41 col8" >0.002812</td>
      <td id="T_0c898_row41_col9" class="data row41 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row42" class="row_heading level0 row42" >OARSYS</th>
      <td id="T_0c898_row42_col0" class="data row42 col0" >0.000000</td>
      <td id="T_0c898_row42_col1" class="data row42 col1" >0.000000</td>
      <td id="T_0c898_row42_col2" class="data row42 col2" >0.000000</td>
      <td id="T_0c898_row42_col3" class="data row42 col3" >0.000000</td>
      <td id="T_0c898_row42_col4" class="data row42 col4" >0.000000</td>
      <td id="T_0c898_row42_col5" class="data row42 col5" >0.000000</td>
      <td id="T_0c898_row42_col6" class="data row42 col6" >0.000000</td>
      <td id="T_0c898_row42_col7" class="data row42 col7" >0.000000</td>
      <td id="T_0c898_row42_col8" class="data row42 col8" >0.000000</td>
      <td id="T_0c898_row42_col9" class="data row42 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row43" class="row_heading level0 row43" >noise1</th>
      <td id="T_0c898_row43_col0" class="data row43 col0" >0.005424</td>
      <td id="T_0c898_row43_col1" class="data row43 col1" >0.002849</td>
      <td id="T_0c898_row43_col2" class="data row43 col2" >0.006610</td>
      <td id="T_0c898_row43_col3" class="data row43 col3" >0.003614</td>
      <td id="T_0c898_row43_col4" class="data row43 col4" >0.006709</td>
      <td id="T_0c898_row43_col5" class="data row43 col5" >0.003801</td>
      <td id="T_0c898_row43_col6" class="data row43 col6" >0.002519</td>
      <td id="T_0c898_row43_col7" class="data row43 col7" >0.005297</td>
      <td id="T_0c898_row43_col8" class="data row43 col8" >0.002566</td>
      <td id="T_0c898_row43_col9" class="data row43 col9" >0.005736</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row44" class="row_heading level0 row44" >noise2</th>
      <td id="T_0c898_row44_col0" class="data row44 col0" >0.000000</td>
      <td id="T_0c898_row44_col1" class="data row44 col1" >-0.000000</td>
      <td id="T_0c898_row44_col2" class="data row44 col2" >-0.000000</td>
      <td id="T_0c898_row44_col3" class="data row44 col3" >-0.000000</td>
      <td id="T_0c898_row44_col4" class="data row44 col4" >0.000000</td>
      <td id="T_0c898_row44_col5" class="data row44 col5" >0.000000</td>
      <td id="T_0c898_row44_col6" class="data row44 col6" >-0.000000</td>
      <td id="T_0c898_row44_col7" class="data row44 col7" >0.000000</td>
      <td id="T_0c898_row44_col8" class="data row44 col8" >0.000000</td>
      <td id="T_0c898_row44_col9" class="data row44 col9" >-0.000000</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row45" class="row_heading level0 row45" >noise3</th>
      <td id="T_0c898_row45_col0" class="data row45 col0" >0.000000</td>
      <td id="T_0c898_row45_col1" class="data row45 col1" >-0.000000</td>
      <td id="T_0c898_row45_col2" class="data row45 col2" >-0.000000</td>
      <td id="T_0c898_row45_col3" class="data row45 col3" >0.000000</td>
      <td id="T_0c898_row45_col4" class="data row45 col4" >0.000000</td>
      <td id="T_0c898_row45_col5" class="data row45 col5" >-0.000000</td>
      <td id="T_0c898_row45_col6" class="data row45 col6" >-0.000000</td>
      <td id="T_0c898_row45_col7" class="data row45 col7" >-0.000000</td>
      <td id="T_0c898_row45_col8" class="data row45 col8" >-0.000000</td>
      <td id="T_0c898_row45_col9" class="data row45 col9" >-0.000000</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row46" class="row_heading level0 row46" >noise4</th>
      <td id="T_0c898_row46_col0" class="data row46 col0" >0.000000</td>
      <td id="T_0c898_row46_col1" class="data row46 col1" >0.000000</td>
      <td id="T_0c898_row46_col2" class="data row46 col2" >0.000000</td>
      <td id="T_0c898_row46_col3" class="data row46 col3" >0.000000</td>
      <td id="T_0c898_row46_col4" class="data row46 col4" >0.000000</td>
      <td id="T_0c898_row46_col5" class="data row46 col5" >0.001688</td>
      <td id="T_0c898_row46_col6" class="data row46 col6" >0.000000</td>
      <td id="T_0c898_row46_col7" class="data row46 col7" >0.003442</td>
      <td id="T_0c898_row46_col8" class="data row46 col8" >0.000000</td>
      <td id="T_0c898_row46_col9" class="data row46 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row47" class="row_heading level0 row47" >noise5</th>
      <td id="T_0c898_row47_col0" class="data row47 col0" >0.000000</td>
      <td id="T_0c898_row47_col1" class="data row47 col1" >0.000000</td>
      <td id="T_0c898_row47_col2" class="data row47 col2" >0.000000</td>
      <td id="T_0c898_row47_col3" class="data row47 col3" >0.000000</td>
      <td id="T_0c898_row47_col4" class="data row47 col4" >0.000000</td>
      <td id="T_0c898_row47_col5" class="data row47 col5" >0.000000</td>
      <td id="T_0c898_row47_col6" class="data row47 col6" >0.000000</td>
      <td id="T_0c898_row47_col7" class="data row47 col7" >0.000000</td>
      <td id="T_0c898_row47_col8" class="data row47 col8" >0.000000</td>
      <td id="T_0c898_row47_col9" class="data row47 col9" >0.000172</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row48" class="row_heading level0 row48" >noise6</th>
      <td id="T_0c898_row48_col0" class="data row48 col0" >-0.000805</td>
      <td id="T_0c898_row48_col1" class="data row48 col1" >-0.001709</td>
      <td id="T_0c898_row48_col2" class="data row48 col2" >-0.002072</td>
      <td id="T_0c898_row48_col3" class="data row48 col3" >-0.004038</td>
      <td id="T_0c898_row48_col4" class="data row48 col4" >-0.001111</td>
      <td id="T_0c898_row48_col5" class="data row48 col5" >-0.003315</td>
      <td id="T_0c898_row48_col6" class="data row48 col6" >-0.000000</td>
      <td id="T_0c898_row48_col7" class="data row48 col7" >-0.004309</td>
      <td id="T_0c898_row48_col8" class="data row48 col8" >-0.002370</td>
      <td id="T_0c898_row48_col9" class="data row48 col9" >-0.000000</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row49" class="row_heading level0 row49" >noise7</th>
      <td id="T_0c898_row49_col0" class="data row49 col0" >-0.000000</td>
      <td id="T_0c898_row49_col1" class="data row49 col1" >-0.000000</td>
      <td id="T_0c898_row49_col2" class="data row49 col2" >-0.000000</td>
      <td id="T_0c898_row49_col3" class="data row49 col3" >-0.000000</td>
      <td id="T_0c898_row49_col4" class="data row49 col4" >0.000000</td>
      <td id="T_0c898_row49_col5" class="data row49 col5" >0.000000</td>
      <td id="T_0c898_row49_col6" class="data row49 col6" >-0.000000</td>
      <td id="T_0c898_row49_col7" class="data row49 col7" >-0.000000</td>
      <td id="T_0c898_row49_col8" class="data row49 col8" >-0.000000</td>
      <td id="T_0c898_row49_col9" class="data row49 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row50" class="row_heading level0 row50" >noise8</th>
      <td id="T_0c898_row50_col0" class="data row50 col0" >0.003441</td>
      <td id="T_0c898_row50_col1" class="data row50 col1" >0.009192</td>
      <td id="T_0c898_row50_col2" class="data row50 col2" >0.004116</td>
      <td id="T_0c898_row50_col3" class="data row50 col3" >0.002452</td>
      <td id="T_0c898_row50_col4" class="data row50 col4" >0.006297</td>
      <td id="T_0c898_row50_col5" class="data row50 col5" >0.004724</td>
      <td id="T_0c898_row50_col6" class="data row50 col6" >0.005267</td>
      <td id="T_0c898_row50_col7" class="data row50 col7" >0.003611</td>
      <td id="T_0c898_row50_col8" class="data row50 col8" >0.005380</td>
      <td id="T_0c898_row50_col9" class="data row50 col9" >0.002053</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row51" class="row_heading level0 row51" >noise9</th>
      <td id="T_0c898_row51_col0" class="data row51 col0" >-0.000000</td>
      <td id="T_0c898_row51_col1" class="data row51 col1" >0.000000</td>
      <td id="T_0c898_row51_col2" class="data row51 col2" >-0.000000</td>
      <td id="T_0c898_row51_col3" class="data row51 col3" >-0.000000</td>
      <td id="T_0c898_row51_col4" class="data row51 col4" >-0.000258</td>
      <td id="T_0c898_row51_col5" class="data row51 col5" >-0.000000</td>
      <td id="T_0c898_row51_col6" class="data row51 col6" >-0.000000</td>
      <td id="T_0c898_row51_col7" class="data row51 col7" >-0.000000</td>
      <td id="T_0c898_row51_col8" class="data row51 col8" >-0.000000</td>
      <td id="T_0c898_row51_col9" class="data row51 col9" >-0.000000</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row52" class="row_heading level0 row52" >noise10</th>
      <td id="T_0c898_row52_col0" class="data row52 col0" >-0.000000</td>
      <td id="T_0c898_row52_col1" class="data row52 col1" >-0.000000</td>
      <td id="T_0c898_row52_col2" class="data row52 col2" >-0.000000</td>
      <td id="T_0c898_row52_col3" class="data row52 col3" >-0.000000</td>
      <td id="T_0c898_row52_col4" class="data row52 col4" >-0.000000</td>
      <td id="T_0c898_row52_col5" class="data row52 col5" >-0.000000</td>
      <td id="T_0c898_row52_col6" class="data row52 col6" >-0.000000</td>
      <td id="T_0c898_row52_col7" class="data row52 col7" >-0.000000</td>
      <td id="T_0c898_row52_col8" class="data row52 col8" >-0.000021</td>
      <td id="T_0c898_row52_col9" class="data row52 col9" >-0.000000</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row53" class="row_heading level0 row53" >noise11</th>
      <td id="T_0c898_row53_col0" class="data row53 col0" >-0.008055</td>
      <td id="T_0c898_row53_col1" class="data row53 col1" >-0.004641</td>
      <td id="T_0c898_row53_col2" class="data row53 col2" >-0.005265</td>
      <td id="T_0c898_row53_col3" class="data row53 col3" >-0.002612</td>
      <td id="T_0c898_row53_col4" class="data row53 col4" >-0.007669</td>
      <td id="T_0c898_row53_col5" class="data row53 col5" >-0.005447</td>
      <td id="T_0c898_row53_col6" class="data row53 col6" >-0.007216</td>
      <td id="T_0c898_row53_col7" class="data row53 col7" >-0.006012</td>
      <td id="T_0c898_row53_col8" class="data row53 col8" >-0.007707</td>
      <td id="T_0c898_row53_col9" class="data row53 col9" >-0.003743</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row54" class="row_heading level0 row54" >noise12</th>
      <td id="T_0c898_row54_col0" class="data row54 col0" >-0.006468</td>
      <td id="T_0c898_row54_col1" class="data row54 col1" >-0.007073</td>
      <td id="T_0c898_row54_col2" class="data row54 col2" >-0.003561</td>
      <td id="T_0c898_row54_col3" class="data row54 col3" >-0.002931</td>
      <td id="T_0c898_row54_col4" class="data row54 col4" >-0.006589</td>
      <td id="T_0c898_row54_col5" class="data row54 col5" >-0.003944</td>
      <td id="T_0c898_row54_col6" class="data row54 col6" >-0.005517</td>
      <td id="T_0c898_row54_col7" class="data row54 col7" >-0.002839</td>
      <td id="T_0c898_row54_col8" class="data row54 col8" >-0.007282</td>
      <td id="T_0c898_row54_col9" class="data row54 col9" >-0.005623</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row55" class="row_heading level0 row55" >noise13</th>
      <td id="T_0c898_row55_col0" class="data row55 col0" >0.000000</td>
      <td id="T_0c898_row55_col1" class="data row55 col1" >0.000000</td>
      <td id="T_0c898_row55_col2" class="data row55 col2" >0.000000</td>
      <td id="T_0c898_row55_col3" class="data row55 col3" >0.000000</td>
      <td id="T_0c898_row55_col4" class="data row55 col4" >0.000212</td>
      <td id="T_0c898_row55_col5" class="data row55 col5" >0.000000</td>
      <td id="T_0c898_row55_col6" class="data row55 col6" >0.000000</td>
      <td id="T_0c898_row55_col7" class="data row55 col7" >0.000000</td>
      <td id="T_0c898_row55_col8" class="data row55 col8" >0.002019</td>
      <td id="T_0c898_row55_col9" class="data row55 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row56" class="row_heading level0 row56" >noise14</th>
      <td id="T_0c898_row56_col0" class="data row56 col0" >-0.000124</td>
      <td id="T_0c898_row56_col1" class="data row56 col1" >-0.000000</td>
      <td id="T_0c898_row56_col2" class="data row56 col2" >0.000000</td>
      <td id="T_0c898_row56_col3" class="data row56 col3" >-0.000000</td>
      <td id="T_0c898_row56_col4" class="data row56 col4" >-0.000000</td>
      <td id="T_0c898_row56_col5" class="data row56 col5" >-0.000000</td>
      <td id="T_0c898_row56_col6" class="data row56 col6" >-0.000000</td>
      <td id="T_0c898_row56_col7" class="data row56 col7" >0.000000</td>
      <td id="T_0c898_row56_col8" class="data row56 col8" >-0.000000</td>
      <td id="T_0c898_row56_col9" class="data row56 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row57" class="row_heading level0 row57" >noise15</th>
      <td id="T_0c898_row57_col0" class="data row57 col0" >0.002332</td>
      <td id="T_0c898_row57_col1" class="data row57 col1" >0.004505</td>
      <td id="T_0c898_row57_col2" class="data row57 col2" >0.004589</td>
      <td id="T_0c898_row57_col3" class="data row57 col3" >0.002373</td>
      <td id="T_0c898_row57_col4" class="data row57 col4" >0.004535</td>
      <td id="T_0c898_row57_col5" class="data row57 col5" >0.003080</td>
      <td id="T_0c898_row57_col6" class="data row57 col6" >0.001490</td>
      <td id="T_0c898_row57_col7" class="data row57 col7" >0.004166</td>
      <td id="T_0c898_row57_col8" class="data row57 col8" >0.004509</td>
      <td id="T_0c898_row57_col9" class="data row57 col9" >0.002482</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row58" class="row_heading level0 row58" >noise16</th>
      <td id="T_0c898_row58_col0" class="data row58 col0" >0.000000</td>
      <td id="T_0c898_row58_col1" class="data row58 col1" >0.000000</td>
      <td id="T_0c898_row58_col2" class="data row58 col2" >0.000000</td>
      <td id="T_0c898_row58_col3" class="data row58 col3" >0.000000</td>
      <td id="T_0c898_row58_col4" class="data row58 col4" >0.000000</td>
      <td id="T_0c898_row58_col5" class="data row58 col5" >0.000000</td>
      <td id="T_0c898_row58_col6" class="data row58 col6" >-0.000000</td>
      <td id="T_0c898_row58_col7" class="data row58 col7" >0.000000</td>
      <td id="T_0c898_row58_col8" class="data row58 col8" >0.000000</td>
      <td id="T_0c898_row58_col9" class="data row58 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row59" class="row_heading level0 row59" >noise17</th>
      <td id="T_0c898_row59_col0" class="data row59 col0" >-0.002321</td>
      <td id="T_0c898_row59_col1" class="data row59 col1" >-0.001854</td>
      <td id="T_0c898_row59_col2" class="data row59 col2" >-0.003085</td>
      <td id="T_0c898_row59_col3" class="data row59 col3" >-0.001049</td>
      <td id="T_0c898_row59_col4" class="data row59 col4" >-0.004635</td>
      <td id="T_0c898_row59_col5" class="data row59 col5" >-0.000000</td>
      <td id="T_0c898_row59_col6" class="data row59 col6" >-0.000465</td>
      <td id="T_0c898_row59_col7" class="data row59 col7" >-0.001222</td>
      <td id="T_0c898_row59_col8" class="data row59 col8" >-0.002072</td>
      <td id="T_0c898_row59_col9" class="data row59 col9" >-0.002135</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row60" class="row_heading level0 row60" >noise18</th>
      <td id="T_0c898_row60_col0" class="data row60 col0" >0.000274</td>
      <td id="T_0c898_row60_col1" class="data row60 col1" >0.000000</td>
      <td id="T_0c898_row60_col2" class="data row60 col2" >0.000000</td>
      <td id="T_0c898_row60_col3" class="data row60 col3" >0.000704</td>
      <td id="T_0c898_row60_col4" class="data row60 col4" >0.000000</td>
      <td id="T_0c898_row60_col5" class="data row60 col5" >0.000000</td>
      <td id="T_0c898_row60_col6" class="data row60 col6" >0.000000</td>
      <td id="T_0c898_row60_col7" class="data row60 col7" >0.000000</td>
      <td id="T_0c898_row60_col8" class="data row60 col8" >0.001272</td>
      <td id="T_0c898_row60_col9" class="data row60 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row61" class="row_heading level0 row61" >noise19</th>
      <td id="T_0c898_row61_col0" class="data row61 col0" >0.000000</td>
      <td id="T_0c898_row61_col1" class="data row61 col1" >0.000000</td>
      <td id="T_0c898_row61_col2" class="data row61 col2" >-0.000000</td>
      <td id="T_0c898_row61_col3" class="data row61 col3" >-0.000000</td>
      <td id="T_0c898_row61_col4" class="data row61 col4" >-0.000000</td>
      <td id="T_0c898_row61_col5" class="data row61 col5" >-0.000000</td>
      <td id="T_0c898_row61_col6" class="data row61 col6" >0.000000</td>
      <td id="T_0c898_row61_col7" class="data row61 col7" >-0.000000</td>
      <td id="T_0c898_row61_col8" class="data row61 col8" >-0.000000</td>
      <td id="T_0c898_row61_col9" class="data row61 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row62" class="row_heading level0 row62" >noise20</th>
      <td id="T_0c898_row62_col0" class="data row62 col0" >-0.000904</td>
      <td id="T_0c898_row62_col1" class="data row62 col1" >-0.002203</td>
      <td id="T_0c898_row62_col2" class="data row62 col2" >-0.001322</td>
      <td id="T_0c898_row62_col3" class="data row62 col3" >-0.000250</td>
      <td id="T_0c898_row62_col4" class="data row62 col4" >-0.000000</td>
      <td id="T_0c898_row62_col5" class="data row62 col5" >-0.000180</td>
      <td id="T_0c898_row62_col6" class="data row62 col6" >-0.001053</td>
      <td id="T_0c898_row62_col7" class="data row62 col7" >-0.001291</td>
      <td id="T_0c898_row62_col8" class="data row62 col8" >-0.005082</td>
      <td id="T_0c898_row62_col9" class="data row62 col9" >-0.000000</td>
    </tr>
    <tr>
      <th id="T_0c898_level0_row63" class="row_heading level0 row63" >ranking</th>
      <td id="T_0c898_row63_col0" class="data row63 col0" >-0.002614</td>
      <td id="T_0c898_row63_col1" class="data row63 col1" >-0.003632</td>
      <td id="T_0c898_row63_col2" class="data row63 col2" >-0.000309</td>
      <td id="T_0c898_row63_col3" class="data row63 col3" >-0.001322</td>
      <td id="T_0c898_row63_col4" class="data row63 col4" >-0.002222</td>
      <td id="T_0c898_row63_col5" class="data row63 col5" >-0.000030</td>
      <td id="T_0c898_row63_col6" class="data row63 col6" >-0.001472</td>
      <td id="T_0c898_row63_col7" class="data row63 col7" >-0.002578</td>
      <td id="T_0c898_row63_col8" class="data row63 col8" >-0.000000</td>
      <td id="T_0c898_row63_col9" class="data row63 col9" >-0.000000</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>As we have seen above, any interpretation needs to take into account the joint distribution of covariates. One possible heuristic is to consider <strong>data-driven subgroups</strong>. For example, we can analyze what differentiates observations whose predictions are high from those whose predictions are low. The following code estimates a flexible Lasso model with splines, ranks the observations into a few subgroups according to their predicted outcomes, and then estimates the average covariate value for each subgroup.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">itertools</span>
<span class="n">nobs</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">nfold</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="c1"># Define folds indices </span>
<span class="n">list_1</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nfold</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span><span class="o">*</span><span class="n">nobs</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">nobs</span><span class="p">,</span><span class="n">nobs</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">foldid</span> <span class="o">=</span> <span class="p">[</span><span class="n">list_1</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">sample</span><span class="p">]</span>

    <span class="c1"># Create split function(similar to R)</span>
<span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
    <span class="n">count</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span> <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">compress</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">el</span> <span class="o">==</span> <span class="n">i</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">f</span><span class="p">)))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">count</span><span class="p">)</span> <span class="p">)</span> 

    <span class="c1"># Split observation indices into folds </span>
<span class="n">list_2</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nobs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
<span class="n">I</span> <span class="o">=</span> <span class="n">split</span><span class="p">(</span><span class="n">list_2</span><span class="p">,</span> <span class="n">foldid</span><span class="p">)</span>


<span class="n">lasso_coef_rank</span><span class="o">=</span><span class="p">[]</span>
<span class="n">lasso_pred</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">I</span><span class="p">)):</span>
        <span class="c1"># Split data - index to keep are in mask as booleans</span>
        <span class="n">include_idx</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">I</span><span class="p">[</span><span class="n">b</span><span class="p">])</span>  <span class="c1">#Here should go I[b] Set is more efficient, but doesn&#39;t reorder your elements if that is desireable</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([(</span><span class="n">i</span> <span class="ow">in</span> <span class="n">include_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))])</span>

        <span class="c1"># Lasso regression, excluding folds selected </span>
        
        <span class="n">lassocv</span> <span class="o">=</span> <span class="n">LassoCV</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">lassocv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scale_X</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">])</span>
        <span class="n">lasso_coef_rank</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lassocv</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
        <span class="n">lasso_pred</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lassocv</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">scale_X</span><span class="p">[</span><span class="n">mask</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">lasso_pred</span>

<span class="n">df_1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]:</span>
    <span class="n">df_2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">y_hat</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    
    <span class="n">b</span> <span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">df_2</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">bins</span> <span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">df_2</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">df_2</span><span class="p">,</span><span class="mi">25</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">df_2</span><span class="p">,</span><span class="mi">50</span><span class="p">),</span>
           <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">df_2</span><span class="p">,</span><span class="mi">75</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">df_2</span><span class="p">,</span><span class="mi">100</span><span class="p">)],</span> <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
    
    <span class="n">df_1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_1</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>
<span class="n">df_1</span> <span class="o">=</span><span class="n">df_1</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">factorize</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">df_1</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span><span class="s1">&#39;ranking&#39;</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df_1</span> <span class="o">=</span><span class="n">df_1</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;index&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">X</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">df_1</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="p">[</span><span class="s1">&#39;ranking&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_frame</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="k">for</span> <span class="n">var_name</span> <span class="ow">in</span> <span class="n">covariates</span><span class="p">:</span>
    <span class="n">form</span> <span class="o">=</span> <span class="n">var_name</span> <span class="o">+</span> <span class="s2">&quot; ~ &quot;</span> <span class="o">+</span> <span class="s2">&quot;0&quot;</span> <span class="o">+</span> <span class="s2">&quot;+&quot;</span> <span class="o">+</span> <span class="s2">&quot;C(ranking)&quot;</span>
    <span class="n">df1</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="n">form</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">cov_type</span> <span class="o">=</span> <span class="s1">&#39;HC2&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">summary2</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">5</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="c1">#iloc to stay with rankings 0,1,2,3</span>
    <span class="n">df1</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;covariate&#39;</span><span class="p">,</span> <span class="n">var_name</span><span class="p">)</span>
    <span class="n">df1</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s1">&#39;ranking&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;G1&#39;</span><span class="p">,</span><span class="s1">&#39;G2&#39;</span><span class="p">,</span><span class="s1">&#39;G3&#39;</span><span class="p">,</span><span class="s1">&#39;G4&#39;</span><span class="p">])</span>
    <span class="n">df1</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;scaling&#39;</span><span class="p">,</span>
               <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">((</span><span class="n">df1</span><span class="p">[</span><span class="s1">&#39;Coef.&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">df1</span><span class="p">[</span><span class="s1">&#39;Coef.&#39;</span><span class="p">]))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">df1</span><span class="p">[</span><span class="s1">&#39;Coef.&#39;</span><span class="p">]))))</span>
    <span class="n">df1</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;variation&#39;</span><span class="p">,</span>
               <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">df1</span><span class="p">[</span><span class="s1">&#39;Coef.&#39;</span><span class="p">])</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">var_name</span><span class="p">]))</span>
    <span class="n">label</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">):</span>
        <span class="n">label</span> <span class="o">+=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">df1</span><span class="p">[</span><span class="s1">&#39;Coef.&#39;</span><span class="p">][</span><span class="n">j</span><span class="p">],</span><span class="mi">3</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot; (&quot;</span> 
                  <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">df1</span><span class="p">[</span><span class="s1">&#39;Std.Err.&#39;</span><span class="p">][</span><span class="n">j</span><span class="p">],</span><span class="mi">3</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;)&quot;</span><span class="p">]</span>
    <span class="n">df1</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">df1</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;index&#39;</span><span class="p">])</span>
    <span class="n">index</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">):</span>
        <span class="n">index</span> <span class="o">+=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">df1</span><span class="p">[</span><span class="s1">&#39;covariate&#39;</span><span class="p">][</span><span class="n">m</span><span class="p">])</span> <span class="o">+</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="s2">&quot;ranking&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">m</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Index</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
    <span class="n">df1</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
    <span class="n">data_frame</span> <span class="o">=</span> <span class="n">data_frame</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">df1</span><span class="p">)</span>
<span class="n">data_frame</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">labels_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">df_mask</span> <span class="o">=</span> <span class="n">data_frame</span><span class="p">[</span><span class="s1">&#39;ranking&#39;</span><span class="p">]</span><span class="o">==</span><span class="sa">f</span><span class="s2">&quot;G</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">filtered_df</span> <span class="o">=</span> <span class="n">data_frame</span><span class="p">[</span><span class="n">df_mask</span><span class="p">]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;index&#39;</span><span class="p">])</span>
    <span class="n">labels_data</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;ranking</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">filtered_df</span><span class="p">[[</span><span class="s1">&#39;labels&#39;</span><span class="p">]]</span>
<span class="n">labels_data</span> <span class="o">=</span> <span class="n">labels_data</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Index</span><span class="p">(</span><span class="n">covariates</span><span class="p">))</span>
<span class="n">labels_data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ranking1</th>
      <th>ranking2</th>
      <th>ranking3</th>
      <th>ranking4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>LOT</th>
      <td>49713.31 (1473.048)</td>
      <td>46479.968 (1390.394)</td>
      <td>47806.63 (1427.658)</td>
      <td>47612.513 (1393.569)</td>
    </tr>
    <tr>
      <th>UNITSF</th>
      <td>2415.869 (24.944)</td>
      <td>2434.834 (24.249)</td>
      <td>2397.706 (23.467)</td>
      <td>2471.907 (26.208)</td>
    </tr>
    <tr>
      <th>BUILT</th>
      <td>1972.286 (0.301)</td>
      <td>1974.925 (0.294)</td>
      <td>1973.672 (0.299)</td>
      <td>1973.017 (0.299)</td>
    </tr>
    <tr>
      <th>BATHS</th>
      <td>1.918 (0.009)</td>
      <td>1.975 (0.009)</td>
      <td>1.946 (0.009)</td>
      <td>1.928 (0.009)</td>
    </tr>
    <tr>
      <th>BEDRMS</th>
      <td>3.218 (0.01)</td>
      <td>3.258 (0.01)</td>
      <td>3.251 (0.01)</td>
      <td>3.243 (0.01)</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>noise16</th>
      <td>0.499 (0.003)</td>
      <td>0.502 (0.003)</td>
      <td>0.498 (0.003)</td>
      <td>0.505 (0.003)</td>
    </tr>
    <tr>
      <th>noise17</th>
      <td>0.501 (0.003)</td>
      <td>0.498 (0.003)</td>
      <td>0.502 (0.003)</td>
      <td>0.498 (0.003)</td>
    </tr>
    <tr>
      <th>noise18</th>
      <td>0.502 (0.003)</td>
      <td>0.499 (0.003)</td>
      <td>0.5 (0.003)</td>
      <td>0.5 (0.003)</td>
    </tr>
    <tr>
      <th>noise19</th>
      <td>0.504 (0.003)</td>
      <td>0.502 (0.003)</td>
      <td>0.498 (0.003)</td>
      <td>0.497 (0.003)</td>
    </tr>
    <tr>
      <th>noise20</th>
      <td>0.502 (0.003)</td>
      <td>0.496 (0.003)</td>
      <td>0.501 (0.003)</td>
      <td>0.5 (0.003)</td>
    </tr>
  </tbody>
</table>
<p>63 rows × 4 columns</p>
</div></div></div>
</div>
<p>The next heatmap visualizes the results. Note how observations ranked higher (i.e., were predicted to have higher prices) have more bedrooms and baths, were built more recently, have fewer cracks, and so on. The next snippet of code displays the average covariate per group along with each standard errors. The rows are ordered according to <span class="math notranslate nohighlight">\(Var(E[X_{ij} | G_i) / Var(X_i)\)</span>, where <span class="math notranslate nohighlight">\(G_i\)</span> denotes the ranking. This is a rough normalized measure of how much variation is “explained” by group membership <span class="math notranslate nohighlight">\(G_i\)</span>. Brighter colors indicate larger values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">new_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">df_mask</span> <span class="o">=</span> <span class="n">data_frame</span><span class="p">[</span><span class="s1">&#39;ranking&#39;</span><span class="p">]</span><span class="o">==</span><span class="sa">f</span><span class="s2">&quot;G</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">filtered_df</span> <span class="o">=</span> <span class="n">data_frame</span><span class="p">[</span><span class="n">df_mask</span><span class="p">]</span>
    <span class="n">new_data</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="sa">f</span><span class="s2">&quot;G</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">filtered_df</span><span class="p">[[</span><span class="s1">&#39;scaling&#39;</span><span class="p">]])</span>
<span class="n">new_data</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">features</span> <span class="o">=</span> <span class="n">covariates</span>
<span class="n">ranks</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;G1&#39;</span><span class="p">,</span><span class="s1">&#39;G2&#39;</span><span class="p">,</span><span class="s1">&#39;G3&#39;</span><span class="p">,</span><span class="s1">&#39;G4&#39;</span><span class="p">]</span>
<span class="n">harvest</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">new_data</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">labels_hm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">labels_data</span><span class="p">))</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>

<span class="c1"># getting the original colormap using cm.get_cmap() function</span>
<span class="n">orig_map</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;copper&#39;</span><span class="p">)</span>
  
<span class="c1"># reversing the original colormap using reversed() function</span>
<span class="n">reversed_map</span> <span class="o">=</span> <span class="n">orig_map</span><span class="o">.</span><span class="n">reversed</span><span class="p">()</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">harvest</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">reversed_map</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>

<span class="c1"># make bar</span>
<span class="n">bar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
  
<span class="c1"># show plot with labels</span>
<span class="n">bar</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="s1">&#39;scaling&#39;</span><span class="p">)</span>

 
<span class="c1"># Setting the labels</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ranks</span><span class="p">)))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)))</span>
<span class="c1"># labeling respective list entries</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">ranks</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

<span class="c1"># Rotate the tick labels and set their alignment.</span>
<span class="n">plt</span><span class="o">.</span><span class="n">setp</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">get_xticklabels</span><span class="p">(),</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;right&quot;</span><span class="p">,</span>
         <span class="n">rotation_mode</span><span class="o">=</span><span class="s2">&quot;anchor&quot;</span><span class="p">)</span>

<span class="c1"># Creating text annotations by using for loop</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ranks</span><span class="p">)):</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">labels_hm</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span>
                       <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;w&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Average covariate values within group (based on prediction ranking)&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>


<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_introduction_to_machine_learning_64_0.png" src="../_images/2_introduction_to_machine_learning_64_0.png" />
</div>
</div>
<p>As we just saw above, houses that have, e.g., been built more recently (<code class="docutils literal notranslate"><span class="pre">BUILT</span></code>), have more baths (<code class="docutils literal notranslate"><span class="pre">BATHS</span></code>) are associated with larger price predictions.</p>
<p>This sort of interpretation exercise did not rely on reading any coefficients, and in fact it could also be done using any other flexible method, including decisions trees and forests.</p>
</section>
<section id="decision-tree">
<h3><span class="section-number">2.2.2. </span>Decision Tree<a class="headerlink" href="#decision-tree" title="Permalink to this headline">#</a></h3>
<p>This next class of algorithms divides the covariate space into “regions” and estimates a constant prediction within each region.</p>
<p>To estimate a decision tree, we following a recursive partition algorithm. At each stage, we select one variable <span class="math notranslate nohighlight">\(j\)</span> and one split point
<span class="math notranslate nohighlight">\(s\)</span>, and divide the observations into “left” and “right” subsets, depending on whether <span class="math notranslate nohighlight">\(X_{ij} \leq s\)</span> or <span class="math notranslate nohighlight">\(X_{ij} &gt; s\)</span>. For regression problems, the variable and split points are often selected so that the sum of the variances of the outcome variable in each “child” subset is smallest. For classification problems, we split to separate the classes. Then, for each child, we separately repeat the process of finding variables and split points. This continues until a minimum subset size is reached, or improvement falls below some threshold.</p>
<p>At prediction time, to find the predictions for some point <span class="math notranslate nohighlight">\(x\)</span>, we just follow the tree we just built, going left or right according to the selected variables and split points, until we reach a terminal node. Then, for regression problems, the predicted value at some point <span class="math notranslate nohighlight">\(x\)</span> is the average outcome of the observations in the same partition as the point <span class="math notranslate nohighlight">\(x\)</span>. For classification problems, we output the majority class in the node.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">import</span> <span class="nn">graphviz</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">export_graphviz</span> 
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">Series</span>
<span class="kn">from</span> <span class="nn">simple_colors</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Here we define our X and Y variable</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">outcome</span><span class="p">]</span>
<span class="n">XX</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">covariates</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># we split data in train and test</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">XX</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(),</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1">#x_train, x_test, y_train, y_test = train_test_split(XX.to_numpy(), Y, test_size=.3)</span>
<span class="n">tree1</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>At this point, we have not constrained the complexity of the tree in any way, so it’s likely too deep and probably overfits. Here’s a plot of what we have so far (without bothering to label the splits to avoid clutter).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">tree</span><span class="o">.</span><span class="n">plot_tree</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Text(0.6937715956946345, 0.96875, &#39;X[12] &lt;= 0.0\nsquared_error = 0.981\nsamples = 20108\nvalue = 11.814&#39;),
 Text(0.45393733459285945, 0.90625, &#39;X[1] &lt;= 2436.5\nsquared_error = 0.774\nsamples = 19386\nvalue = 11.889&#39;),
 Text(0.23878549020890355, 0.84375, &#39;X[3] &lt;= 1.5\nsquared_error = 0.631\nsamples = 13895\nvalue = 11.687&#39;),
 Text(0.11155233658026083, 0.78125, &#39;X[19] &lt;= 1.5\nsquared_error = 0.705\nsamples = 5094\nvalue = 11.392&#39;),
 Text(0.05217431774271454, 0.71875, &#39;X[29] &lt;= 0.5\nsquared_error = 0.677\nsamples = 2626\nvalue = 11.544&#39;),
 Text(0.02204757687972951, 0.65625, &#39;X[14] &lt;= -3.0\nsquared_error = 0.82\nsamples = 1133\nvalue = 11.421&#39;),
 Text(0.0019320560296248591, 0.59375, &#39;X[47] &lt;= 0.904\nsquared_error = 15.873\nsamples = 7\nvalue = 9.561&#39;),
 Text(0.0016100466913540493, 0.53125, &#39;X[30] &lt;= 1.5\nsquared_error = 0.743\nsamples = 6\nvalue = 11.155&#39;),
 Text(0.0012880373530832394, 0.46875, &#39;X[44] &lt;= 0.392\nsquared_error = 0.256\nsamples = 5\nvalue = 10.829&#39;),
 Text(0.0006440186765416197, 0.40625, &#39;X[1] &lt;= 1536.402\nsquared_error = 0.012\nsamples = 2\nvalue = 10.234&#39;),
 Text(0.00032200933827080985, 0.34375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.342&#39;),
 Text(0.0009660280148124296, 0.34375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.127&#39;),
 Text(0.0019320560296248591, 0.40625, &#39;X[57] &lt;= 0.52\nsquared_error = 0.027\nsamples = 3\nvalue = 11.226&#39;),
 Text(0.0016100466913540493, 0.34375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.002&#39;),
 Text(0.0022540653678956688, 0.34375, &#39;X[58] &lt;= 0.333\nsquared_error = 0.002\nsamples = 2\nvalue = 11.337&#39;),
 Text(0.0019320560296248591, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.385&#39;),
 Text(0.002576074706166479, 0.28125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.29&#39;),
 Text(0.0019320560296248591, 0.46875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.782&#39;),
 Text(0.0022540653678956688, 0.53125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 0.0&#39;),
 Text(0.04216309772983416, 0.59375, &#39;X[1] &lt;= 2431.402\nsquared_error = 0.705\nsamples = 1126\nvalue = 11.432&#39;),
 Text(0.021499154725487038, 0.53125, &#39;X[15] &lt;= 1.5\nsquared_error = 0.461\nsamples = 976\nvalue = 11.488&#39;),
 Text(0.00612824021896635, 0.46875, &#39;X[49] &lt;= 0.024\nsquared_error = 1.171\nsamples = 184\nvalue = 11.304&#39;),
 Text(0.0038641120592497183, 0.40625, &#39;X[1] &lt;= 1495.0\nsquared_error = 21.608\nsamples = 5\nvalue = 9.292&#39;),
 Text(0.003542102720978908, 0.34375, &#39;X[1] &lt;= 750.0\nsquared_error = 0.028\nsamples = 4\nvalue = 11.615&#39;),
 Text(0.0032200933827080985, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.35&#39;),
 Text(0.0038641120592497183, 0.28125, &#39;X[49] &lt;= 0.003\nsquared_error = 0.006\nsamples = 3\nvalue = 11.703&#39;),
 Text(0.003542102720978908, 0.21875, &#39;X[38] &lt;= 1.5\nsquared_error = 0.0\nsamples = 2\nvalue = 11.756&#39;),
 Text(0.0032200933827080985, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.775&#39;),
 Text(0.0038641120592497183, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.736&#39;),
 Text(0.004186121397520528, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.599&#39;),
 Text(0.004186121397520528, 0.34375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 0.0&#39;),
 Text(0.008392368378682982, 0.40625, &#39;X[50] &lt;= 0.067\nsquared_error = 0.484\nsamples = 179\nvalue = 11.361&#39;),
 Text(0.0056351634197391726, 0.34375, &#39;X[48] &lt;= 0.202\nsquared_error = 4.939\nsamples = 7\nvalue = 10.455&#39;),
 Text(0.005313154081468363, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 5.075&#39;),
 Text(0.005957172758009982, 0.28125, &#39;X[61] &lt;= 0.559\nsquared_error = 0.136\nsamples = 6\nvalue = 11.351&#39;),
 Text(0.005152149412332958, 0.21875, &#39;X[2] &lt;= 1945.0\nsquared_error = 0.049\nsamples = 4\nvalue = 11.559&#39;),
 Text(0.0045081307357913375, 0.15625, &#39;X[55] &lt;= 0.564\nsquared_error = 0.019\nsamples = 2\nvalue = 11.364&#39;),
 Text(0.004186121397520528, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.503&#39;),
 Text(0.004830140074062148, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.225&#39;),
 Text(0.005796168088874577, 0.15625, &#39;X[6] &lt;= 4.0\nsquared_error = 0.003\nsamples = 2\nvalue = 11.754&#39;),
 Text(0.005474158750603767, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.695&#39;),
 Text(0.0061181774271453875, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.813&#39;),
 Text(0.006762196103687007, 0.21875, &#39;X[50] &lt;= 0.045\nsquared_error = 0.049\nsamples = 2\nvalue = 10.935&#39;),
 Text(0.006440186765416197, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.156&#39;),
 Text(0.007084205441957816, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.714&#39;),
 Text(0.01114957333762679, 0.34375, &#39;X[1] &lt;= 775.0\nsquared_error = 0.268\nsamples = 172\nvalue = 11.397&#39;),
 Text(0.008211238125905651, 0.28125, &#39;X[46] &lt;= 0.405\nsquared_error = 0.917\nsamples = 7\nvalue = 10.553&#39;),
 Text(0.007889228787634841, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 8.412&#39;),
 Text(0.008533247464176462, 0.21875, &#39;X[50] &lt;= 0.602\nsquared_error = 0.178\nsamples = 6\nvalue = 10.91&#39;),
 Text(0.0077282241184994365, 0.15625, &#39;X[49] &lt;= 0.622\nsquared_error = 0.055\nsamples = 2\nvalue = 10.362&#39;),
 Text(0.007406214780228627, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.597&#39;),
 Text(0.008050233456770247, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.127&#39;),
 Text(0.009338270809853486, 0.15625, &#39;X[0] &lt;= 6500.0\nsquared_error = 0.014\nsamples = 4\nvalue = 11.184&#39;),
 Text(0.008694252133311866, 0.09375, &#39;X[46] &lt;= 0.652\nsquared_error = 0.006\nsamples = 2\nvalue = 11.079&#39;),
 Text(0.008372242795041056, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.002&#39;),
 Text(0.009016261471582675, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.156&#39;),
 Text(0.009982289486395105, 0.09375, &#39;X[24] &lt;= 1.5\nsquared_error = 0.0\nsamples = 2\nvalue = 11.29&#39;),
 Text(0.009660280148124296, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.29&#39;),
 Text(0.010304298824665915, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.29&#39;),
 Text(0.014087908549347931, 0.28125, &#39;X[57] &lt;= 0.867\nsquared_error = 0.208\nsamples = 165\nvalue = 11.433&#39;),
 Text(0.01175334084688456, 0.21875, &#39;X[52] &lt;= 0.013\nsquared_error = 0.195\nsamples = 140\nvalue = 11.481&#39;),
 Text(0.010948317501207535, 0.15625, &#39;X[47] &lt;= 0.342\nsquared_error = 0.12\nsamples = 2\nvalue = 10.473&#39;),
 Text(0.010626308162936726, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.82&#39;),
 Text(0.011270326839478345, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.127&#39;),
 Text(0.012558364192561584, 0.15625, &#39;X[1] &lt;= 912.5\nsquared_error = 0.181\nsamples = 138\nvalue = 11.496&#39;),
 Text(0.011914345516019964, 0.09375, &#39;X[60] &lt;= 0.831\nsquared_error = 0.122\nsamples = 14\nvalue = 11.195&#39;),
 Text(0.011592336177749154, 0.03125, &#39;squared_error = 0.064\nsamples = 12\nvalue = 11.301&#39;),
 Text(0.012236354854290775, 0.03125, &#39;squared_error = 0.002\nsamples = 2\nvalue = 10.558&#39;),
 Text(0.013202382869103205, 0.09375, &#39;X[8] &lt;= 3.5\nsquared_error = 0.176\nsamples = 124\nvalue = 11.53&#39;),
 Text(0.012880373530832394, 0.03125, &#39;squared_error = 0.161\nsamples = 114\nvalue = 11.495&#39;),
 Text(0.013524392207374013, 0.03125, &#39;squared_error = 0.174\nsamples = 10\nvalue = 11.932&#39;),
 Text(0.016422476251811303, 0.21875, &#39;X[62] &lt;= 0.43\nsquared_error = 0.198\nsamples = 25\nvalue = 11.165&#39;),
 Text(0.015134438898728063, 0.15625, &#39;X[43] &lt;= 0.366\nsquared_error = 0.2\nsamples = 10\nvalue = 11.448&#39;),
 Text(0.014490420222186443, 0.09375, &#39;X[52] &lt;= 0.181\nsquared_error = 0.253\nsamples = 4\nvalue = 11.084&#39;),
 Text(0.014168410883915633, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.918&#39;),
 Text(0.014812429560457254, 0.03125, &#39;squared_error = 0.027\nsamples = 3\nvalue = 10.806&#39;),
 Text(0.015778457575269682, 0.09375, &#39;X[48] &lt;= 0.25\nsquared_error = 0.019\nsamples = 6\nvalue = 11.69&#39;),
 Text(0.015456448236998873, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 11.513&#39;),
 Text(0.016100466913540494, 0.03125, &#39;squared_error = 0.005\nsamples = 4\nvalue = 11.779&#39;),
 Text(0.01771051360489454, 0.15625, &#39;X[60] &lt;= 0.278\nsquared_error = 0.108\nsamples = 15\nvalue = 10.976&#39;),
 Text(0.017066494928352924, 0.09375, &#39;X[0] &lt;= 25034.602\nsquared_error = 0.027\nsamples = 3\nvalue = 10.514&#39;),
 Text(0.01674448559008211, 0.03125, &#39;squared_error = 0.01\nsamples = 2\nvalue = 10.617&#39;),
 Text(0.017388504266623733, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.309&#39;),
 Text(0.018354532281436162, 0.09375, &#39;X[47] &lt;= 0.262\nsquared_error = 0.062\nsamples = 12\nvalue = 11.091&#39;),
 Text(0.01803252294316535, 0.03125, &#39;squared_error = 0.011\nsamples = 4\nvalue = 11.388&#39;),
 Text(0.01867654161970697, 0.03125, &#39;squared_error = 0.021\nsamples = 8\nvalue = 10.943&#39;),
 Text(0.03687006923200773, 0.46875, &#39;X[34] &lt;= 2.154\nsquared_error = 0.286\nsamples = 792\nvalue = 11.53&#39;),
 Text(0.02849782643696667, 0.40625, &#39;X[47] &lt;= 0.991\nsquared_error = 0.347\nsamples = 81\nvalue = 11.265&#39;),
 Text(0.028175817098695863, 0.34375, &#39;X[46] &lt;= 0.888\nsquared_error = 0.282\nsamples = 80\nvalue = 11.295&#39;),
 Text(0.024150700370310738, 0.28125, &#39;X[46] &lt;= 0.465\nsquared_error = 0.247\nsamples = 69\nvalue = 11.216&#39;),
 Text(0.02157462566414426, 0.21875, &#39;X[60] &lt;= 0.197\nsquared_error = 0.203\nsamples = 43\nvalue = 11.374&#39;),
 Text(0.020286588311061022, 0.15625, &#39;X[8] &lt;= 3.5\nsquared_error = 0.137\nsamples = 6\nvalue = 10.849&#39;),
 Text(0.0196425696345194, 0.09375, &#39;X[51] &lt;= 0.46\nsquared_error = 0.013\nsamples = 3\nvalue = 11.196&#39;),
 Text(0.019320560296248592, 0.03125, &#39;squared_error = 0.001\nsamples = 2\nvalue = 11.119&#39;),
 Text(0.01996457897279021, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.35&#39;),
 Text(0.02093060698760264, 0.09375, &#39;X[4] &lt;= 1.5\nsquared_error = 0.018\nsamples = 3\nvalue = 10.501&#39;),
 Text(0.02060859764933183, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.309&#39;),
 Text(0.02125261632587345, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 10.597&#39;),
 Text(0.0228626630172275, 0.15625, &#39;X[47] &lt;= 0.935\nsquared_error = 0.162\nsamples = 37\nvalue = 11.459&#39;),
 Text(0.02221864434068588, 0.09375, &#39;X[44] &lt;= 0.052\nsquared_error = 0.126\nsamples = 35\nvalue = 11.506&#39;),
 Text(0.02189663500241507, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.463&#39;),
 Text(0.02254065367895669, 0.03125, &#39;squared_error = 0.097\nsamples = 34\nvalue = 11.537&#39;),
 Text(0.02350668169376912, 0.09375, &#39;X[34] &lt;= 0.5\nsquared_error = 0.055\nsamples = 2\nvalue = 10.624&#39;),
 Text(0.023184672355498308, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.859&#39;),
 Text(0.02382869103203993, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.389&#39;),
 Text(0.026726775076477218, 0.21875, &#39;X[48] &lt;= 0.556\nsquared_error = 0.211\nsamples = 26\nvalue = 10.956&#39;),
 Text(0.02543873772339398, 0.15625, &#39;X[51] &lt;= 0.711\nsquared_error = 0.184\nsamples = 16\nvalue = 10.775&#39;),
 Text(0.02479471904685236, 0.09375, &#39;X[56] &lt;= 0.506\nsquared_error = 0.144\nsamples = 13\nvalue = 10.653&#39;),
 Text(0.02447270970858155, 0.03125, &#39;squared_error = 0.09\nsamples = 6\nvalue = 10.381&#39;),
 Text(0.025116728385123167, 0.03125, &#39;squared_error = 0.074\nsamples = 7\nvalue = 10.886&#39;),
 Text(0.026082756399935597, 0.09375, &#39;X[62] &lt;= 0.298\nsquared_error = 0.012\nsamples = 3\nvalue = 11.305&#39;),
 Text(0.02576074706166479, 0.03125, &#39;squared_error = 0.001\nsamples = 2\nvalue = 11.379&#39;),
 Text(0.02640476573820641, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.156&#39;),
 Text(0.028014812429560457, 0.15625, &#39;X[62] &lt;= 0.479\nsquared_error = 0.118\nsamples = 10\nvalue = 11.245&#39;),
 Text(0.02737079375301884, 0.09375, &#39;X[54] &lt;= 0.347\nsquared_error = 0.045\nsamples = 6\nvalue = 11.44&#39;),
 Text(0.027048784414748027, 0.03125, &#39;squared_error = 0.004\nsamples = 3\nvalue = 11.245&#39;),
 Text(0.027692803091289648, 0.03125, &#39;squared_error = 0.011\nsamples = 3\nvalue = 11.634&#39;),
 Text(0.028658831106102078, 0.09375, &#39;X[41] &lt;= 0.5\nsquared_error = 0.086\nsamples = 4\nvalue = 10.954&#39;),
 Text(0.028336821767831265, 0.03125, &#39;squared_error = 0.027\nsamples = 3\nvalue = 10.806&#39;),
 Text(0.028980840444372886, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.396&#39;),
 Text(0.03220093382708099, 0.28125, &#39;X[2] &lt;= 1970.0\nsquared_error = 0.216\nsamples = 11\nvalue = 11.788&#39;),
 Text(0.030912896473997746, 0.21875, &#39;X[54] &lt;= 0.653\nsquared_error = 0.112\nsamples = 6\nvalue = 12.113&#39;),
 Text(0.030590887135726937, 0.15625, &#39;X[1] &lt;= 1050.0\nsquared_error = 0.032\nsamples = 5\nvalue = 12.244&#39;),
 Text(0.029946868459185316, 0.09375, &#39;X[50] &lt;= 0.725\nsquared_error = 0.007\nsamples = 3\nvalue = 12.371&#39;),
 Text(0.029624859120914507, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 12.429&#39;),
 Text(0.030268877797456125, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 12.255&#39;),
 Text(0.031234905812268555, 0.09375, &#39;X[43] &lt;= 0.796\nsquared_error = 0.01\nsamples = 2\nvalue = 12.053&#39;),
 Text(0.030912896473997746, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.155&#39;),
 Text(0.031556915150539364, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.951&#39;),
 Text(0.031234905812268555, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.462&#39;),
 Text(0.03348897118016422, 0.21875, &#39;X[49] &lt;= 0.805\nsquared_error = 0.06\nsamples = 5\nvalue = 11.397&#39;),
 Text(0.03316696184189342, 0.15625, &#39;X[48] &lt;= 0.568\nsquared_error = 0.018\nsamples = 4\nvalue = 11.504&#39;),
 Text(0.03252294316535179, 0.09375, &#39;X[2] &lt;= 1990.0\nsquared_error = 0.004\nsamples = 2\nvalue = 11.628&#39;),
 Text(0.03220093382708099, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.562&#39;),
 Text(0.032844952503622606, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.695&#39;),
 Text(0.033810980518435035, 0.09375, &#39;X[4] &lt;= 1.5\nsquared_error = 0.001\nsamples = 2\nvalue = 11.379&#39;),
 Text(0.03348897118016422, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.35&#39;),
 Text(0.03413298985670585, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.408&#39;),
 Text(0.033810980518435035, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.968&#39;),
 Text(0.02881983577523748, 0.34375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 8.923&#39;),
 Text(0.04524231202704879, 0.40625, &#39;X[8] &lt;= 3.5\nsquared_error = 0.27\nsamples = 711\nvalue = 11.56&#39;),
 Text(0.04053292545483819, 0.34375, &#39;X[42] &lt;= 1.5\nsquared_error = 0.261\nsamples = 642\nvalue = 11.533&#39;),
 Text(0.03823860891965867, 0.28125, &#39;X[62] &lt;= 0.829\nsquared_error = 0.356\nsamples = 113\nvalue = 11.317&#39;),
 Text(0.03687006923200773, 0.21875, &#39;X[2] &lt;= 1919.5\nsquared_error = 0.309\nsamples = 94\nvalue = 11.235&#39;),
 Text(0.035743036548059895, 0.15625, &#39;X[6] &lt;= 4.0\nsquared_error = 0.18\nsamples = 6\nvalue = 10.371&#39;),
 Text(0.03509901787151828, 0.09375, &#39;X[43] &lt;= 0.405\nsquared_error = 0.08\nsamples = 3\nvalue = 10.006&#39;),
 Text(0.034777008533247465, 0.03125, &#39;squared_error = 0.006\nsamples = 2\nvalue = 10.201&#39;),
 Text(0.03542102720978908, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.616&#39;),
 Text(0.03638705522460151, 0.09375, &#39;X[48] &lt;= 0.894\nsquared_error = 0.014\nsamples = 3\nvalue = 10.737&#39;),
 Text(0.0360650458863307, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 10.82&#39;),
 Text(0.036709064562872325, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.571&#39;),
 Text(0.03799710191595556, 0.15625, &#39;X[5] &lt;= 1.5\nsquared_error = 0.263\nsamples = 88\nvalue = 11.294&#39;),
 Text(0.037675092577684755, 0.09375, &#39;X[59] &lt;= 0.019\nsquared_error = 0.234\nsamples = 87\nvalue = 11.314&#39;),
 Text(0.03735308323941394, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.903&#39;),
 Text(0.03799710191595556, 0.03125, &#39;squared_error = 0.213\nsamples = 86\nvalue = 11.33&#39;),
 Text(0.03831911125422637, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.616&#39;),
 Text(0.039607148607309614, 0.21875, &#39;X[0] &lt;= 269841.5\nsquared_error = 0.389\nsamples = 19\nvalue = 11.722&#39;),
 Text(0.0392851392690388, 0.15625, &#39;X[9] &lt;= 5.5\nsquared_error = 0.161\nsamples = 18\nvalue = 11.607&#39;),
 Text(0.03896312993076799, 0.09375, &#39;X[56] &lt;= 0.846\nsquared_error = 0.112\nsamples = 17\nvalue = 11.55&#39;),
 Text(0.038641120592497184, 0.03125, &#39;squared_error = 0.078\nsamples = 14\nvalue = 11.457&#39;),
 Text(0.0392851392690388, 0.03125, &#39;squared_error = 0.04\nsamples = 3\nvalue = 11.983&#39;),
 Text(0.039607148607309614, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.578&#39;),
 Text(0.03992915794558042, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 13.786&#39;),
 Text(0.04282724199001771, 0.28125, &#39;X[1] &lt;= 1060.0\nsquared_error = 0.229\nsamples = 529\nvalue = 11.579&#39;),
 Text(0.041539204636934474, 0.21875, &#39;X[52] &lt;= 0.971\nsquared_error = 0.178\nsamples = 162\nvalue = 11.447&#39;),
 Text(0.040573176622122044, 0.15625, &#39;X[56] &lt;= 0.006\nsquared_error = 0.161\nsamples = 159\nvalue = 11.465&#39;),
 Text(0.04025116728385123, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 13.017&#39;),
 Text(0.04089518596039285, 0.09375, &#39;X[2] &lt;= 2000.0\nsquared_error = 0.147\nsamples = 158\nvalue = 11.455&#39;),
 Text(0.040573176622122044, 0.03125, &#39;squared_error = 0.132\nsamples = 152\nvalue = 11.43&#39;),
 Text(0.04121719529866366, 0.03125, &#39;squared_error = 0.107\nsamples = 6\nvalue = 12.086&#39;),
 Text(0.0425052326517469, 0.15625, &#39;X[49] &lt;= 0.168\nsquared_error = 0.201\nsamples = 3\nvalue = 10.523&#39;),
 Text(0.04218322331347609, 0.09375, &#39;X[56] &lt;= 0.292\nsquared_error = 0.014\nsamples = 2\nvalue = 10.833&#39;),
 Text(0.04186121397520528, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.951&#39;),
 Text(0.0425052326517469, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.714&#39;),
 Text(0.04282724199001771, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.903&#39;),
 Text(0.04411527934310095, 0.21875, &#39;X[39] &lt;= 1.5\nsquared_error = 0.241\nsamples = 367\nvalue = 11.637&#39;),
 Text(0.04379327000483014, 0.15625, &#39;X[56] &lt;= 0.999\nsquared_error = 0.234\nsamples = 366\nvalue = 11.633&#39;),
 Text(0.04347126066655933, 0.09375, &#39;X[62] &lt;= 0.053\nsquared_error = 0.229\nsamples = 365\nvalue = 11.629&#39;),
 Text(0.04314925132828852, 0.03125, &#39;squared_error = 0.176\nsamples = 14\nvalue = 11.983&#39;),
 Text(0.04379327000483014, 0.03125, &#39;squared_error = 0.226\nsamples = 351\nvalue = 11.615&#39;),
 Text(0.04411527934310095, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 13.039&#39;),
 Text(0.04443728868137176, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 13.305&#39;),
 Text(0.04995169859925938, 0.34375, &#39;X[22] &lt;= 1.5\nsquared_error = 0.281\nsamples = 69\nvalue = 11.816&#39;),
 Text(0.047818386733215264, 0.28125, &#39;X[61] &lt;= 0.94\nsquared_error = 0.208\nsamples = 54\nvalue = 11.971&#39;),
 Text(0.04669135404926743, 0.21875, &#39;X[62] &lt;= 0.646\nsquared_error = 0.182\nsamples = 52\nvalue = 11.937&#39;),
 Text(0.04540331669618419, 0.15625, &#39;X[45] &lt;= 0.63\nsquared_error = 0.123\nsamples = 26\nvalue = 12.108&#39;),
 Text(0.04475929801964257, 0.09375, &#39;X[45] &lt;= 0.186\nsquared_error = 0.071\nsamples = 15\nvalue = 12.273&#39;),
 Text(0.04443728868137176, 0.03125, &#39;squared_error = 0.004\nsamples = 4\nvalue = 11.956&#39;),
 Text(0.04508130735791338, 0.03125, &#39;squared_error = 0.046\nsamples = 11\nvalue = 12.388&#39;),
 Text(0.04604733537272581, 0.09375, &#39;X[44] &lt;= 0.637\nsquared_error = 0.105\nsamples = 11\nvalue = 11.884&#39;),
 Text(0.045725326034455, 0.03125, &#39;squared_error = 0.007\nsamples = 3\nvalue = 12.236&#39;),
 Text(0.046369344710996616, 0.03125, &#39;squared_error = 0.078\nsamples = 8\nvalue = 11.752&#39;),
 Text(0.04797939140235067, 0.15625, &#39;X[1] &lt;= 1310.0\nsquared_error = 0.183\nsamples = 26\nvalue = 11.765&#39;),
 Text(0.047335372725809045, 0.09375, &#39;X[47] &lt;= 0.536\nsquared_error = 0.156\nsamples = 14\nvalue = 11.539&#39;),
 Text(0.04701336338753824, 0.03125, &#39;squared_error = 0.121\nsamples = 7\nvalue = 11.264&#39;),
 Text(0.04765738206407986, 0.03125, &#39;squared_error = 0.04\nsamples = 7\nvalue = 11.814&#39;),
 Text(0.04862341007889229, 0.09375, &#39;X[10] &lt;= 1.5\nsquared_error = 0.086\nsamples = 12\nvalue = 12.029&#39;),
 Text(0.048301400740621475, 0.03125, &#39;squared_error = 0.04\nsamples = 11\nvalue = 12.097&#39;),
 Text(0.0489454194171631, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.29&#39;),
 Text(0.0489454194171631, 0.21875, &#39;X[59] &lt;= 0.209\nsquared_error = 0.061\nsamples = 2\nvalue = 12.859&#39;),
 Text(0.04862341007889229, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 13.106&#39;),
 Text(0.049267428755433905, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.612&#39;),
 Text(0.05208501046530349, 0.28125, &#39;X[58] &lt;= 0.37\nsquared_error = 0.144\nsamples = 15\nvalue = 11.257&#39;),
 Text(0.05055546610851715, 0.21875, &#39;X[45] &lt;= 0.784\nsquared_error = 0.022\nsamples = 5\nvalue = 10.835&#39;),
 Text(0.04991144743197553, 0.15625, &#39;X[60] &lt;= 0.356\nsquared_error = 0.005\nsamples = 3\nvalue = 10.936&#39;),
 Text(0.04958943809370472, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.035&#39;),
 Text(0.050233456770246335, 0.09375, &#39;X[18] &lt;= -4.0\nsquared_error = 0.001\nsamples = 2\nvalue = 10.887&#39;),
 Text(0.04991144743197553, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.915&#39;),
 Text(0.05055546610851715, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.859&#39;),
 Text(0.051199484785058764, 0.15625, &#39;X[45] &lt;= 0.898\nsquared_error = 0.007\nsamples = 2\nvalue = 10.683&#39;),
 Text(0.05087747544678796, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.768&#39;),
 Text(0.05152149412332958, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.597&#39;),
 Text(0.05361455482208984, 0.21875, &#39;X[45] &lt;= 0.849\nsquared_error = 0.071\nsamples = 10\nvalue = 11.468&#39;),
 Text(0.05280953147641282, 0.15625, &#39;X[45] &lt;= 0.41\nsquared_error = 0.048\nsamples = 8\nvalue = 11.377&#39;),
 Text(0.052165512799871194, 0.09375, &#39;X[51] &lt;= 0.308\nsquared_error = 0.007\nsamples = 2\nvalue = 11.694&#39;),
 Text(0.05184350346160039, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.613&#39;),
 Text(0.052487522138142007, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.775&#39;),
 Text(0.053453550152954436, 0.09375, &#39;X[58] &lt;= 0.812\nsquared_error = 0.017\nsamples = 6\nvalue = 11.271&#39;),
 Text(0.053131540814683624, 0.03125, &#39;squared_error = 0.006\nsamples = 5\nvalue = 11.222&#39;),
 Text(0.05377555949122525, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.513&#39;),
 Text(0.054419578167766866, 0.15625, &#39;X[46] &lt;= 0.191\nsquared_error = 0.0\nsamples = 2\nvalue = 11.831&#39;),
 Text(0.054097568829496054, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.849&#39;),
 Text(0.05474158750603768, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.813&#39;),
 Text(0.06282704073418129, 0.53125, &#39;X[59] &lt;= 0.013\nsquared_error = 2.146\nsamples = 150\nvalue = 11.071&#39;),
 Text(0.06160441152793431, 0.46875, &#39;X[4] &lt;= 2.5\nsquared_error = 28.394\nsamples = 2\nvalue = 5.329&#39;),
 Text(0.0612824021896635, 0.40625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.657&#39;),
 Text(0.06192642086620512, 0.40625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 0.0&#39;),
 Text(0.06404966994042827, 0.46875, &#39;X[11] &lt;= 1.5\nsquared_error = 1.339\nsamples = 148\nvalue = 11.148&#39;),
 Text(0.06257043954274674, 0.40625, &#39;X[50] &lt;= 0.995\nsquared_error = 0.5\nsamples = 146\nvalue = 11.223&#39;),
 Text(0.0605780067621961, 0.34375, &#39;X[62] &lt;= 0.943\nsquared_error = 0.389\nsamples = 143\nvalue = 11.253&#39;),
 Text(0.05820318789244888, 0.28125, &#39;X[49] &lt;= 0.07\nsquared_error = 0.359\nsamples = 134\nvalue = 11.304&#39;),
 Text(0.05683464820479794, 0.21875, &#39;X[61] &lt;= 0.706\nsquared_error = 1.462\nsamples = 8\nvalue = 10.542&#39;),
 Text(0.05602962485912091, 0.15625, &#39;X[0] &lt;= 8250.0\nsquared_error = 0.459\nsamples = 6\nvalue = 11.15&#39;),
 Text(0.055385606182579296, 0.09375, &#39;X[53] &lt;= 0.44\nsquared_error = 0.038\nsamples = 3\nvalue = 10.517&#39;),
 Text(0.055063596844308484, 0.03125, &#39;squared_error = 0.006\nsamples = 2\nvalue = 10.386&#39;),
 Text(0.05570761552085011, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.779&#39;),
 Text(0.05667364353566253, 0.09375, &#39;X[57] &lt;= 0.517\nsquared_error = 0.08\nsamples = 3\nvalue = 11.782&#39;),
 Text(0.056351634197391726, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.15&#39;),
 Text(0.05699565287393334, 0.03125, &#39;squared_error = 0.019\nsamples = 2\nvalue = 11.599&#39;),
 Text(0.05763967155047496, 0.15625, &#39;X[33] &lt;= 0.5\nsquared_error = 0.041\nsamples = 2\nvalue = 8.72&#39;),
 Text(0.057317662212204155, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 8.923&#39;),
 Text(0.05796168088874577, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 8.517&#39;),
 Text(0.05957172758009982, 0.21875, &#39;X[48] &lt;= 0.996\nsquared_error = 0.25\nsamples = 126\nvalue = 11.353&#39;),
 Text(0.059249718241829015, 0.15625, &#39;X[45] &lt;= 0.924\nsquared_error = 0.218\nsamples = 125\nvalue = 11.369&#39;),
 Text(0.05860569956528739, 0.09375, &#39;X[45] &lt;= 0.844\nsquared_error = 0.21\nsamples = 119\nvalue = 11.397&#39;),
 Text(0.058283690227016585, 0.03125, &#39;squared_error = 0.201\nsamples = 111\nvalue = 11.367&#39;),
 Text(0.0589277089035582, 0.03125, &#39;squared_error = 0.142\nsamples = 8\nvalue = 11.813&#39;),
 Text(0.05989373691837063, 0.09375, &#39;X[38] &lt;= 1.5\nsquared_error = 0.065\nsamples = 6\nvalue = 10.82&#39;),
 Text(0.05957172758009982, 0.03125, &#39;squared_error = 0.0\nsamples = 4\nvalue = 10.994&#39;),
 Text(0.060215746256641445, 0.03125, &#39;squared_error = 0.015\nsamples = 2\nvalue = 10.474&#39;),
 Text(0.05989373691837063, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 9.306&#39;),
 Text(0.06295282563194332, 0.28125, &#39;X[53] &lt;= 0.76\nsquared_error = 0.221\nsamples = 9\nvalue = 10.494&#39;),
 Text(0.06263081629367252, 0.21875, &#39;X[50] &lt;= 0.901\nsquared_error = 0.108\nsamples = 8\nvalue = 10.369&#39;),
 Text(0.06182579294799549, 0.15625, &#39;X[57] &lt;= 0.575\nsquared_error = 0.013\nsamples = 6\nvalue = 10.535&#39;),
 Text(0.061181774271453875, 0.09375, &#39;X[41] &lt;= 0.5\nsquared_error = 0.003\nsamples = 3\nvalue = 10.636&#39;),
 Text(0.06085976493318306, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 10.597&#39;),
 Text(0.06150378360972468, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.714&#39;),
 Text(0.06246981162453711, 0.09375, &#39;X[52] &lt;= 0.83\nsquared_error = 0.002\nsamples = 3\nvalue = 10.433&#39;),
 Text(0.062147802286266304, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 10.463&#39;),
 Text(0.06279182096280791, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.373&#39;),
 Text(0.06343583963934954, 0.15625, &#39;X[54] &lt;= 0.47\nsquared_error = 0.065\nsamples = 2\nvalue = 9.871&#39;),
 Text(0.06311383030107873, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.127&#39;),
 Text(0.06375784897762035, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.616&#39;),
 Text(0.06327483497021413, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.493&#39;),
 Text(0.06456287232329738, 0.34375, &#39;X[48] &lt;= 0.548\nsquared_error = 3.655\nsamples = 3\nvalue = 9.772&#39;),
 Text(0.06424086298502657, 0.28125, &#39;X[50] &lt;= 0.997\nsquared_error = 0.086\nsamples = 2\nvalue = 11.114&#39;),
 Text(0.06391885364675576, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.82&#39;),
 Text(0.06456287232329738, 0.21875, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.408&#39;),
 Text(0.06488488166156818, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 7.09&#39;),
 Text(0.0655289003381098, 0.40625, &#39;X[52] &lt;= 0.705\nsquared_error = 32.469\nsamples = 2\nvalue = 5.698&#39;),
 Text(0.06520689099983899, 0.34375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.396&#39;),
 Text(0.06585090967638062, 0.34375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 0.0&#39;),
 Text(0.08230105860569957, 0.65625, &#39;X[1] &lt;= 524.5\nsquared_error = 0.548\nsamples = 1493\nvalue = 11.638&#39;),
 Text(0.06919175656094027, 0.59375, &#39;X[35] &lt;= -2.0\nsquared_error = 16.388\nsamples = 7\nvalue = 9.87&#39;),
 Text(0.06886974722266946, 0.53125, &#39;X[53] &lt;= 0.574\nsquared_error = 0.176\nsamples = 6\nvalue = 11.515&#39;),
 Text(0.06806472387699243, 0.46875, &#39;X[1] &lt;= 420.0\nsquared_error = 0.058\nsamples = 4\nvalue = 11.765&#39;),
 Text(0.0674207052004508, 0.40625, &#39;X[60] &lt;= 0.1\nsquared_error = 0.012\nsamples = 2\nvalue = 11.993&#39;),
 Text(0.06709869586218001, 0.34375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.101&#39;),
 Text(0.06774271453872162, 0.34375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.884&#39;),
 Text(0.06870874255353406, 0.40625, &#39;X[22] &lt;= 1.5\nsquared_error = 0.001\nsamples = 2\nvalue = 11.537&#39;),
 Text(0.06838673321526324, 0.34375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.513&#39;),
 Text(0.06903075189180487, 0.34375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.562&#39;),
 Text(0.06967477056834648, 0.46875, &#39;X[55] &lt;= 0.597\nsquared_error = 0.038\nsamples = 2\nvalue = 11.016&#39;),
 Text(0.06935276123007567, 0.40625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.212&#39;),
 Text(0.06999677990661729, 0.40625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.82&#39;),
 Text(0.06951376589921107, 0.53125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 0.0&#39;),
 Text(0.09541036065045887, 0.59375, &#39;X[8] &lt;= 3.5\nsquared_error = 0.458\nsamples = 1486\nvalue = 11.646&#39;),
 Text(0.07988850426662374, 0.53125, &#39;X[4] &lt;= 2.5\nsquared_error = 0.406\nsamples = 1324\nvalue = 11.619&#39;),
 Text(0.07288480115923361, 0.46875, &#39;X[49] &lt;= 0.973\nsquared_error = 0.444\nsamples = 259\nvalue = 11.473&#39;),
 Text(0.07138544517791016, 0.40625, &#39;X[46] &lt;= 0.989\nsquared_error = 0.245\nsamples = 255\nvalue = 11.499&#39;),
 Text(0.06967477056834648, 0.34375, &#39;X[10] &lt;= 1.5\nsquared_error = 0.234\nsamples = 250\nvalue = 11.483&#39;),
 Text(0.06818547737884399, 0.28125, &#39;X[34] &lt;= 0.5\nsquared_error = 0.22\nsamples = 248\nvalue = 11.493&#39;),
 Text(0.06617291901465143, 0.21875, &#39;X[43] &lt;= 0.707\nsquared_error = 0.42\nsamples = 20\nvalue = 11.141&#39;),
 Text(0.06504588633070359, 0.15625, &#39;X[62] &lt;= 0.787\nsquared_error = 0.127\nsamples = 16\nvalue = 11.39&#39;),
 Text(0.06440186765416198, 0.09375, &#39;X[61] &lt;= 0.331\nsquared_error = 0.067\nsamples = 14\nvalue = 11.487&#39;),
 Text(0.06407985831589116, 0.03125, &#39;squared_error = 0.038\nsamples = 6\nvalue = 11.301&#39;),
 Text(0.06472387699243277, 0.03125, &#39;squared_error = 0.043\nsamples = 8\nvalue = 11.627&#39;),
 Text(0.06568990500724521, 0.09375, &#39;X[41] &lt;= 0.5\nsquared_error = 0.012\nsamples = 2\nvalue = 10.708&#39;),
 Text(0.0653678956689744, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.597&#39;),
 Text(0.06601191434551602, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.82&#39;),
 Text(0.06729995169859926, 0.15625, &#39;X[61] &lt;= 0.586\nsquared_error = 0.355\nsamples = 4\nvalue = 10.145&#39;),
 Text(0.06697794236032845, 0.09375, &#39;X[45] &lt;= 0.556\nsquared_error = 0.018\nsamples = 3\nvalue = 9.808&#39;),
 Text(0.06665593302205763, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 9.903&#39;),
 Text(0.06729995169859926, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.616&#39;),
 Text(0.06762196103687007, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.156&#39;),
 Text(0.07019803574303655, 0.21875, &#39;X[55] &lt;= 0.776\nsquared_error = 0.19\nsamples = 228\nvalue = 11.524&#39;),
 Text(0.0689099983899533, 0.15625, &#39;X[54] &lt;= 0.991\nsquared_error = 0.182\nsamples = 184\nvalue = 11.48&#39;),
 Text(0.0682659797134117, 0.09375, &#39;X[49] &lt;= 0.844\nsquared_error = 0.175\nsamples = 181\nvalue = 11.469&#39;),
 Text(0.06794397037514088, 0.03125, &#39;squared_error = 0.166\nsamples = 149\nvalue = 11.426&#39;),
 Text(0.0685879890516825, 0.03125, &#39;squared_error = 0.17\nsamples = 32\nvalue = 11.666&#39;),
 Text(0.06955401706649493, 0.09375, &#39;X[57] &lt;= 0.353\nsquared_error = 0.095\nsamples = 3\nvalue = 12.171&#39;),
 Text(0.06923200772822412, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.736&#39;),
 Text(0.06987602640476574, 0.03125, &#39;squared_error = -0.0\nsamples = 2\nvalue = 12.388&#39;),
 Text(0.07148607309611979, 0.15625, &#39;X[58] &lt;= 0.953\nsquared_error = 0.186\nsamples = 44\nvalue = 11.708&#39;),
 Text(0.07084205441957817, 0.09375, &#39;X[58] &lt;= 0.29\nsquared_error = 0.147\nsamples = 42\nvalue = 11.753&#39;),
 Text(0.07052004508130735, 0.03125, &#39;squared_error = 0.144\nsamples = 12\nvalue = 11.471&#39;),
 Text(0.07116406375784898, 0.03125, &#39;squared_error = 0.103\nsamples = 30\nvalue = 11.866&#39;),
 Text(0.0721300917726614, 0.09375, &#39;X[58] &lt;= 0.961\nsquared_error = 0.048\nsamples = 2\nvalue = 10.75&#39;),
 Text(0.0718080824343906, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.968&#39;),
 Text(0.07245210111093221, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.532&#39;),
 Text(0.07116406375784898, 0.28125, &#39;X[60] &lt;= 0.31\nsquared_error = 0.362\nsamples = 2\nvalue = 10.218&#39;),
 Text(0.07084205441957817, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.82&#39;),
 Text(0.07148607309611979, 0.21875, &#39;squared_error = -0.0\nsamples = 1\nvalue = 9.616&#39;),
 Text(0.07309611978747384, 0.34375, &#39;X[50] &lt;= 0.751\nsquared_error = 0.088\nsamples = 5\nvalue = 12.326&#39;),
 Text(0.07245210111093221, 0.28125, &#39;X[49] &lt;= 0.373\nsquared_error = 0.003\nsamples = 2\nvalue = 12.635&#39;),
 Text(0.0721300917726614, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.692&#39;),
 Text(0.07277411044920302, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.578&#39;),
 Text(0.07374013846401546, 0.28125, &#39;X[0] &lt;= 27684.602\nsquared_error = 0.038\nsamples = 3\nvalue = 12.121&#39;),
 Text(0.07341812912574465, 0.21875, &#39;X[50] &lt;= 0.797\nsquared_error = 0.002\nsamples = 2\nvalue = 12.256&#39;),
 Text(0.07309611978747384, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.211&#39;),
 Text(0.07374013846401546, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.301&#39;),
 Text(0.07406214780228626, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.849&#39;),
 Text(0.07438415714055707, 0.40625, &#39;X[2] &lt;= 1962.5\nsquared_error = 10.278\nsamples = 4\nvalue = 9.798&#39;),
 Text(0.07406214780228626, 0.34375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 4.248&#39;),
 Text(0.07470616647882788, 0.34375, &#39;X[1] &lt;= 1600.0\nsquared_error = 0.015\nsamples = 3\nvalue = 11.648&#39;),
 Text(0.07438415714055707, 0.28125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 11.736&#39;),
 Text(0.0750281758170987, 0.28125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.472&#39;),
 Text(0.08689220737401385, 0.46875, &#39;X[48] &lt;= 0.923\nsquared_error = 0.39\nsamples = 1065\nvalue = 11.654&#39;),
 Text(0.08168974400257607, 0.40625, &#39;X[6] &lt;= 1.5\nsquared_error = 0.261\nsamples = 995\nvalue = 11.675&#39;),
 Text(0.07748349702141362, 0.34375, &#39;X[59] &lt;= 0.989\nsquared_error = 0.414\nsamples = 244\nvalue = 11.542&#39;),
 Text(0.07623571083561423, 0.28125, &#39;X[29] &lt;= 1.5\nsquared_error = 0.255\nsamples = 242\nvalue = 11.57&#39;),
 Text(0.07470616647882788, 0.21875, &#39;X[10] &lt;= 1.5\nsquared_error = 0.239\nsamples = 223\nvalue = 11.538&#39;),
 Text(0.07438415714055707, 0.15625, &#39;X[1] &lt;= 2250.0\nsquared_error = 0.231\nsamples = 221\nvalue = 11.528&#39;),
 Text(0.07374013846401546, 0.09375, &#39;X[51] &lt;= 0.039\nsquared_error = 0.227\nsamples = 174\nvalue = 11.579&#39;),
 Text(0.07341812912574465, 0.03125, &#39;squared_error = 0.079\nsamples = 3\nvalue = 10.67&#39;),
 Text(0.07406214780228626, 0.03125, &#39;squared_error = 0.215\nsamples = 171\nvalue = 11.595&#39;),
 Text(0.0750281758170987, 0.09375, &#39;X[52] &lt;= 0.04\nsquared_error = 0.2\nsamples = 47\nvalue = 11.341&#39;),
 Text(0.07470616647882788, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.766&#39;),
 Text(0.07535018515536951, 0.03125, &#39;squared_error = 0.16\nsamples = 46\nvalue = 11.31&#39;),
 Text(0.0750281758170987, 0.15625, &#39;squared_error = -0.0\nsamples = 2\nvalue = 12.612&#39;),
 Text(0.07776525519240057, 0.21875, &#39;X[0] &lt;= 21000.0\nsquared_error = 0.278\nsamples = 19\nvalue = 11.954&#39;),
 Text(0.07696023184672356, 0.15625, &#39;X[57] &lt;= 0.187\nsquared_error = 0.115\nsamples = 13\nvalue = 12.188&#39;),
 Text(0.07631621317018193, 0.09375, &#39;X[59] &lt;= 0.893\nsquared_error = 0.001\nsamples = 2\nvalue = 11.585&#39;),
 Text(0.07599420383191112, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.562&#39;),
 Text(0.07663822250845274, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.608&#39;),
 Text(0.07760425052326518, 0.09375, &#39;X[56] &lt;= 0.858\nsquared_error = 0.057\nsamples = 11\nvalue = 12.298&#39;),
 Text(0.07728224118499437, 0.03125, &#39;squared_error = 0.023\nsamples = 10\nvalue = 12.358&#39;),
 Text(0.07792625986153598, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.695&#39;),
 Text(0.0785702785380776, 0.15625, &#39;X[51] &lt;= 0.414\nsquared_error = 0.257\nsamples = 6\nvalue = 11.448&#39;),
 Text(0.07824826919980679, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.468&#39;),
 Text(0.07889228787634842, 0.09375, &#39;X[51] &lt;= 0.632\nsquared_error = 0.058\nsamples = 5\nvalue = 11.244&#39;),
 Text(0.0785702785380776, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.82&#39;),
 Text(0.07921429721461923, 0.03125, &#39;squared_error = 0.016\nsamples = 4\nvalue = 11.35&#39;),
 Text(0.07873128320721301, 0.28125, &#39;X[55] &lt;= 0.782\nsquared_error = 8.133\nsamples = 2\nvalue = 8.15&#39;),
 Text(0.0784092738689422, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 5.298&#39;),
 Text(0.07905329254548382, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.002&#39;),
 Text(0.08589599098373853, 0.34375, &#39;X[9] &lt;= 1.5\nsquared_error = 0.204\nsamples = 751\nvalue = 11.719&#39;),
 Text(0.08372242795041056, 0.28125, &#39;X[1] &lt;= 1650.0\nsquared_error = 0.283\nsamples = 50\nvalue = 12.071&#39;),
 Text(0.08211238125905651, 0.21875, &#39;X[53] &lt;= 0.291\nsquared_error = 0.208\nsamples = 28\nvalue = 11.802&#39;),
 Text(0.08082434390597328, 0.15625, &#39;X[43] &lt;= 0.598\nsquared_error = 0.116\nsamples = 7\nvalue = 11.342&#39;),
 Text(0.08018032522943165, 0.09375, &#39;X[58] &lt;= 0.359\nsquared_error = 0.035\nsamples = 4\nvalue = 11.084&#39;),
 Text(0.07985831589116084, 0.03125, &#39;squared_error = 0.015\nsamples = 3\nvalue = 10.995&#39;),
 Text(0.08050233456770246, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.35&#39;),
 Text(0.08146836258251489, 0.09375, &#39;X[53] &lt;= 0.108\nsquared_error = 0.017\nsamples = 3\nvalue = 11.687&#39;),
 Text(0.08114635324424409, 0.03125, &#39;squared_error = 0.005\nsamples = 2\nvalue = 11.606&#39;),
 Text(0.0817903719207857, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.849&#39;),
 Text(0.08340041861213975, 0.15625, &#39;X[61] &lt;= 0.952\nsquared_error = 0.145\nsamples = 21\nvalue = 11.956&#39;),
 Text(0.08275639993559814, 0.09375, &#39;X[57] &lt;= 0.024\nsquared_error = 0.096\nsamples = 19\nvalue = 12.029&#39;),
 Text(0.08243439059732732, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.794&#39;),
 Text(0.08307840927386895, 0.03125, &#39;squared_error = 0.067\nsamples = 18\nvalue = 11.987&#39;),
 Text(0.08404443728868137, 0.09375, &#39;X[6] &lt;= 3.5\nsquared_error = 0.065\nsamples = 2\nvalue = 11.258&#39;),
 Text(0.08372242795041056, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.513&#39;),
 Text(0.08436644662695218, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.002&#39;),
 Text(0.0853324746417646, 0.21875, &#39;X[51] &lt;= 0.121\nsquared_error = 0.168\nsamples = 22\nvalue = 12.414&#39;),
 Text(0.0850104653034938, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 13.592&#39;),
 Text(0.08565448398003542, 0.15625, &#39;X[60] &lt;= 0.89\nsquared_error = 0.107\nsamples = 21\nvalue = 12.358&#39;),
 Text(0.0853324746417646, 0.09375, &#39;X[61] &lt;= 0.932\nsquared_error = 0.049\nsamples = 20\nvalue = 12.303&#39;),
 Text(0.0850104653034938, 0.03125, &#39;squared_error = 0.017\nsamples = 19\nvalue = 12.344&#39;),
 Text(0.08565448398003542, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.513&#39;),
 Text(0.08597649331830623, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 13.459&#39;),
 Text(0.0880695540170665, 0.28125, &#39;X[1] &lt;= 1306.5\nsquared_error = 0.189\nsamples = 701\nvalue = 11.694&#39;),
 Text(0.08726453067138946, 0.21875, &#39;X[0] &lt;= 445841.5\nsquared_error = 0.122\nsamples = 164\nvalue = 11.561&#39;),
 Text(0.08694252133311867, 0.15625, &#39;X[34] &lt;= 0.5\nsquared_error = 0.098\nsamples = 163\nvalue = 11.573&#39;),
 Text(0.08662051199484785, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.766&#39;),
 Text(0.08726453067138946, 0.09375, &#39;X[53] &lt;= 0.24\nsquared_error = 0.09\nsamples = 162\nvalue = 11.566&#39;),
 Text(0.08694252133311867, 0.03125, &#39;squared_error = 0.07\nsamples = 38\nvalue = 11.705&#39;),
 Text(0.08758654000966028, 0.03125, &#39;squared_error = 0.089\nsamples = 124\nvalue = 11.523&#39;),
 Text(0.08758654000966028, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.582&#39;),
 Text(0.08887457736274353, 0.21875, &#39;X[1] &lt;= 1318.5\nsquared_error = 0.202\nsamples = 537\nvalue = 11.734&#39;),
 Text(0.08855256802447271, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 13.872&#39;),
 Text(0.08919658670101432, 0.15625, &#39;X[16] &lt;= -3.5\nsquared_error = 0.194\nsamples = 536\nvalue = 11.73&#39;),
 Text(0.08855256802447271, 0.09375, &#39;X[55] &lt;= 0.725\nsquared_error = 1.046\nsamples = 3\nvalue = 10.653&#39;),
 Text(0.0882305586862019, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.21&#39;),
 Text(0.08887457736274353, 0.03125, &#39;squared_error = 0.008\nsamples = 2\nvalue = 11.374&#39;),
 Text(0.08984060537755595, 0.09375, &#39;X[0] &lt;= 11001.5\nsquared_error = 0.183\nsamples = 533\nvalue = 11.736&#39;),
 Text(0.08951859603928514, 0.03125, &#39;squared_error = 0.2\nsamples = 251\nvalue = 11.654&#39;),
 Text(0.09016261471582676, 0.03125, &#39;squared_error = 0.156\nsamples = 282\nvalue = 11.809&#39;),
 Text(0.09209467074545162, 0.40625, &#39;X[58] &lt;= 0.054\nsquared_error = 2.127\nsamples = 70\nvalue = 11.357&#39;),
 Text(0.09112864273063918, 0.34375, &#39;X[6] &lt;= 4.0\nsquared_error = 14.257\nsamples = 5\nvalue = 9.141&#39;),
 Text(0.09080663339236839, 0.28125, &#39;X[52] &lt;= 0.613\nsquared_error = 0.093\nsamples = 4\nvalue = 11.025&#39;),
 Text(0.09016261471582676, 0.21875, &#39;X[48] &lt;= 0.958\nsquared_error = 0.008\nsamples = 2\nvalue = 10.733&#39;),
 Text(0.08984060537755595, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.645&#39;),
 Text(0.09048462405409757, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.82&#39;),
 Text(0.09145065206891, 0.21875, &#39;X[48] &lt;= 0.954\nsquared_error = 0.008\nsamples = 2\nvalue = 11.316&#39;),
 Text(0.09112864273063918, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.225&#39;),
 Text(0.09177266140718081, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.408&#39;),
 Text(0.09145065206891, 0.28125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 1.609&#39;),
 Text(0.09306069876026404, 0.34375, &#39;X[49] &lt;= 0.04\nsquared_error = 0.787\nsamples = 65\nvalue = 11.527&#39;),
 Text(0.09241668008372243, 0.28125, &#39;X[51] &lt;= 0.683\nsquared_error = 9.845\nsamples = 2\nvalue = 8.213&#39;),
 Text(0.09209467074545162, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.35&#39;),
 Text(0.09273868942199323, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 5.075&#39;),
 Text(0.09370471743680567, 0.28125, &#39;X[44] &lt;= 0.009\nsquared_error = 0.14\nsamples = 63\nvalue = 11.632&#39;),
 Text(0.09338270809853486, 0.21875, &#39;squared_error = 0.0\nsamples = 2\nvalue = 12.612&#39;),
 Text(0.09402672677507648, 0.21875, &#39;X[47] &lt;= 0.069\nsquared_error = 0.112\nsamples = 61\nvalue = 11.6&#39;),
 Text(0.09273868942199323, 0.15625, &#39;X[0] &lt;= 13125.0\nsquared_error = 0.096\nsamples = 7\nvalue = 12.0&#39;),
 Text(0.09209467074545162, 0.09375, &#39;X[45] &lt;= 0.36\nsquared_error = 0.025\nsamples = 5\nvalue = 11.837&#39;),
 Text(0.09177266140718081, 0.03125, &#39;squared_error = 0.004\nsamples = 2\nvalue = 11.672&#39;),
 Text(0.09241668008372243, 0.03125, &#39;squared_error = 0.009\nsamples = 3\nvalue = 11.947&#39;),
 Text(0.09338270809853486, 0.09375, &#39;X[41] &lt;= 0.5\nsquared_error = 0.041\nsamples = 2\nvalue = 12.409&#39;),
 Text(0.09306069876026404, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.612&#39;),
 Text(0.09370471743680567, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.206&#39;),
 Text(0.09531476412815972, 0.15625, &#39;X[59] &lt;= 0.031\nsquared_error = 0.091\nsamples = 54\nvalue = 11.548&#39;),
 Text(0.09467074545161809, 0.09375, &#39;X[42] &lt;= -2.0\nsquared_error = 0.002\nsamples = 2\nvalue = 12.254&#39;),
 Text(0.09434873611334729, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.301&#39;),
 Text(0.0949927547898889, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.206&#39;),
 Text(0.09595878280470134, 0.09375, &#39;X[58] &lt;= 0.616\nsquared_error = 0.074\nsamples = 52\nvalue = 11.521&#39;),
 Text(0.09563677346643053, 0.03125, &#39;squared_error = 0.044\nsamples = 34\nvalue = 11.408&#39;),
 Text(0.09628079214297215, 0.03125, &#39;squared_error = 0.061\nsamples = 18\nvalue = 11.735&#39;),
 Text(0.110932217034294, 0.53125, &#39;X[59] &lt;= 0.829\nsquared_error = 0.833\nsamples = 162\nvalue = 11.868&#39;),
 Text(0.10380776042505233, 0.46875, &#39;X[22] &lt;= 1.5\nsquared_error = 0.362\nsamples = 137\nvalue = 11.968&#39;),
 Text(0.09801159233617775, 0.40625, &#39;X[10] &lt;= -3.0\nsquared_error = 0.273\nsamples = 103\nvalue = 12.114&#39;),
 Text(0.09628079214297215, 0.34375, &#39;X[45] &lt;= 0.451\nsquared_error = 0.281\nsamples = 3\nvalue = 10.997&#39;),
 Text(0.09595878280470134, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.309&#39;),
 Text(0.09660280148124295, 0.28125, &#39;X[59] &lt;= 0.474\nsquared_error = 0.067\nsamples = 2\nvalue = 11.341&#39;),
 Text(0.09628079214297215, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.599&#39;),
 Text(0.09692481081951376, 0.21875, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.082&#39;),
 Text(0.09974239252938336, 0.34375, &#39;X[2] &lt;= 1935.0\nsquared_error = 0.234\nsamples = 100\nvalue = 12.147&#39;),
 Text(0.0983738528417324, 0.28125, &#39;X[50] &lt;= 0.73\nsquared_error = 0.096\nsamples = 9\nvalue = 12.684&#39;),
 Text(0.09756882949605539, 0.21875, &#39;X[52] &lt;= 0.701\nsquared_error = 0.014\nsamples = 7\nvalue = 12.839&#39;),
 Text(0.09692481081951376, 0.15625, &#39;X[0] &lt;= 2275.0\nsquared_error = 0.004\nsamples = 5\nvalue = 12.9&#39;),
 Text(0.09660280148124295, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 13.017&#39;),
 Text(0.09724682015778457, 0.09375, &#39;X[47] &lt;= 0.569\nsquared_error = 0.001\nsamples = 4\nvalue = 12.87&#39;),
 Text(0.09692481081951376, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 12.899&#39;),
 Text(0.09756882949605539, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 12.841&#39;),
 Text(0.09821284817259701, 0.15625, &#39;X[51] &lt;= 0.57\nsquared_error = 0.006\nsamples = 2\nvalue = 12.689&#39;),
 Text(0.0978908388343262, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.766&#39;),
 Text(0.09853485751086781, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 12.612&#39;),
 Text(0.09917887618740943, 0.21875, &#39;X[50] &lt;= 0.776\nsquared_error = 0.004\nsamples = 2\nvalue = 12.139&#39;),
 Text(0.09885686684913862, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.206&#39;),
 Text(0.09950088552568025, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.073&#39;),
 Text(0.1011109322170343, 0.28125, &#39;X[1] &lt;= 1050.0\nsquared_error = 0.216\nsamples = 91\nvalue = 12.094&#39;),
 Text(0.10078892287876348, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.309&#39;),
 Text(0.1014329415553051, 0.21875, &#39;X[0] &lt;= 21780.0\nsquared_error = 0.183\nsamples = 90\nvalue = 12.114&#39;),
 Text(0.10014490420222187, 0.15625, &#39;X[29] &lt;= 1.5\nsquared_error = 0.169\nsamples = 79\nvalue = 12.052&#39;),
 Text(0.09950088552568025, 0.09375, &#39;X[21] &lt;= 1.5\nsquared_error = 0.17\nsamples = 66\nvalue = 11.991&#39;),
 Text(0.09917887618740943, 0.03125, &#39;squared_error = 0.149\nsamples = 65\nvalue = 11.972&#39;),
 Text(0.09982289486395106, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 13.218&#39;),
 Text(0.10078892287876348, 0.09375, &#39;X[49] &lt;= 0.233\nsquared_error = 0.052\nsamples = 13\nvalue = 12.36&#39;),
 Text(0.10046691354049267, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.96&#39;),
 Text(0.1011109322170343, 0.03125, &#39;squared_error = 0.024\nsamples = 12\nvalue = 12.31&#39;),
 Text(0.10272097890838834, 0.15625, &#39;X[61] &lt;= 0.597\nsquared_error = 0.051\nsamples = 11\nvalue = 12.561&#39;),
 Text(0.10207696023184672, 0.09375, &#39;X[55] &lt;= 0.805\nsquared_error = 0.025\nsamples = 5\nvalue = 12.76&#39;),
 Text(0.10175495089357592, 0.03125, &#39;squared_error = 0.007\nsamples = 3\nvalue = 12.876&#39;),
 Text(0.10239896957011753, 0.03125, &#39;squared_error = 0.001\nsamples = 2\nvalue = 12.586&#39;),
 Text(0.10336499758492997, 0.09375, &#39;X[55] &lt;= 0.389\nsquared_error = 0.013\nsamples = 6\nvalue = 12.396&#39;),
 Text(0.10304298824665915, 0.03125, &#39;squared_error = 0.003\nsamples = 4\nvalue = 12.467&#39;),
 Text(0.10368700692320078, 0.03125, &#39;squared_error = 0.002\nsamples = 2\nvalue = 12.254&#39;),
 Text(0.1096039285139269, 0.40625, &#39;X[57] &lt;= 0.303\nsquared_error = 0.373\nsamples = 34\nvalue = 11.527&#39;),
 Text(0.10698760264047658, 0.34375, &#39;X[55] &lt;= 0.598\nsquared_error = 0.519\nsamples = 10\nvalue = 11.058&#39;),
 Text(0.10578006762196104, 0.28125, &#39;X[44] &lt;= 0.865\nsquared_error = 0.285\nsamples = 7\nvalue = 10.715&#39;),
 Text(0.10497504427628401, 0.21875, &#39;X[51] &lt;= 0.386\nsquared_error = 0.083\nsamples = 5\nvalue = 10.434&#39;),
 Text(0.10433102559974239, 0.15625, &#39;X[9] &lt;= 1.5\nsquared_error = 0.025\nsamples = 2\nvalue = 10.756&#39;),
 Text(0.10400901626147158, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.915&#39;),
 Text(0.1046530349380132, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.597&#39;),
 Text(0.10561906295282564, 0.15625, &#39;X[46] &lt;= 0.275\nsquared_error = 0.006\nsamples = 3\nvalue = 10.219&#39;),
 Text(0.10529705361455483, 0.09375, &#39;X[22] &lt;= 2.5\nsquared_error = 0.002\nsamples = 2\nvalue = 10.265&#39;),
 Text(0.10497504427628401, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.222&#39;),
 Text(0.10561906295282564, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.309&#39;),
 Text(0.10594107229109644, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.127&#39;),
 Text(0.10658509096763806, 0.21875, &#39;X[0] &lt;= 24434.602\nsquared_error = 0.102\nsamples = 2\nvalue = 11.417&#39;),
 Text(0.10626308162936725, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.097&#39;),
 Text(0.10690710030590887, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.736&#39;),
 Text(0.10819513765899211, 0.28125, &#39;X[47] &lt;= 0.545\nsquared_error = 0.148\nsamples = 3\nvalue = 11.86&#39;),
 Text(0.1078731283207213, 0.21875, &#39;X[44] &lt;= 0.523\nsquared_error = 0.027\nsamples = 2\nvalue = 12.115&#39;),
 Text(0.1075511189824505, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.278&#39;),
 Text(0.10819513765899211, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.951&#39;),
 Text(0.10851714699726292, 0.21875, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.35&#39;),
 Text(0.11222025438737723, 0.34375, &#39;X[61] &lt;= 0.992\nsquared_error = 0.182\nsamples = 24\nvalue = 11.723&#39;),
 Text(0.11189824504910642, 0.28125, &#39;X[58] &lt;= 0.692\nsquared_error = 0.061\nsamples = 23\nvalue = 11.796&#39;),
 Text(0.11012719368861697, 0.21875, &#39;X[57] &lt;= 0.599\nsquared_error = 0.054\nsamples = 16\nvalue = 11.706&#39;),
 Text(0.10883915633553373, 0.15625, &#39;X[46] &lt;= 0.167\nsquared_error = 0.044\nsamples = 8\nvalue = 11.561&#39;),
 Text(0.10819513765899211, 0.09375, &#39;X[57] &lt;= 0.438\nsquared_error = 0.021\nsamples = 2\nvalue = 11.839&#39;),
 Text(0.1078731283207213, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.983&#39;),
 Text(0.10851714699726292, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.695&#39;),
 Text(0.10948317501207536, 0.09375, &#39;X[38] &lt;= 1.5\nsquared_error = 0.017\nsamples = 6\nvalue = 11.468&#39;),
 Text(0.10916116567380454, 0.03125, &#39;squared_error = 0.004\nsamples = 5\nvalue = 11.415&#39;),
 Text(0.10980518435034615, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.736&#39;),
 Text(0.11141523104170022, 0.15625, &#39;X[48] &lt;= 0.45\nsquared_error = 0.022\nsamples = 8\nvalue = 11.852&#39;),
 Text(0.11077121236515859, 0.09375, &#39;X[55] &lt;= 0.461\nsquared_error = 0.006\nsamples = 2\nvalue = 12.052&#39;),
 Text(0.11044920302688778, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.128&#39;),
 Text(0.1110932217034294, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.977&#39;),
 Text(0.11205924971824183, 0.09375, &#39;X[44] &lt;= 0.655\nsquared_error = 0.009\nsamples = 6\nvalue = 11.785&#39;),
 Text(0.11173724037997101, 0.03125, &#39;squared_error = 0.001\nsamples = 4\nvalue = 11.851&#39;),
 Text(0.11238125905651264, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 11.653&#39;),
 Text(0.11366929640959587, 0.21875, &#39;X[52] &lt;= 0.39\nsquared_error = 0.019\nsamples = 7\nvalue = 12.0&#39;),
 Text(0.11302527773305426, 0.15625, &#39;X[51] &lt;= 0.094\nsquared_error = 0.001\nsamples = 2\nvalue = 12.18&#39;),
 Text(0.11270326839478345, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.155&#39;),
 Text(0.11334728707132506, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.206&#39;),
 Text(0.1143133150861375, 0.15625, &#39;X[2] &lt;= 1989.0\nsquared_error = 0.007\nsamples = 5\nvalue = 11.928&#39;),
 Text(0.11399130574786669, 0.09375, &#39;X[28] &lt;= 0.5\nsquared_error = 0.002\nsamples = 4\nvalue = 11.966&#39;),
 Text(0.11366929640959587, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 11.925&#39;),
 Text(0.1143133150861375, 0.03125, &#39;squared_error = 0.001\nsamples = 2\nvalue = 12.007&#39;),
 Text(0.11463532442440831, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.775&#39;),
 Text(0.11254226372564805, 0.28125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.043&#39;),
 Text(0.11805667364353566, 0.46875, &#39;X[59] &lt;= 0.836\nsquared_error = 3.056\nsamples = 25\nvalue = 11.316&#39;),
 Text(0.11705039446143937, 0.40625, &#39;X[53] &lt;= 0.732\nsquared_error = 0.002\nsamples = 2\nvalue = 5.569&#39;),
 Text(0.11672838512316858, 0.34375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 5.521&#39;),
 Text(0.11737240379971019, 0.34375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 5.617&#39;),
 Text(0.11906295282563194, 0.40625, &#39;X[53] &lt;= 0.918\nsquared_error = 0.2\nsamples = 23\nvalue = 11.816&#39;),
 Text(0.11801642247625181, 0.34375, &#39;X[61] &lt;= 0.909\nsquared_error = 0.128\nsamples = 21\nvalue = 11.729&#39;),
 Text(0.11688938979230398, 0.28125, &#39;X[54] &lt;= 0.096\nsquared_error = 0.079\nsamples = 18\nvalue = 11.815&#39;),
 Text(0.11656738045403317, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.082&#39;),
 Text(0.11721139913057478, 0.21875, &#39;X[51] &lt;= 0.453\nsquared_error = 0.051\nsamples = 17\nvalue = 11.858&#39;),
 Text(0.11592336177749155, 0.15625, &#39;X[27] &lt;= 0.5\nsquared_error = 0.043\nsamples = 6\nvalue = 12.061&#39;),
 Text(0.11527934310094992, 0.09375, &#39;X[52] &lt;= 0.412\nsquared_error = 0.009\nsamples = 4\nvalue = 11.932&#39;),
 Text(0.11495733376267912, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.783&#39;),
 Text(0.11560135243922073, 0.03125, &#39;squared_error = 0.003\nsamples = 3\nvalue = 11.982&#39;),
 Text(0.11656738045403317, 0.09375, &#39;X[62] &lt;= 0.356\nsquared_error = 0.012\nsamples = 2\nvalue = 12.318&#39;),
 Text(0.11624537111576236, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.206&#39;),
 Text(0.11688938979230398, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.429&#39;),
 Text(0.11849943648365803, 0.15625, &#39;X[52] &lt;= 0.223\nsquared_error = 0.02\nsamples = 11\nvalue = 11.747&#39;),
 Text(0.1178554178071164, 0.09375, &#39;X[47] &lt;= 0.216\nsquared_error = 0.004\nsamples = 2\nvalue = 11.981&#39;),
 Text(0.1175334084688456, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.918&#39;),
 Text(0.11817742714538722, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.044&#39;),
 Text(0.11914345516019964, 0.09375, &#39;X[51] &lt;= 0.959\nsquared_error = 0.009\nsamples = 9\nvalue = 11.695&#39;),
 Text(0.11882144582192884, 0.03125, &#39;squared_error = 0.005\nsamples = 6\nvalue = 11.747&#39;),
 Text(0.11946546449847045, 0.03125, &#39;squared_error = 0.0\nsamples = 3\nvalue = 11.593&#39;),
 Text(0.11914345516019964, 0.28125, &#39;X[51] &lt;= 0.156\nsquared_error = 0.112\nsamples = 3\nvalue = 11.216&#39;),
 Text(0.11882144582192884, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.653&#39;),
 Text(0.11946546449847045, 0.21875, &#39;X[45] &lt;= 0.473\nsquared_error = 0.025\nsamples = 2\nvalue = 10.998&#39;),
 Text(0.11914345516019964, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.84&#39;),
 Text(0.11978747383674127, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.156&#39;),
 Text(0.12010948317501208, 0.34375, &#39;X[58] &lt;= 0.612\nsquared_error = 0.049\nsamples = 2\nvalue = 12.727&#39;),
 Text(0.11978747383674127, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.506&#39;),
 Text(0.12043149251328289, 0.28125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 12.948&#39;),
 Text(0.17093035541780713, 0.71875, &#39;X[42] &lt;= -2.5\nsquared_error = 0.684\nsamples = 2468\nvalue = 11.231&#39;),
 Text(0.15910783287715344, 0.65625, &#39;X[8] &lt;= 3.5\nsquared_error = 0.913\nsamples = 1001\nvalue = 11.053&#39;),
 Text(0.14441112542263726, 0.59375, &#39;X[6] &lt;= 1.5\nsquared_error = 0.919\nsamples = 947\nvalue = 11.016&#39;),
 Text(0.13016221220415392, 0.53125, &#39;X[1] &lt;= 686.0\nsquared_error = 0.981\nsamples = 484\nvalue = 10.859&#39;),
 Text(0.12294719046852359, 0.46875, &#39;X[10] &lt;= 1.5\nsquared_error = 5.368\nsamples = 25\nvalue = 9.989&#39;),
 Text(0.12262518113025278, 0.40625, &#39;X[54] &lt;= 0.08\nsquared_error = 1.261\nsamples = 24\nvalue = 10.405&#39;),
 Text(0.12139752052809531, 0.34375, &#39;X[45] &lt;= 0.171\nsquared_error = 1.325\nsamples = 2\nvalue = 7.366&#39;),
 Text(0.1210755111898245, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 6.215&#39;),
 Text(0.12171952986636612, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 8.517&#39;),
 Text(0.12385284173241023, 0.34375, &#39;X[8] &lt;= 1.5\nsquared_error = 0.339\nsamples = 22\nvalue = 10.681&#39;),
 Text(0.12236354854290775, 0.28125, &#39;X[47] &lt;= 0.334\nsquared_error = 0.045\nsamples = 3\nvalue = 9.915&#39;),
 Text(0.12204153920463694, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.616&#39;),
 Text(0.12268555788117855, 0.21875, &#39;X[47] &lt;= 0.477\nsquared_error = 0.0\nsamples = 2\nvalue = 10.065&#39;),
 Text(0.12236354854290775, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.043&#39;),
 Text(0.12300756721944936, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.086&#39;),
 Text(0.12534213492191273, 0.28125, &#39;X[53] &lt;= 0.247\nsquared_error = 0.278\nsamples = 19\nvalue = 10.802&#39;),
 Text(0.1239735952342618, 0.21875, &#39;X[59] &lt;= 0.368\nsquared_error = 0.067\nsamples = 5\nvalue = 10.295&#39;),
 Text(0.12365158589599098, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.714&#39;),
 Text(0.12429560457253261, 0.15625, &#39;X[48] &lt;= 0.644\nsquared_error = 0.028\nsamples = 4\nvalue = 10.19&#39;),
 Text(0.1239735952342618, 0.09375, &#39;X[52] &lt;= 0.635\nsquared_error = 0.001\nsamples = 3\nvalue = 10.286&#39;),
 Text(0.12365158589599098, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 10.309&#39;),
 Text(0.12429560457253261, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.24&#39;),
 Text(0.12461761391080341, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.903&#39;),
 Text(0.12671067460956367, 0.21875, &#39;X[55] &lt;= 0.796\nsquared_error = 0.23\nsamples = 14\nvalue = 10.983&#39;),
 Text(0.12590565126388664, 0.15625, &#39;X[54] &lt;= 0.417\nsquared_error = 0.166\nsamples = 11\nvalue = 11.146&#39;),
 Text(0.12526163258734505, 0.09375, &#39;X[8] &lt;= 2.5\nsquared_error = 0.02\nsamples = 4\nvalue = 10.763&#39;),
 Text(0.12493962324907422, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.002&#39;),
 Text(0.12558364192561583, 0.03125, &#39;squared_error = 0.001\nsamples = 3\nvalue = 10.684&#39;),
 Text(0.12654966994042827, 0.09375, &#39;X[53] &lt;= 0.614\nsquared_error = 0.118\nsamples = 7\nvalue = 11.364&#39;),
 Text(0.12622766060215745, 0.03125, &#39;squared_error = 0.046\nsamples = 3\nvalue = 11.018&#39;),
 Text(0.12687167927869908, 0.03125, &#39;squared_error = 0.015\nsamples = 4\nvalue = 11.623&#39;),
 Text(0.1275156979552407, 0.15625, &#39;X[57] &lt;= 0.32\nsquared_error = 0.012\nsamples = 3\nvalue = 10.388&#39;),
 Text(0.1271936886169699, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.545&#39;),
 Text(0.12783770729351152, 0.09375, &#39;squared_error = -0.0\nsamples = 2\nvalue = 10.309&#39;),
 Text(0.12326919980679439, 0.40625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 0.0&#39;),
 Text(0.13737723393978427, 0.46875, &#39;X[48] &lt;= 0.051\nsquared_error = 0.698\nsamples = 459\nvalue = 10.906&#39;),
 Text(0.1312993076799227, 0.40625, &#39;X[62] &lt;= 0.028\nsquared_error = 2.904\nsamples = 22\nvalue = 10.172&#39;),
 Text(0.13097729834165192, 0.34375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 4.248&#39;),
 Text(0.13162131701819352, 0.34375, &#39;X[49] &lt;= 0.028\nsquared_error = 1.292\nsamples = 21\nvalue = 10.454&#39;),
 Text(0.1312993076799227, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 6.908&#39;),
 Text(0.13194332635646433, 0.28125, &#39;X[56] &lt;= 0.461\nsquared_error = 0.696\nsamples = 20\nvalue = 10.632&#39;),
 Text(0.12960875865400096, 0.21875, &#39;X[47] &lt;= 0.404\nsquared_error = 0.711\nsamples = 7\nvalue = 9.85&#39;),
 Text(0.12880373530832395, 0.15625, &#39;X[26] &lt;= 1.5\nsquared_error = 0.206\nsamples = 3\nvalue = 9.019&#39;),
 Text(0.12848172597005314, 0.09375, &#39;X[60] &lt;= 0.674\nsquared_error = 0.041\nsamples = 2\nvalue = 8.72&#39;),
 Text(0.12815971663178233, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 8.517&#39;),
 Text(0.12880373530832395, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 8.923&#39;),
 Text(0.12912574464659476, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 9.616&#39;),
 Text(0.13041378199967799, 0.15625, &#39;X[57] &lt;= 0.364\nsquared_error = 0.184\nsamples = 4\nvalue = 10.473&#39;),
 Text(0.12976976332313636, 0.09375, &#39;X[0] &lt;= 5250.0\nsquared_error = 0.059\nsamples = 2\nvalue = 10.839&#39;),
 Text(0.12944775398486555, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.597&#39;),
 Text(0.13009177266140717, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.082&#39;),
 Text(0.1310578006762196, 0.09375, &#39;X[55] &lt;= 0.9\nsquared_error = 0.041\nsamples = 2\nvalue = 10.106&#39;),
 Text(0.1307357913379488, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.309&#39;),
 Text(0.13137981001449042, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.903&#39;),
 Text(0.1342778940589277, 0.21875, &#39;X[47] &lt;= 0.261\nsquared_error = 0.182\nsamples = 13\nvalue = 11.053&#39;),
 Text(0.13298985670584446, 0.15625, &#39;X[20] &lt;= 1.5\nsquared_error = 0.072\nsamples = 4\nvalue = 11.526&#39;),
 Text(0.13234583802930286, 0.09375, &#39;X[45] &lt;= 0.366\nsquared_error = 0.007\nsamples = 2\nvalue = 11.787&#39;),
 Text(0.13202382869103205, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.871&#39;),
 Text(0.13266784736757367, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.704&#39;),
 Text(0.13363387538238608, 0.09375, &#39;X[0] &lt;= 6015.0\nsquared_error = 0.0\nsamples = 2\nvalue = 11.264&#39;),
 Text(0.13331186604411527, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.277&#39;),
 Text(0.1339558847206569, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.252&#39;),
 Text(0.13556593141201095, 0.15625, &#39;X[22] &lt;= 1.5\nsquared_error = 0.087\nsamples = 9\nvalue = 10.842&#39;),
 Text(0.13492191273546933, 0.09375, &#39;X[46] &lt;= 0.199\nsquared_error = 0.033\nsamples = 6\nvalue = 11.013&#39;),
 Text(0.13459990339719852, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.669&#39;),
 Text(0.13524392207374014, 0.03125, &#39;squared_error = 0.012\nsamples = 5\nvalue = 11.082&#39;),
 Text(0.13620995008855258, 0.09375, &#39;X[41] &lt;= 0.5\nsquared_error = 0.018\nsamples = 3\nvalue = 10.501&#39;),
 Text(0.13588794075028177, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 10.597&#39;),
 Text(0.1365319594268234, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.309&#39;),
 Text(0.1434551601996458, 0.40625, &#39;X[15] &lt;= -3.5\nsquared_error = 0.559\nsamples = 437\nvalue = 10.943&#39;),
 Text(0.14313315086137499, 0.34375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 14.226&#39;),
 Text(0.1437771695379166, 0.34375, &#39;X[8] &lt;= 1.5\nsquared_error = 0.535\nsamples = 436\nvalue = 10.935&#39;),
 Text(0.1405570761552085, 0.28125, &#39;X[35] &lt;= -2.5\nsquared_error = 0.764\nsamples = 54\nvalue = 10.562&#39;),
 Text(0.13830301078731283, 0.21875, &#39;X[52] &lt;= 0.096\nsquared_error = 0.422\nsamples = 39\nvalue = 10.789&#39;),
 Text(0.137175978103365, 0.15625, &#39;X[47] &lt;= 0.602\nsquared_error = 0.113\nsamples = 4\nvalue = 11.817&#39;),
 Text(0.13685396876509417, 0.09375, &#39;squared_error = 0.0\nsamples = 2\nvalue = 11.513&#39;),
 Text(0.1374979874416358, 0.09375, &#39;X[47] &lt;= 0.806\nsquared_error = 0.041\nsamples = 2\nvalue = 12.121&#39;),
 Text(0.137175978103365, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.918&#39;),
 Text(0.1378199967799066, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.324&#39;),
 Text(0.13943004347126067, 0.15625, &#39;X[52] &lt;= 0.954\nsquared_error = 0.323\nsamples = 35\nvalue = 10.671&#39;),
 Text(0.13878602479471905, 0.09375, &#39;X[47] &lt;= 0.944\nsquared_error = 0.241\nsamples = 32\nvalue = 10.765&#39;),
 Text(0.13846401545644824, 0.03125, &#39;squared_error = 0.186\nsamples = 31\nvalue = 10.809&#39;),
 Text(0.13910803413298986, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.393&#39;),
 Text(0.1400740621478023, 0.09375, &#39;X[58] &lt;= 0.235\nsquared_error = 0.107\nsamples = 3\nvalue = 9.672&#39;),
 Text(0.13975205280953149, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.21&#39;),
 Text(0.1403960714860731, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 9.903&#39;),
 Text(0.14281114152310417, 0.21875, &#39;X[49] &lt;= 0.718\nsquared_error = 1.173\nsamples = 15\nvalue = 9.973&#39;),
 Text(0.14200611817742714, 0.15625, &#39;X[53] &lt;= 0.844\nsquared_error = 0.58\nsamples = 11\nvalue = 10.46&#39;),
 Text(0.14136209950088552, 0.09375, &#39;X[49] &lt;= 0.435\nsquared_error = 0.285\nsamples = 9\nvalue = 10.738&#39;),
 Text(0.1410400901626147, 0.03125, &#39;squared_error = 0.106\nsamples = 6\nvalue = 10.417&#39;),
 Text(0.14168410883915633, 0.03125, &#39;squared_error = 0.024\nsamples = 3\nvalue = 11.38&#39;),
 Text(0.14265013685396877, 0.09375, &#39;X[2] &lt;= 1919.5\nsquared_error = 0.0\nsamples = 2\nvalue = 9.21&#39;),
 Text(0.14232812751569796, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.21&#39;),
 Text(0.14297214619223958, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.21&#39;),
 Text(0.1436161648687812, 0.15625, &#39;X[60] &lt;= 0.215\nsquared_error = 0.359\nsamples = 4\nvalue = 8.635&#39;),
 Text(0.1432941555305104, 0.09375, &#39;squared_error = 0.0\nsamples = 2\nvalue = 9.21&#39;),
 Text(0.14393817420705202, 0.09375, &#39;X[48] &lt;= 0.396\nsquared_error = 0.055\nsamples = 2\nvalue = 8.059&#39;),
 Text(0.1436161648687812, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 7.824&#39;),
 Text(0.1442601835453228, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 8.294&#39;),
 Text(0.1469972629206247, 0.28125, &#39;X[4] &lt;= 4.5\nsquared_error = 0.48\nsamples = 382\nvalue = 10.988&#39;),
 Text(0.14619223957494767, 0.21875, &#39;X[61] &lt;= 0.99\nsquared_error = 0.454\nsamples = 375\nvalue = 10.97&#39;),
 Text(0.14587023023667686, 0.15625, &#39;X[7] &lt;= 1.5\nsquared_error = 0.439\nsamples = 374\nvalue = 10.976&#39;),
 Text(0.14522621156013524, 0.09375, &#39;X[48] &lt;= 0.975\nsquared_error = 0.52\nsamples = 48\nvalue = 10.683&#39;),
 Text(0.14490420222186443, 0.03125, &#39;squared_error = 0.366\nsamples = 45\nvalue = 10.777&#39;),
 Text(0.14554822089840605, 0.03125, &#39;squared_error = 0.721\nsamples = 3\nvalue = 9.278&#39;),
 Text(0.1465142489132185, 0.09375, &#39;X[62] &lt;= 0.017\nsquared_error = 0.413\nsamples = 326\nvalue = 11.02&#39;),
 Text(0.14619223957494767, 0.03125, &#39;squared_error = 0.431\nsamples = 6\nvalue = 11.807&#39;),
 Text(0.1468362582514893, 0.03125, &#39;squared_error = 0.401\nsamples = 320\nvalue = 11.005&#39;),
 Text(0.1465142489132185, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 8.517&#39;),
 Text(0.14780228626630174, 0.21875, &#39;X[21] &lt;= 1.5\nsquared_error = 0.889\nsamples = 7\nvalue = 11.971&#39;),
 Text(0.14748027692803092, 0.15625, &#39;X[1] &lt;= 1466.0\nsquared_error = 0.209\nsamples = 6\nvalue = 11.627&#39;),
 Text(0.1471582675897601, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.714&#39;),
 Text(0.14780228626630174, 0.09375, &#39;X[58] &lt;= 0.559\nsquared_error = 0.051\nsamples = 5\nvalue = 11.81&#39;),
 Text(0.14748027692803092, 0.03125, &#39;squared_error = 0.015\nsamples = 3\nvalue = 11.976&#39;),
 Text(0.14812429560457252, 0.03125, &#39;squared_error = 0.002\nsamples = 2\nvalue = 11.561&#39;),
 Text(0.14812429560457252, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 14.036&#39;),
 Text(0.1586600386411206, 0.53125, &#39;X[62] &lt;= 1.0\nsquared_error = 0.801\nsamples = 463\nvalue = 11.18&#39;),
 Text(0.1583380293028498, 0.46875, &#39;X[4] &lt;= 2.5\nsquared_error = 0.77\nsamples = 462\nvalue = 11.189&#39;),
 Text(0.15482611495733375, 0.40625, &#39;X[50] &lt;= 0.991\nsquared_error = 0.663\nsamples = 188\nvalue = 11.001&#39;),
 Text(0.15315569151505393, 0.34375, &#39;X[45] &lt;= 0.97\nsquared_error = 0.482\nsamples = 186\nvalue = 11.032&#39;),
 Text(0.1507808726453067, 0.28125, &#39;X[9] &lt;= 1.5\nsquared_error = 0.435\nsamples = 180\nvalue = 11.0&#39;),
 Text(0.14941233295765577, 0.21875, &#39;X[6] &lt;= 2.5\nsquared_error = 0.072\nsamples = 7\nvalue = 11.759&#39;),
 Text(0.14876831428111414, 0.15625, &#39;X[53] &lt;= 0.818\nsquared_error = 0.003\nsamples = 2\nvalue = 12.153&#39;),
 Text(0.14844630494284333, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.206&#39;),
 Text(0.14909032361938496, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.101&#39;),
 Text(0.1500563516341974, 0.15625, &#39;X[21] &lt;= 1.5\nsquared_error = 0.013\nsamples = 5\nvalue = 11.602&#39;),
 Text(0.14973434229592658, 0.09375, &#39;X[51] &lt;= 0.691\nsquared_error = 0.002\nsamples = 2\nvalue = 11.735&#39;),
 Text(0.14941233295765577, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.775&#39;),
 Text(0.1500563516341974, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.695&#39;),
 Text(0.1503783609724682, 0.09375, &#39;squared_error = -0.0\nsamples = 3\nvalue = 11.513&#39;),
 Text(0.15214941233295765, 0.21875, &#39;X[59] &lt;= 0.022\nsquared_error = 0.426\nsamples = 173\nvalue = 10.969&#39;),
 Text(0.15134438898728064, 0.15625, &#39;X[47] &lt;= 0.667\nsquared_error = 0.378\nsamples = 3\nvalue = 12.089&#39;),
 Text(0.15102237964900983, 0.09375, &#39;X[49] &lt;= 0.118\nsquared_error = 0.008\nsamples = 2\nvalue = 12.52&#39;),
 Text(0.15070037031073902, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.612&#39;),
 Text(0.15134438898728064, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 12.429&#39;),
 Text(0.15166639832555145, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.225&#39;),
 Text(0.15295443567863468, 0.15625, &#39;X[54] &lt;= 0.994\nsquared_error = 0.404\nsamples = 170\nvalue = 10.949&#39;),
 Text(0.15231041700209305, 0.09375, &#39;X[2] &lt;= 1987.5\nsquared_error = 0.383\nsamples = 168\nvalue = 10.933&#39;),
 Text(0.15198840766382224, 0.03125, &#39;squared_error = 0.355\nsamples = 165\nvalue = 10.953&#39;),
 Text(0.15263242634036386, 0.03125, &#39;squared_error = 0.67\nsamples = 3\nvalue = 9.821&#39;),
 Text(0.1535984543551763, 0.09375, &#39;X[4] &lt;= 1.5\nsquared_error = 0.179\nsamples = 2\nvalue = 12.342&#39;),
 Text(0.1532764450169055, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.766&#39;),
 Text(0.1539204636934471, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.918&#39;),
 Text(0.15553051038480115, 0.28125, &#39;X[2] &lt;= 1955.0\nsquared_error = 0.906\nsamples = 6\nvalue = 12.003&#39;),
 Text(0.15488649170825955, 0.21875, &#39;X[54] &lt;= 0.745\nsquared_error = 0.191\nsamples = 4\nvalue = 11.386&#39;),
 Text(0.15456448236998874, 0.15625, &#39;X[57] &lt;= 0.891\nsquared_error = 0.007\nsamples = 3\nvalue = 11.634&#39;),
 Text(0.15424247303171792, 0.09375, &#39;squared_error = 0.0\nsamples = 2\nvalue = 11.695&#39;),
 Text(0.15488649170825955, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.513&#39;),
 Text(0.15520850104653036, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.641&#39;),
 Text(0.15617452906134277, 0.21875, &#39;X[59] &lt;= 0.327\nsquared_error = 0.049\nsamples = 2\nvalue = 13.238&#39;),
 Text(0.15585251972307196, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 13.017&#39;),
 Text(0.15649653839961358, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 13.459&#39;),
 Text(0.15649653839961358, 0.34375, &#39;X[58] &lt;= 0.555\nsquared_error = 9.064\nsamples = 2\nvalue = 8.117&#39;),
 Text(0.15617452906134277, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 5.106&#39;),
 Text(0.1568185477378844, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.127&#39;),
 Text(0.1618499436483658, 0.40625, &#39;X[62] &lt;= 0.034\nsquared_error = 0.803\nsamples = 274\nvalue = 11.318&#39;),
 Text(0.15850909676380615, 0.34375, &#39;X[49] &lt;= 0.166\nsquared_error = 8.605\nsamples = 14\nvalue = 10.385&#39;),
 Text(0.15818708742553533, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 0.0&#39;),
 Text(0.15883110610207696, 0.28125, &#39;X[1] &lt;= 1650.0\nsquared_error = 0.333\nsamples = 13\nvalue = 11.183&#39;),
 Text(0.15794558042183224, 0.21875, &#39;X[60] &lt;= 0.356\nsquared_error = 0.184\nsamples = 10\nvalue = 10.95&#39;),
 Text(0.1571405570761552, 0.15625, &#39;X[46] &lt;= 0.591\nsquared_error = 0.035\nsamples = 4\nvalue = 10.547&#39;),
 Text(0.15649653839961358, 0.09375, &#39;X[53] &lt;= 0.674\nsquared_error = 0.012\nsamples = 2\nvalue = 10.708&#39;),
 Text(0.15617452906134277, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.597&#39;),
 Text(0.1568185477378844, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.82&#39;),
 Text(0.15778457575269683, 0.09375, &#39;X[62] &lt;= 0.02\nsquared_error = 0.006\nsamples = 2\nvalue = 10.386&#39;),
 Text(0.15746256641442602, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.309&#39;),
 Text(0.15810658509096764, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.463&#39;),
 Text(0.15875060376750927, 0.15625, &#39;X[46] &lt;= 0.059\nsquared_error = 0.103\nsamples = 6\nvalue = 11.219&#39;),
 Text(0.15842859442923846, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.918&#39;),
 Text(0.15907261310578008, 0.09375, &#39;X[1] &lt;= 1225.0\nsquared_error = 0.007\nsamples = 5\nvalue = 11.079&#39;),
 Text(0.15875060376750927, 0.03125, &#39;squared_error = 0.002\nsamples = 4\nvalue = 11.042&#39;),
 Text(0.15939462244405087, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.225&#39;),
 Text(0.15971663178232168, 0.21875, &#39;X[43] &lt;= 0.66\nsquared_error = 0.044\nsamples = 3\nvalue = 11.961&#39;),
 Text(0.15939462244405087, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.695&#39;),
 Text(0.1600386411205925, 0.15625, &#39;X[54] &lt;= 0.711\nsquared_error = 0.012\nsamples = 2\nvalue = 12.095&#39;),
 Text(0.15971663178232168, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.983&#39;),
 Text(0.1603606504588633, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 12.206&#39;),
 Text(0.16519079053292546, 0.34375, &#39;X[52] &lt;= 0.995\nsquared_error = 0.333\nsamples = 260\nvalue = 11.368&#39;),
 Text(0.16486878119465465, 0.28125, &#39;X[0] &lt;= 90200.0\nsquared_error = 0.303\nsamples = 259\nvalue = 11.357&#39;),
 Text(0.16293672516502977, 0.21875, &#39;X[54] &lt;= 0.762\nsquared_error = 0.235\nsamples = 230\nvalue = 11.309&#39;),
 Text(0.16164868781194655, 0.15625, &#39;X[60] &lt;= 0.973\nsquared_error = 0.225\nsamples = 177\nvalue = 11.362&#39;),
 Text(0.16100466913540493, 0.09375, &#39;X[60] &lt;= 0.667\nsquared_error = 0.214\nsamples = 174\nvalue = 11.376&#39;),
 Text(0.16068265979713411, 0.03125, &#39;squared_error = 0.2\nsamples = 122\nvalue = 11.3&#39;),
 Text(0.16132667847367574, 0.03125, &#39;squared_error = 0.204\nsamples = 52\nvalue = 11.554&#39;),
 Text(0.16229270648848818, 0.09375, &#39;X[15] &lt;= 1.5\nsquared_error = 0.231\nsamples = 3\nvalue = 10.575&#39;),
 Text(0.16197069715021736, 0.03125, &#39;squared_error = 0.008\nsamples = 2\nvalue = 10.911&#39;),
 Text(0.162614715826759, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.903&#39;),
 Text(0.16422476251811302, 0.15625, &#39;X[48] &lt;= 0.197\nsquared_error = 0.225\nsamples = 53\nvalue = 11.132&#39;),
 Text(0.1635807438415714, 0.09375, &#39;X[50] &lt;= 0.973\nsquared_error = 0.15\nsamples = 15\nvalue = 11.433&#39;),
 Text(0.16325873450330058, 0.03125, &#39;squared_error = 0.064\nsamples = 14\nvalue = 11.513&#39;),
 Text(0.1639027531798422, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.309&#39;),
 Text(0.16486878119465465, 0.09375, &#39;X[49] &lt;= 0.885\nsquared_error = 0.205\nsamples = 38\nvalue = 11.013&#39;),
 Text(0.16454677185638383, 0.03125, &#39;squared_error = 0.159\nsamples = 34\nvalue = 10.934&#39;),
 Text(0.16519079053292546, 0.03125, &#39;squared_error = 0.098\nsamples = 4\nvalue = 11.681&#39;),
 Text(0.1668008372242795, 0.21875, &#39;X[46] &lt;= 0.99\nsquared_error = 0.685\nsamples = 29\nvalue = 11.733&#39;),
 Text(0.1664788278860087, 0.15625, &#39;X[61] &lt;= 0.937\nsquared_error = 0.474\nsamples = 28\nvalue = 11.823&#39;),
 Text(0.1661568185477379, 0.09375, &#39;X[48] &lt;= 0.6\nsquared_error = 0.34\nsamples = 27\nvalue = 11.749&#39;),
 Text(0.16583480920946708, 0.03125, &#39;squared_error = 0.205\nsamples = 19\nvalue = 11.971&#39;),
 Text(0.1664788278860087, 0.03125, &#39;squared_error = 0.265\nsamples = 8\nvalue = 11.222&#39;),
 Text(0.1668008372242795, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 13.816&#39;),
 Text(0.1671228465625503, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 9.21&#39;),
 Text(0.16551279987119627, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 14.226&#39;),
 Text(0.1589820479793914, 0.46875, &#39;squared_error = -0.0\nsamples = 1\nvalue = 7.313&#39;),
 Text(0.17380454033166962, 0.59375, &#39;X[60] &lt;= 0.881\nsquared_error = 0.36\nsamples = 54\nvalue = 11.711&#39;),
 Text(0.1705039446143938, 0.53125, &#39;X[47] &lt;= 0.2\nsquared_error = 0.271\nsamples = 50\nvalue = 11.789&#39;),
 Text(0.16808887457736274, 0.46875, &#39;X[20] &lt;= 1.5\nsquared_error = 0.098\nsamples = 7\nvalue = 11.268&#39;),
 Text(0.16776686523909193, 0.40625, &#39;X[53] &lt;= 0.085\nsquared_error = 0.049\nsamples = 6\nvalue = 11.171&#39;),
 Text(0.16744485590082112, 0.34375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.714&#39;),
 Text(0.16808887457736274, 0.34375, &#39;X[58] &lt;= 0.552\nsquared_error = 0.009\nsamples = 5\nvalue = 11.263&#39;),
 Text(0.16776686523909193, 0.28125, &#39;X[54] &lt;= 0.382\nsquared_error = 0.001\nsamples = 4\nvalue = 11.308&#39;),
 Text(0.16744485590082112, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.362&#39;),
 Text(0.16808887457736274, 0.21875, &#39;squared_error = 0.0\nsamples = 3\nvalue = 11.29&#39;),
 Text(0.16841088391563355, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.082&#39;),
 Text(0.16841088391563355, 0.40625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.849&#39;),
 Text(0.1729190146514249, 0.46875, &#39;X[55] &lt;= 0.026\nsquared_error = 0.248\nsamples = 43\nvalue = 11.874&#39;),
 Text(0.17259700531315408, 0.40625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.434&#39;),
 Text(0.1732410239896957, 0.40625, &#39;X[44] &lt;= 0.869\nsquared_error = 0.204\nsamples = 42\nvalue = 11.908&#39;),
 Text(0.17034293994525843, 0.34375, &#39;X[57] &lt;= 0.046\nsquared_error = 0.129\nsamples = 35\nvalue = 12.008&#39;),
 Text(0.1700209306069876, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.947&#39;),
 Text(0.1706649492835292, 0.28125, &#39;X[62] &lt;= 0.664\nsquared_error = 0.099\nsamples = 34\nvalue = 12.04&#39;),
 Text(0.16873289325390436, 0.21875, &#39;X[51] &lt;= 0.093\nsquared_error = 0.091\nsamples = 19\nvalue = 12.16&#39;),
 Text(0.16792786990822733, 0.15625, &#39;X[51] &lt;= 0.079\nsquared_error = 0.002\nsamples = 3\nvalue = 12.554&#39;),
 Text(0.16760586056995652, 0.09375, &#39;X[55] &lt;= 0.725\nsquared_error = 0.0\nsamples = 2\nvalue = 12.525&#39;),
 Text(0.1672838512316857, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.525&#39;),
 Text(0.16792786990822733, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.525&#39;),
 Text(0.16824987924649815, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 12.612&#39;),
 Text(0.1695379165995814, 0.15625, &#39;X[1] &lt;= 1150.0\nsquared_error = 0.074\nsamples = 16\nvalue = 12.087&#39;),
 Text(0.16889389792303977, 0.09375, &#39;X[43] &lt;= 0.455\nsquared_error = 0.02\nsamples = 9\nvalue = 12.235&#39;),
 Text(0.16857188858476896, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 11.977&#39;),
 Text(0.16921590726131058, 0.03125, &#39;squared_error = 0.002\nsamples = 7\nvalue = 12.309&#39;),
 Text(0.17018193527612302, 0.09375, &#39;X[47] &lt;= 0.493\nsquared_error = 0.077\nsamples = 7\nvalue = 11.896&#39;),
 Text(0.1698599259378522, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 11.513&#39;),
 Text(0.1705039446143938, 0.03125, &#39;squared_error = 0.026\nsamples = 5\nvalue = 12.049&#39;),
 Text(0.17259700531315408, 0.21875, &#39;X[0] &lt;= 16500.0\nsquared_error = 0.066\nsamples = 15\nvalue = 11.887&#39;),
 Text(0.17179198196747705, 0.15625, &#39;X[46] &lt;= 0.916\nsquared_error = 0.029\nsamples = 11\nvalue = 12.006&#39;),
 Text(0.17146997262920624, 0.09375, &#39;X[56] &lt;= 0.445\nsquared_error = 0.013\nsamples = 10\nvalue = 11.964&#39;),
 Text(0.17114796329093543, 0.03125, &#39;squared_error = 0.003\nsamples = 4\nvalue = 11.857&#39;),
 Text(0.17179198196747705, 0.03125, &#39;squared_error = 0.006\nsamples = 6\nvalue = 12.036&#39;),
 Text(0.17211399130574787, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 12.429&#39;),
 Text(0.17340202865883111, 0.15625, &#39;X[44] &lt;= 0.502\nsquared_error = 0.019\nsamples = 4\nvalue = 11.557&#39;),
 Text(0.1727580099822895, 0.09375, &#39;X[17] &lt;= 1.5\nsquared_error = 0.007\nsamples = 2\nvalue = 11.432&#39;),
 Text(0.17243600064401868, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.513&#39;),
 Text(0.1730800193205603, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.35&#39;),
 Text(0.17404604733537274, 0.09375, &#39;X[21] &lt;= 1.5\nsquared_error = 0.0\nsamples = 2\nvalue = 11.683&#39;),
 Text(0.17372403799710193, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.67&#39;),
 Text(0.17436805667364352, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.695&#39;),
 Text(0.176139108034133, 0.34375, &#39;X[0] &lt;= 7275.0\nsquared_error = 0.274\nsamples = 7\nvalue = 11.405&#39;),
 Text(0.17501207535018515, 0.28125, &#39;X[17] &lt;= 1.5\nsquared_error = 0.072\nsamples = 4\nvalue = 10.993&#39;),
 Text(0.17436805667364352, 0.21875, &#39;X[55] &lt;= 0.66\nsquared_error = 0.006\nsamples = 2\nvalue = 11.231&#39;),
 Text(0.17404604733537274, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.156&#39;),
 Text(0.17469006601191434, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.306&#39;),
 Text(0.17565609402672677, 0.21875, &#39;X[62] &lt;= 0.215\nsquared_error = 0.025\nsamples = 2\nvalue = 10.756&#39;),
 Text(0.17533408468845596, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.915&#39;),
 Text(0.17597810336499758, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.597&#39;),
 Text(0.17726614071808083, 0.28125, &#39;X[49] &lt;= 0.531\nsquared_error = 0.016\nsamples = 3\nvalue = 11.954&#39;),
 Text(0.17694413137981002, 0.21875, &#39;X[55] &lt;= 0.62\nsquared_error = 0.0\nsamples = 2\nvalue = 12.044&#39;),
 Text(0.1766221220415392, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.044&#39;),
 Text(0.17726614071808083, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.044&#39;),
 Text(0.17758815005635165, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.775&#39;),
 Text(0.17710513604894543, 0.53125, &#39;X[50] &lt;= 0.287\nsquared_error = 0.448\nsamples = 4\nvalue = 10.742&#39;),
 Text(0.17678312671067462, 0.46875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.752&#39;),
 Text(0.17742714538721624, 0.46875, &#39;X[41] &lt;= 0.5\nsquared_error = 0.144\nsamples = 3\nvalue = 10.405&#39;),
 Text(0.17710513604894543, 0.40625, &#39;X[1] &lt;= 860.0\nsquared_error = 0.027\nsamples = 2\nvalue = 10.656&#39;),
 Text(0.17678312671067462, 0.34375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.82&#39;),
 Text(0.17742714538721624, 0.34375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.491&#39;),
 Text(0.17774915472548705, 0.40625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.903&#39;),
 Text(0.1827528779584608, 0.65625, &#39;X[57] &lt;= 0.999\nsquared_error = 0.491\nsamples = 1467\nvalue = 11.352&#39;),
 Text(0.18243086862018998, 0.59375, &#39;X[54] &lt;= 0.001\nsquared_error = 0.465\nsamples = 1466\nvalue = 11.356&#39;),
 Text(0.17863468040573177, 0.53125, &#39;X[46] &lt;= 0.496\nsquared_error = 9.56\nsamples = 2\nvalue = 8.198&#39;),
 Text(0.17831267106746096, 0.46875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 5.106&#39;),
 Text(0.17895668974400258, 0.46875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.29&#39;),
 Text(0.1862270568346482, 0.53125, &#39;X[1] &lt;= 512.5\nsquared_error = 0.438\nsamples = 1464\nvalue = 11.36&#39;),
 Text(0.1796007084205442, 0.46875, &#39;X[45] &lt;= 0.977\nsquared_error = 9.075\nsamples = 13\nvalue = 10.277&#39;),
 Text(0.1792786990822734, 0.40625, &#39;X[1] &lt;= 122.0\nsquared_error = 0.297\nsamples = 12\nvalue = 11.133&#39;),
 Text(0.17823216873289324, 0.34375, &#39;X[1] &lt;= 99.5\nsquared_error = 0.109\nsamples = 2\nvalue = 12.098&#39;),
 Text(0.17791015939462243, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.429&#39;),
 Text(0.17855417807116405, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.768&#39;),
 Text(0.18032522943165352, 0.34375, &#39;X[56] &lt;= 0.449\nsquared_error = 0.111\nsamples = 10\nvalue = 10.94&#39;),
 Text(0.17919819674770568, 0.28125, &#39;X[58] &lt;= 0.595\nsquared_error = 0.028\nsamples = 7\nvalue = 11.133&#39;),
 Text(0.17855417807116405, 0.21875, &#39;X[52] &lt;= 0.519\nsquared_error = 0.001\nsamples = 3\nvalue = 11.318&#39;),
 Text(0.17823216873289324, 0.15625, &#39;X[47] &lt;= 0.502\nsquared_error = 0.0\nsamples = 2\nvalue = 11.302&#39;),
 Text(0.17791015939462243, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.314&#39;),
 Text(0.17855417807116405, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.29&#39;),
 Text(0.17887618740943487, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.35&#39;),
 Text(0.1798422154242473, 0.21875, &#39;X[27] &lt;= 0.5\nsquared_error = 0.004\nsamples = 4\nvalue = 10.993&#39;),
 Text(0.1795202060859765, 0.15625, &#39;X[60] &lt;= 0.421\nsquared_error = 0.001\nsamples = 3\nvalue = 10.964&#39;),
 Text(0.17919819674770568, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.915&#39;),
 Text(0.1798422154242473, 0.09375, &#39;X[46] &lt;= 0.574\nsquared_error = 0.0\nsamples = 2\nvalue = 10.988&#39;),
 Text(0.1795202060859765, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.99&#39;),
 Text(0.18016422476251812, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.985&#39;),
 Text(0.18016422476251812, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.082&#39;),
 Text(0.18145226211560136, 0.28125, &#39;X[46] &lt;= 0.713\nsquared_error = 0.017\nsamples = 3\nvalue = 10.492&#39;),
 Text(0.18113025277733055, 0.21875, &#39;X[58] &lt;= 0.406\nsquared_error = 0.0\nsamples = 2\nvalue = 10.584&#39;),
 Text(0.18080824343905974, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.597&#39;),
 Text(0.18145226211560136, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.571&#39;),
 Text(0.18177427145387215, 0.21875, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.309&#39;),
 Text(0.179922717758815, 0.40625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 0.0&#39;),
 Text(0.19285340524875222, 0.46875, &#39;X[6] &lt;= 1.5\nsquared_error = 0.35\nsamples = 1451\nvalue = 11.37&#39;),
 Text(0.18630252777330542, 0.40625, &#39;X[43] &lt;= 0.004\nsquared_error = 0.342\nsamples = 603\nvalue = 11.246&#39;),
 Text(0.18402833682176784, 0.34375, &#39;X[58] &lt;= 0.356\nsquared_error = 1.332\nsamples = 2\nvalue = 13.072&#39;),
 Text(0.18370632748349702, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.918&#39;),
 Text(0.18435034616003865, 0.28125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 14.226&#39;),
 Text(0.18857671872484302, 0.34375, &#39;X[29] &lt;= 0.5\nsquared_error = 0.328\nsamples = 601\nvalue = 11.239&#39;),
 Text(0.18499436483658027, 0.28125, &#39;X[54] &lt;= 0.64\nsquared_error = 0.357\nsamples = 403\nvalue = 11.181&#39;),
 Text(0.18290130413782, 0.21875, &#39;X[62] &lt;= 0.02\nsquared_error = 0.315\nsamples = 253\nvalue = 11.281&#39;),
 Text(0.18209628079214296, 0.15625, &#39;X[53] &lt;= 0.751\nsquared_error = 2.213\nsamples = 2\nvalue = 10.005&#39;),
 Text(0.18177427145387215, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.493&#39;),
 Text(0.18241829013041377, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 8.517&#39;),
 Text(0.18370632748349702, 0.15625, &#39;X[46] &lt;= 0.386\nsquared_error = 0.287\nsamples = 251\nvalue = 11.292&#39;),
 Text(0.1830623088069554, 0.09375, &#39;X[58] &lt;= 0.068\nsquared_error = 0.284\nsamples = 98\nvalue = 11.414&#39;),
 Text(0.18274029946868459, 0.03125, &#39;squared_error = 0.265\nsamples = 2\nvalue = 9.725&#39;),
 Text(0.1833843181452262, 0.03125, &#39;squared_error = 0.224\nsamples = 96\nvalue = 11.449&#39;),
 Text(0.18435034616003865, 0.09375, &#39;X[57] &lt;= 0.016\nsquared_error = 0.273\nsamples = 153\nvalue = 11.213&#39;),
 Text(0.18402833682176784, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.788&#39;),
 Text(0.18467235549830946, 0.03125, &#39;squared_error = 0.258\nsamples = 152\nvalue = 11.203&#39;),
 Text(0.18708742553534052, 0.21875, &#39;X[44] &lt;= 0.992\nsquared_error = 0.381\nsamples = 150\nvalue = 11.011&#39;),
 Text(0.1862824021896635, 0.15625, &#39;X[59] &lt;= 0.101\nsquared_error = 0.334\nsamples = 148\nvalue = 11.033&#39;),
 Text(0.18563838351312187, 0.09375, &#39;X[56] &lt;= 0.228\nsquared_error = 0.443\nsamples = 7\nvalue = 10.326&#39;),
 Text(0.18531637417485108, 0.03125, &#39;squared_error = 0.103\nsamples = 4\nvalue = 9.815&#39;),
 Text(0.18596039285139268, 0.03125, &#39;squared_error = 0.086\nsamples = 3\nvalue = 11.006&#39;),
 Text(0.18692642086620512, 0.09375, &#39;X[46] &lt;= 0.73\nsquared_error = 0.302\nsamples = 141\nvalue = 11.068&#39;),
 Text(0.1866044115279343, 0.03125, &#39;squared_error = 0.267\nsamples = 100\nvalue = 11.182&#39;),
 Text(0.18724843020447593, 0.03125, &#39;squared_error = 0.281\nsamples = 41\nvalue = 10.791&#39;),
 Text(0.18789244888101755, 0.15625, &#39;X[43] &lt;= 0.454\nsquared_error = 1.176\nsamples = 2\nvalue = 9.379&#39;),
 Text(0.18757043954274674, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.463&#39;),
 Text(0.18821445821928837, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 8.294&#39;),
 Text(0.19215907261310577, 0.28125, &#39;X[0] &lt;= 1950.0\nsquared_error = 0.247\nsamples = 198\nvalue = 11.359&#39;),
 Text(0.19030751891804862, 0.21875, &#39;X[0] &lt;= 1775.0\nsquared_error = 0.3\nsamples = 17\nvalue = 11.728&#39;),
 Text(0.1895024955723716, 0.15625, &#39;X[1] &lt;= 1575.0\nsquared_error = 0.157\nsamples = 15\nvalue = 11.581&#39;),
 Text(0.18885847689583, 0.09375, &#39;X[1] &lt;= 1250.0\nsquared_error = 0.014\nsamples = 5\nvalue = 11.181&#39;),
 Text(0.18853646755755918, 0.03125, &#39;squared_error = 0.0\nsamples = 3\nvalue = 11.273&#39;),
 Text(0.18918048623410078, 0.03125, &#39;squared_error = 0.002\nsamples = 2\nvalue = 11.042&#39;),
 Text(0.1901465142489132, 0.09375, &#39;X[55] &lt;= 0.629\nsquared_error = 0.109\nsamples = 10\nvalue = 11.782&#39;),
 Text(0.1898245049106424, 0.03125, &#39;squared_error = 0.07\nsamples = 7\nvalue = 11.622&#39;),
 Text(0.19046852358718402, 0.03125, &#39;squared_error = 0.002\nsamples = 3\nvalue = 12.154&#39;),
 Text(0.19111254226372565, 0.15625, &#39;X[32] &lt;= 0.5\nsquared_error = 0.004\nsamples = 2\nvalue = 12.826&#39;),
 Text(0.19079053292545484, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.887&#39;),
 Text(0.19143455160199646, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.766&#39;),
 Text(0.19401062630816293, 0.21875, &#39;X[50] &lt;= 0.503\nsquared_error = 0.228\nsamples = 181\nvalue = 11.325&#39;),
 Text(0.1927225889550797, 0.15625, &#39;X[50] &lt;= 0.465\nsquared_error = 0.166\nsamples = 87\nvalue = 11.207&#39;),
 Text(0.19207857027853809, 0.09375, &#39;X[46] &lt;= 0.004\nsquared_error = 0.129\nsamples = 81\nvalue = 11.254&#39;),
 Text(0.19175656094026727, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.405&#39;),
 Text(0.1924005796168089, 0.03125, &#39;squared_error = 0.114\nsamples = 80\nvalue = 11.24&#39;),
 Text(0.1933666076316213, 0.09375, &#39;X[54] &lt;= 0.195\nsquared_error = 0.248\nsamples = 6\nvalue = 10.579&#39;),
 Text(0.1930445982933505, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.616&#39;),
 Text(0.19368861696989212, 0.03125, &#39;squared_error = 0.075\nsamples = 5\nvalue = 10.772&#39;),
 Text(0.19529866366124618, 0.15625, &#39;X[48] &lt;= 0.926\nsquared_error = 0.261\nsamples = 94\nvalue = 11.433&#39;),
 Text(0.19465464498470456, 0.09375, &#39;X[60] &lt;= 0.247\nsquared_error = 0.24\nsamples = 91\nvalue = 11.404&#39;),
 Text(0.19433263564643374, 0.03125, &#39;squared_error = 0.19\nsamples = 24\nvalue = 11.151&#39;),
 Text(0.19497665432297537, 0.03125, &#39;squared_error = 0.227\nsamples = 67\nvalue = 11.495&#39;),
 Text(0.1959426823377878, 0.09375, &#39;X[47] &lt;= 0.49\nsquared_error = 0.103\nsamples = 3\nvalue = 12.313&#39;),
 Text(0.195620672999517, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.766&#39;),
 Text(0.19626469167605862, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 12.087&#39;),
 Text(0.199404282724199, 0.40625, &#39;X[50] &lt;= 0.005\nsquared_error = 0.338\nsamples = 848\nvalue = 11.458&#39;),
 Text(0.19739172436000643, 0.34375, &#39;X[45] &lt;= 0.431\nsquared_error = 8.712\nsamples = 4\nvalue = 9.713&#39;),
 Text(0.19706971502173562, 0.28125, &#39;X[43] &lt;= 0.513\nsquared_error = 0.019\nsamples = 3\nvalue = 11.416&#39;),
 Text(0.1967477056834648, 0.21875, &#39;X[0] &lt;= 28600.0\nsquared_error = 0.001\nsamples = 2\nvalue = 11.32&#39;),
 Text(0.19642569634519402, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.29&#39;),
 Text(0.19706971502173562, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.35&#39;),
 Text(0.19739172436000643, 0.21875, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.608&#39;),
 Text(0.19771373369827724, 0.28125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 4.605&#39;),
 Text(0.20141684108839156, 0.34375, &#39;X[9] &lt;= 1.5\nsquared_error = 0.283\nsamples = 844\nvalue = 11.467&#39;),
 Text(0.19835775237481887, 0.28125, &#39;X[4] &lt;= 1.5\nsquared_error = 0.463\nsamples = 60\nvalue = 11.87&#39;),
 Text(0.19803574303654806, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 14.344&#39;),
 Text(0.19867976171308968, 0.21875, &#39;X[53] &lt;= 0.938\nsquared_error = 0.366\nsamples = 59\nvalue = 11.828&#39;),
 Text(0.19787473836741265, 0.15625, &#39;X[61] &lt;= 0.122\nsquared_error = 0.248\nsamples = 57\nvalue = 11.767&#39;),
 Text(0.19723071969087103, 0.09375, &#39;X[52] &lt;= 0.503\nsquared_error = 0.197\nsamples = 4\nvalue = 11.048&#39;),
 Text(0.1969087103526002, 0.03125, &#39;squared_error = 0.027\nsamples = 3\nvalue = 10.806&#39;),
 Text(0.19755272902914184, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.775&#39;),
 Text(0.19851875704395427, 0.09375, &#39;X[60] &lt;= 0.696\nsquared_error = 0.21\nsamples = 53\nvalue = 11.822&#39;),
 Text(0.19819674770568346, 0.03125, &#39;squared_error = 0.15\nsamples = 38\nvalue = 11.957&#39;),
 Text(0.1988407663822251, 0.03125, &#39;squared_error = 0.201\nsamples = 15\nvalue = 11.48&#39;),
 Text(0.1994847850587667, 0.15625, &#39;X[53] &lt;= 0.968\nsquared_error = 0.623\nsamples = 2\nvalue = 13.555&#39;),
 Text(0.1991627757204959, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 14.344&#39;),
 Text(0.19980679439703752, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.766&#39;),
 Text(0.20447592980196425, 0.28125, &#39;X[0] &lt;= 80520.0\nsquared_error = 0.256\nsamples = 784\nvalue = 11.436&#39;),
 Text(0.202382869103204, 0.21875, &#39;X[4] &lt;= 2.5\nsquared_error = 0.236\nsamples = 701\nvalue = 11.404&#39;),
 Text(0.20109483175012074, 0.15625, &#39;X[54] &lt;= 0.839\nsquared_error = 0.261\nsamples = 243\nvalue = 11.278&#39;),
 Text(0.20045081307357912, 0.09375, &#39;X[1] &lt;= 1850.0\nsquared_error = 0.196\nsamples = 202\nvalue = 11.334&#39;),
 Text(0.20012880373530834, 0.03125, &#39;squared_error = 0.192\nsamples = 157\nvalue = 11.39&#39;),
 Text(0.20077282241184993, 0.03125, &#39;squared_error = 0.158\nsamples = 45\nvalue = 11.138&#39;),
 Text(0.20173885042666237, 0.09375, &#39;X[47] &lt;= 0.469\nsquared_error = 0.493\nsamples = 41\nvalue = 11.002&#39;),
 Text(0.20141684108839156, 0.03125, &#39;squared_error = 0.61\nsamples = 16\nvalue = 10.514&#39;),
 Text(0.20206085976493318, 0.03125, &#39;squared_error = 0.169\nsamples = 25\nvalue = 11.314&#39;),
 Text(0.20367090645628724, 0.15625, &#39;X[1] &lt;= 958.5\nsquared_error = 0.209\nsamples = 458\nvalue = 11.472&#39;),
 Text(0.20302688777974562, 0.09375, &#39;X[0] &lt;= 3700.0\nsquared_error = 0.176\nsamples = 27\nvalue = 11.098&#39;),
 Text(0.2027048784414748, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.206&#39;),
 Text(0.20334889711801643, 0.03125, &#39;squared_error = 0.133\nsamples = 26\nvalue = 11.055&#39;),
 Text(0.20431492513282884, 0.09375, &#39;X[48] &lt;= 0.957\nsquared_error = 0.202\nsamples = 431\nvalue = 11.495&#39;),
 Text(0.20399291579455806, 0.03125, &#39;squared_error = 0.193\nsamples = 413\nvalue = 11.479&#39;),
 Text(0.20463693447109965, 0.03125, &#39;squared_error = 0.248\nsamples = 18\nvalue = 11.874&#39;),
 Text(0.20656899050072453, 0.21875, &#39;X[50] &lt;= 0.928\nsquared_error = 0.351\nsamples = 83\nvalue = 11.702&#39;),
 Text(0.2059249718241829, 0.15625, &#39;X[26] &lt;= 1.5\nsquared_error = 0.293\nsamples = 79\nvalue = 11.654&#39;),
 Text(0.2056029624859121, 0.09375, &#39;X[61] &lt;= 0.951\nsquared_error = 0.255\nsamples = 78\nvalue = 11.631&#39;),
 Text(0.20528095314764128, 0.03125, &#39;squared_error = 0.231\nsamples = 75\nvalue = 11.664&#39;),
 Text(0.2059249718241829, 0.03125, &#39;squared_error = 0.121\nsamples = 3\nvalue = 10.798&#39;),
 Text(0.2062469811624537, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 13.459&#39;),
 Text(0.20721300917726615, 0.15625, &#39;X[45] &lt;= 0.841\nsquared_error = 0.54\nsamples = 4\nvalue = 12.654&#39;),
 Text(0.20689099983899534, 0.09375, &#39;X[48] &lt;= 0.393\nsquared_error = 0.063\nsamples = 3\nvalue = 12.248&#39;),
 Text(0.20656899050072453, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.918&#39;),
 Text(0.20721300917726615, 0.03125, &#39;squared_error = 0.012\nsamples = 2\nvalue = 12.413&#39;),
 Text(0.20753501851553696, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 13.87&#39;),
 Text(0.1830748872967316, 0.59375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 5.106&#39;),
 Text(0.3660186438375463, 0.78125, &#39;X[1] &lt;= 1581.0\nsquared_error = 0.509\nsamples = 8801\nvalue = 11.858&#39;),
 Text(0.2880914405892771, 0.71875, &#39;X[19] &lt;= 1.5\nsquared_error = 0.59\nsamples = 2586\nvalue = 11.65&#39;),
 Text(0.2612602640476574, 0.65625, &#39;X[29] &lt;= 0.5\nsquared_error = 0.538\nsamples = 2247\nvalue = 11.685&#39;),
 Text(0.2408705321204315, 0.59375, &#39;X[21] &lt;= 1.5\nsquared_error = 0.557\nsamples = 1847\nvalue = 11.652&#39;),
 Text(0.22988951054580584, 0.53125, &#39;X[0] &lt;= 301467.0\nsquared_error = 0.534\nsamples = 1817\nvalue = 11.662&#39;),
 Text(0.21970093382708097, 0.46875, &#39;X[0] &lt;= 1537.5\nsquared_error = 0.53\nsamples = 1788\nvalue = 11.653&#39;),
 Text(0.21027209789083884, 0.40625, &#39;X[44] &lt;= 0.033\nsquared_error = 2.919\nsamples = 53\nvalue = 11.306&#39;),
 Text(0.2096280792142972, 0.34375, &#39;X[56] &lt;= 0.728\nsquared_error = 31.115\nsamples = 2\nvalue = 5.578&#39;),
 Text(0.2093060698760264, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 0.0&#39;),
 Text(0.20995008855256803, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.156&#39;),
 Text(0.21091611656738046, 0.34375, &#39;X[47] &lt;= 0.036\nsquared_error = 0.476\nsamples = 51\nvalue = 11.531&#39;),
 Text(0.21059410722910965, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 8.517&#39;),
 Text(0.21123812590565128, 0.28125, &#39;X[2] &lt;= 1965.0\nsquared_error = 0.301\nsamples = 50\nvalue = 11.591&#39;),
 Text(0.209145065206891, 0.21875, &#39;X[59] &lt;= 0.558\nsquared_error = 0.801\nsamples = 7\nvalue = 11.013&#39;),
 Text(0.20850104653034937, 0.15625, &#39;X[61] &lt;= 0.223\nsquared_error = 0.302\nsamples = 2\nvalue = 9.76&#39;),
 Text(0.20817903719207856, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.309&#39;),
 Text(0.20882305586862018, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 9.21&#39;),
 Text(0.20978908388343262, 0.15625, &#39;X[62] &lt;= 0.133\nsquared_error = 0.121\nsamples = 5\nvalue = 11.515&#39;),
 Text(0.2094670745451618, 0.09375, &#39;squared_error = 0.0\nsamples = 2\nvalue = 11.918&#39;),
 Text(0.21011109322170343, 0.09375, &#39;X[0] &lt;= 1300.0\nsquared_error = 0.02\nsamples = 3\nvalue = 11.246&#39;),
 Text(0.20978908388343262, 0.03125, &#39;squared_error = 0.005\nsamples = 2\nvalue = 11.154&#39;),
 Text(0.21043310255997424, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.43&#39;),
 Text(0.21333118660441153, 0.21875, &#39;X[53] &lt;= 0.309\nsquared_error = 0.156\nsamples = 43\nvalue = 11.685&#39;),
 Text(0.21204314925132828, 0.15625, &#39;X[59] &lt;= 0.162\nsquared_error = 0.1\nsamples = 6\nvalue = 11.21&#39;),
 Text(0.21139913057478668, 0.09375, &#39;X[48] &lt;= 0.799\nsquared_error = 0.021\nsamples = 3\nvalue = 11.489&#39;),
 Text(0.21107712123651587, 0.03125, &#39;squared_error = 0.002\nsamples = 2\nvalue = 11.39&#39;),
 Text(0.21172113991305747, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.687&#39;),
 Text(0.2126871679278699, 0.09375, &#39;X[60] &lt;= 0.217\nsquared_error = 0.025\nsamples = 3\nvalue = 10.932&#39;),
 Text(0.2123651585895991, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.156&#39;),
 Text(0.21300917726614071, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 10.82&#39;),
 Text(0.21461922395749478, 0.15625, &#39;X[47] &lt;= 0.635\nsquared_error = 0.122\nsamples = 37\nvalue = 11.762&#39;),
 Text(0.21397520528095315, 0.09375, &#39;X[27] &lt;= 0.5\nsquared_error = 0.083\nsamples = 19\nvalue = 11.959&#39;),
 Text(0.21365319594268234, 0.03125, &#39;squared_error = 0.062\nsamples = 16\nvalue = 11.885&#39;),
 Text(0.21429721461922396, 0.03125, &#39;squared_error = 0.011\nsamples = 3\nvalue = 12.355&#39;),
 Text(0.2152632426340364, 0.09375, &#39;X[61] &lt;= 0.947\nsquared_error = 0.079\nsamples = 18\nvalue = 11.555&#39;),
 Text(0.2149412332957656, 0.03125, &#39;squared_error = 0.049\nsamples = 17\nvalue = 11.511&#39;),
 Text(0.21558525197230718, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.301&#39;),
 Text(0.22912976976332314, 0.40625, &#39;X[28] &lt;= 0.5\nsquared_error = 0.453\nsamples = 1735\nvalue = 11.664&#39;),
 Text(0.22311222025438737, 0.34375, &#39;X[2] &lt;= 1935.0\nsquared_error = 0.478\nsamples = 1559\nvalue = 11.645&#39;),
 Text(0.22025438737723393, 0.28125, &#39;X[8] &lt;= 3.5\nsquared_error = 0.273\nsamples = 76\nvalue = 11.921&#39;),
 Text(0.2184833360167445, 0.21875, &#39;X[51] &lt;= 0.689\nsquared_error = 0.249\nsamples = 69\nvalue = 11.853&#39;),
 Text(0.21719529866366125, 0.15625, &#39;X[0] &lt;= 4856.0\nsquared_error = 0.186\nsamples = 39\nvalue = 12.018&#39;),
 ...]
</pre></div>
</div>
<img alt="../_images/2_introduction_to_machine_learning_73_1.png" src="../_images/2_introduction_to_machine_learning_73_1.png" />
</div>
</div>
<p>To reduce the complexity of the tree, we <strong>prune</strong> the tree: we collapse its leaves, permitting bias to increase but forcing variance to decrease until the desired trade-off is achieved. In <code class="docutils literal notranslate"><span class="pre">rpart</span></code>, this is done by considering a modified loss function that takes into account the number of terminal nodes (i.e., the number of regions in which the original data was partitioned). Somewhat heuristically, if we denote tree predictions by <span class="math notranslate nohighlight">\(T(x)\)</span> and its number of terminal nodes by  <span class="math notranslate nohighlight">\(|T|\)</span>, the modified regression problem can be written as:</p>
<div class="math notranslate nohighlight" id="equation-pruned-tree">
<span class="eqno">(2.4)<a class="headerlink" href="#equation-pruned-tree" title="Permalink to this equation">#</a></span>\[
  \widehat{T} = \arg\min_{T} \sum_{i=1}^m \left( T(X_i) - Y_i \right)^2 + c_p |T|
\]</div>
<p>The complexity of the tree is controlled by the scalar parameter <span class="math notranslate nohighlight">\(c_p\)</span>, denoted as <code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> in <code class="docutils literal notranslate"><span class="pre">sklearn.tree.DecisionTreeRegressor</span></code>. For each value of <span class="math notranslate nohighlight">\(c_p\)</span>, we find the subtree that solves <a class="reference internal" href="#equation-pruned-tree">(2.4)</a>. Large values of <span class="math notranslate nohighlight">\(c_p\)</span> lead to aggressively pruned trees, which have more bias and less variance. Small values of <span class="math notranslate nohighlight">\(c_p\)</span> allow for deeper trees whose predictions can vary more wildly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">itertools</span>
<span class="n">path</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">cost_complexity_pruning_path</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">alphas_dt</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">path</span><span class="p">[</span><span class="s1">&#39;ccp_alphas&#39;</span><span class="p">],</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;alphas&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># A function with a manual cross validation</span>
<span class="c1">#This function can replicate cp_table that R&#39;s rplot package creates to get the best complexity parameter</span>
<span class="c1">#This function can be used to prune the tree but it is a lar process, so if you have the computational power, you can use this function</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">def run_cross_validation_on_trees2(X, y, tree_ccp, nfold=10):</span>

<span class="sd">    cp_table_error = []</span>
<span class="sd">    cp_table_std = []</span>
<span class="sd">    cp_table_rel_error = []</span>
<span class="sd">    cp_table_size = []</span>
<span class="sd">   </span>
<span class="sd">     # Num ob observations</span>
<span class="sd">    nobs = y.shape[0]</span>
<span class="sd">    </span>
<span class="sd">    # Define folds indices </span>
<span class="sd">    list_1 = [*range(0, nfold, 1)]*nobs</span>
<span class="sd">    sample = np.random.choice(nobs,nobs, replace=False).tolist()</span>
<span class="sd">    foldid = [list_1[index] for index in sample]</span>

<span class="sd">    # Create split function(similar to R)</span>
<span class="sd">    def split(x, f):</span>
<span class="sd">        count = max(f) + 1</span>
<span class="sd">        return tuple( list(itertools.compress(x, (el == i for el in f))) for i in range(count) ) </span>

<span class="sd">    # Split observation indices into folds </span>
<span class="sd">    list_2 = [*range(0, nobs, 1)]</span>
<span class="sd">    I = split(list_2, foldid)</span>
<span class="sd">    </span>
<span class="sd">    for i in tree_ccp:</span>
<span class="sd">        cv_error_list = []</span>
<span class="sd">        cv_rel_error_list = []</span>
<span class="sd">        </span>
<span class="sd">        dtree = DecisionTreeRegressor( ccp_alpha= i, random_state = 0)</span>
<span class="sd">        </span>
<span class="sd">    # loop to save results</span>
<span class="sd">        for b in range(0,len(I)):</span>
<span class="sd">            </span>
<span class="sd">            # Split data - index to keep are in mask as booleans</span>
<span class="sd">            include_idx = set(I[b])  #Here should go I[b] Set is more efficient, but doesn&#39;t reorder your elements if that is desireable</span>
<span class="sd">            mask = np.array([(a in include_idx) for a in range(len(y))])</span>
<span class="sd">            </span>
<span class="sd">            dtree.fit(X[~mask], Y[~mask])</span>
<span class="sd">            pred = dtree.predict(X[mask])</span>
<span class="sd">            xerror_fold = np.mean(np.power(pred - y[mask],2))</span>
<span class="sd">            rel_error_fold = 1- r2_score(y[mask], pred)</span>
<span class="sd">            </span>
<span class="sd">            cv_error_list.append(xerror_fold)</span>
<span class="sd">            cv_rel_error_list.append(rel_error_fold)</span>
<span class="sd">            </span>
<span class="sd">        rel_error = np.mean(cv_rel_error_list)</span>
<span class="sd">        xerror = np.mean(cv_error_list)</span>
<span class="sd">        xstd = np.std(cv_error_list)</span>

<span class="sd">        cp_table_rel_error.append(rel_error)</span>
<span class="sd">        cp_table_error.append(xerror)</span>
<span class="sd">        cp_table_std.append(xstd)</span>
<span class="sd">        cp_table_size.append(dtree.tree_.node_count)</span>
<span class="sd">    cp_table = pd.DataFrame([pd.Series(tree_ccp, name = &quot;cp&quot;), pd.Series(cp_table_size, name = &quot;size&quot;)</span>
<span class="sd">                        , pd.Series(cp_table_rel_error, name = &quot;rel error&quot;),</span>
<span class="sd">                         pd.Series(cp_table_error, name = &quot;xerror&quot;),</span>
<span class="sd">                         pd.Series(cp_table_std, name = &quot;xstd&quot;)]).T    </span>
<span class="sd">    return cp_table</span>
<span class="sd">&#39;&#39;&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Here we create a loop to get an arrange with all Mean Squared Errors for each cp_alpha</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="n">mse_gini</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">cp_table_size</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">alphas_dt</span><span class="p">:</span>
    <span class="n">dtree</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span> <span class="n">ccp_alpha</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">dtree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">dtree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
    <span class="n">mse_gini</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">))</span>
    <span class="n">cp_table_size</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dtree</span><span class="o">.</span><span class="n">tree_</span><span class="o">.</span><span class="n">node_count</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;acc_gini&#39;</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">mse_gini</span><span class="p">),</span><span class="s1">&#39;ccp_alphas&#39;</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">alphas_dt</span><span class="p">)})</span>

<span class="c1">#plt.style.context(&quot;dark_background&quot;)</span>

<span class="c1"># visualizing changes in parameters</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">facecolor</span> <span class="o">=</span> <span class="s2">&quot;white&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="s1">&#39;ccp_alphas&#39;</span><span class="p">,</span><span class="s1">&#39;acc_gini&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">d2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="c1">#plt.gca().invert_xaxis()</span>


<span class="c1">#plt.xticks(np.arange(0, 0.15, step=0.01))  # Set label locations.</span>
<span class="c1">#plt.yticks(np.arange(0.5, 1.5, step=0.1))  # Set label locations.</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">labelrotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">labelrotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>


<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;cp&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x229901244c0&gt;
</pre></div>
</div>
<img alt="../_images/2_introduction_to_machine_learning_78_1.png" src="../_images/2_introduction_to_machine_learning_78_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#It is a function to get the best max_depth parametor with cross-validation</span>
<span class="k">def</span> <span class="nf">prune_max_depth</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">nfold</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">cv_mean_mse</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">max_depth</span> <span class="o">=</span> <span class="p">[]</span>
     <span class="c1"># Num ob observations</span>
    <span class="n">nobs</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># Define folds indices </span>
    <span class="n">list_1</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nfold</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span><span class="o">*</span><span class="n">nobs</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">nobs</span><span class="p">,</span><span class="n">nobs</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">foldid</span> <span class="o">=</span> <span class="p">[</span><span class="n">list_1</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">sample</span><span class="p">]</span>

    <span class="c1"># Create split function(similar to R)</span>
    <span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
        <span class="n">count</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span> <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">compress</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">el</span> <span class="o">==</span> <span class="n">i</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">f</span><span class="p">)))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">count</span><span class="p">)</span> <span class="p">)</span> 

    <span class="c1"># Split observation indices into folds </span>
    <span class="n">list_2</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nobs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
    <span class="n">I</span> <span class="o">=</span> <span class="n">split</span><span class="p">(</span><span class="n">list_2</span><span class="p">,</span> <span class="n">foldid</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">):</span>
        <span class="n">max_depth</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="n">mse_depth</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">dtree</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span> <span class="n">max_depth</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">I</span><span class="p">)):</span>
            
            <span class="c1"># Split data - index to keep are in mask as booleans</span>
            <span class="n">include_idx</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">I</span><span class="p">[</span><span class="n">b</span><span class="p">])</span>  <span class="c1">#Here should go I[b] Set is more efficient, but doesn&#39;t reorder your elements if that is desireable</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([(</span><span class="n">a</span> <span class="ow">in</span> <span class="n">include_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))])</span>
            
            <span class="n">dtree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">])</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">dtree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span>
            <span class="n">mse_depth</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">mask</span><span class="p">],</span><span class="n">pred</span><span class="p">))</span>
            
        <span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mse_depth</span><span class="p">)</span>
        <span class="n">cv_mean_mse</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mse</span><span class="p">)</span>
    
    <span class="n">d1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;acc_depth&#39;</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">cv_mean_mse</span><span class="p">),</span><span class="s1">&#39;max_depth&#39;</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">max_depth</span><span class="p">)})</span>
    <span class="k">return</span> <span class="n">d1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d1</span> <span class="o">=</span> <span class="n">prune_max_depth</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># visualizing changes in parameters</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="s1">&#39;max_depth&#39;</span><span class="p">,</span><span class="s1">&#39;acc_depth&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">d1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;max_depth&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;mse&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x229926aff40&gt;
</pre></div>
</div>
<img alt="../_images/2_introduction_to_machine_learning_81_1.png" src="../_images/2_introduction_to_machine_learning_81_1.png" />
</div>
</div>
<p>The following code retrieves the optimal parameter and prunes the tree. Here, instead of choosing the parameter that minimizes the mean-squared-error, we’re following another common heuristic: we will choose the most regularized model whose error is within one standard error of the minimum error.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We get the best parameters</span>
<span class="n">best_max_depth</span> <span class="o">=</span> <span class="n">d1</span><span class="p">[</span><span class="n">d1</span><span class="p">[</span><span class="s2">&quot;acc_depth&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">d1</span><span class="p">[</span><span class="s2">&quot;acc_depth&quot;</span><span class="p">])]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">best_ccp</span> <span class="o">=</span> <span class="n">d2</span><span class="p">[</span><span class="n">d2</span><span class="p">[</span><span class="s2">&quot;acc_gini&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">d2</span><span class="p">[</span><span class="s2">&quot;acc_gini&quot;</span><span class="p">])]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Prune the tree</span>
<span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">best_max_depth</span> <span class="p">,</span> <span class="n">ccp_alpha</span><span class="o">=</span> <span class="n">best_ccp</span> <span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tree1</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Plotting the pruned tree. See also the package <a class="reference external" href="http://www.milbo.org/rpart-plot/prp.pdf">rpart.plot</a> for more advanced plotting capabilities.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span><span class="mi">16</span><span class="p">))</span>
<span class="n">tree</span><span class="o">.</span><span class="n">plot_tree</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rounded</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">feature_names</span> <span class="o">=</span> <span class="n">XX</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Text(0.65, 0.9, &#39;NUNIT2 &lt;= 3.5\nsquared_error = 1.01\nsamples = 20108\nvalue = 11.812&#39;),
 Text(0.4, 0.7, &#39;UNITSF &lt;= 2436.5\nsquared_error = 0.823\nsamples = 19378\nvalue = 11.884&#39;),
 Text(0.2, 0.5, &#39;BATHS &lt;= 1.5\nsquared_error = 0.698\nsamples = 13909\nvalue = 11.68&#39;),
 Text(0.1, 0.3, &#39;KITCH &lt;= 0.5\nsquared_error = 0.782\nsamples = 5112\nvalue = 11.38&#39;),
 Text(0.05, 0.1, &#39;squared_error = 0.0\nsamples = 1\nvalue = 0.0&#39;),
 Text(0.15, 0.1, &#39;squared_error = 0.757\nsamples = 5111\nvalue = 11.382&#39;),
 Text(0.3, 0.3, &#39;UNITSF &lt;= 1692.0\nsquared_error = 0.567\nsamples = 8797\nvalue = 11.854&#39;),
 Text(0.25, 0.1, &#39;squared_error = 0.533\nsamples = 3227\nvalue = 11.684&#39;),
 Text(0.35, 0.1, &#39;squared_error = 0.56\nsamples = 5570\nvalue = 11.953&#39;),
 Text(0.6, 0.5, &#39;BATHS &lt;= 2.5\nsquared_error = 0.768\nsamples = 5469\nvalue = 12.402&#39;),
 Text(0.5, 0.3, &#39;BATHS &lt;= 1.5\nsquared_error = 0.739\nsamples = 2839\nvalue = 12.156&#39;),
 Text(0.45, 0.1, &#39;squared_error = 1.112\nsamples = 328\nvalue = 11.625&#39;),
 Text(0.55, 0.1, &#39;squared_error = 0.649\nsamples = 2511\nvalue = 12.225&#39;),
 Text(0.7, 0.3, &#39;UNITSF &lt;= 3999.0\nsquared_error = 0.664\nsamples = 2630\nvalue = 12.667&#39;),
 Text(0.65, 0.1, &#39;squared_error = 0.538\nsamples = 1645\nvalue = 12.495&#39;),
 Text(0.75, 0.1, &#39;squared_error = 0.742\nsamples = 985\nvalue = 12.954&#39;),
 Text(0.9, 0.7, &#39;MOBILTYP &lt;= 1.5\nsquared_error = 2.186\nsamples = 730\nvalue = 9.901&#39;),
 Text(0.85, 0.5, &#39;UNITSF &lt;= 15977.5\nsquared_error = 2.4\nsamples = 417\nvalue = 9.372&#39;),
 Text(0.8, 0.3, &#39;squared_error = 2.194\nsamples = 416\nvalue = 9.394&#39;),
 Text(0.9, 0.3, &#39;squared_error = -0.0\nsamples = 1\nvalue = 0.0&#39;),
 Text(0.95, 0.5, &#39;squared_error = 1.031\nsamples = 313\nvalue = 10.606&#39;)]
</pre></div>
</div>
<img alt="../_images/2_introduction_to_machine_learning_85_1.png" src="../_images/2_introduction_to_machine_learning_85_1.png" />
</div>
</div>
<p>Finally, here’s how to extract predictions and mse estimates from the pruned tree.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tree MSE estimate:&quot;</span><span class="p">,</span> <span class="n">mse</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tree MSE estimate: 0.5442705453177679
</pre></div>
</div>
</div>
</div>
<p>It’s often said that trees are “interpretable.” To some extent, that’s true – we can look at the tree and clearly visualize the mapping from inputs to prediction. This can be important in settings in which conveying how one got to a prediction is important. For example, if a decision tree were to be used for credit scoring, it would be easy to explain to a client how their credit was scored.</p>
<p>Beyond that, however, there are several reasons for not interpreting the obtained decision tree further. First, even though a tree may have used a particular variable for a split, that does not mean that it’s indeed an important variable: if two covariates are highly correlated, the tree may split on one variable but not the other, and there’s no guarantee which variables are relevant in the underlying data-generating process.</p>
<p>Similar to what we did for Lasso above, we can estimate the average value of each covariate per leaf. Although results are noisier here because there are many leaves, we see somewhat similar trends in that houses with higher predictions are also correlated with more bedrooms, bathrooms and room sizes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">Series</span>
<span class="kn">from</span> <span class="nn">simple_colors</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span>
<span class="n">num_leaves</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>

<span class="n">categ</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">categories</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)))</span>
<span class="n">leaf</span> <span class="o">=</span> <span class="n">categ</span><span class="o">.</span><span class="n">rename_categories</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">categ</span><span class="o">.</span><span class="n">categories</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>

<span class="n">data1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span> <span class="n">covariates</span><span class="p">)</span>
<span class="n">data1</span><span class="p">[</span><span class="s2">&quot;leaf&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">leaf</span>

<span class="k">for</span> <span class="n">var_name</span> <span class="ow">in</span> <span class="n">covariates</span><span class="p">:</span>
    <span class="n">form2</span> <span class="o">=</span> <span class="n">var_name</span> <span class="o">+</span> <span class="s2">&quot; ~ &quot;</span> <span class="o">+</span> <span class="s2">&quot;0&quot;</span> <span class="o">+</span> <span class="s2">&quot;+&quot;</span> <span class="o">+</span> <span class="s2">&quot;leaf&quot;</span>
    <span class="n">ols</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="n">form2</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">cov_type</span> <span class="o">=</span> <span class="s1">&#39;HC2&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">summary2</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">red</span><span class="p">(</span><span class="n">var_name</span><span class="p">,</span> <span class="s1">&#39;bold&#39;</span><span class="p">),</span><span class="n">ols</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Red">LOT</span>                leaf[1]        leaf[2]       leaf[3]       leaf[4]  \
Coef.     76491.899559  129102.123280  35058.201371  59815.230609   
Std.Err.  12405.242699   17921.335635   2134.642203  11159.843199   

               leaf[5]       leaf[6]       leaf[7]       leaf[8]       leaf[9]  
Coef.     37474.451548  44275.523233  54379.653727  49666.380327  77444.878973  
Std.Err.   2632.981719   2282.706417   3883.339922   4296.772925   7871.645357   

<span class=" -Color -Color-Bold -Color-Bold-Red">UNITSF</span>               leaf[1]      leaf[2]      leaf[3]      leaf[4]      leaf[5]  \
Coef.     1253.436921  1919.370259  1564.492235  5085.737805  1368.844174   
Std.Err.    43.006720   151.630637    12.265223   375.593742     5.763695   

              leaf[6]      leaf[7]      leaf[8]      leaf[9]  
Coef.     2087.609012  3608.566364  3138.982620  8380.256000  
Std.Err.     5.101007    69.986674    14.813386   228.071867   

<span class=" -Color -Color-Bold -Color-Bold-Red">BUILT</span>               leaf[1]      leaf[2]      leaf[3]      leaf[4]      leaf[5]  \
Coef.     1982.409326  1986.656000  1951.951955  1949.652439  1978.756254   
Std.Err.     0.975004     1.125603     0.457493     1.686299     0.574931   

              leaf[6]      leaf[7]      leaf[8]      leaf[9]  
Coef.     1978.991221  1982.609091  1988.700535  1989.653333  
Std.Err.     0.464422     0.662736     0.685155     1.091059   

<span class=" -Color -Color-Bold -Color-Bold-Red">BATHS</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.538860  2.000000  0.997174  0.993902  2.028592  2.134197   
Std.Err.  0.036718  0.027824  0.001152  0.006098  0.004457  0.007777   

               leaf[7]   leaf[8]   leaf[9]  
Coef.     2.000000e+00  3.153743  3.634667  
Std.Err.  2.545218e-16  0.014613  0.040049   

<span class=" -Color -Color-Bold -Color-Bold-Red">BEDRMS</span>            leaf[1]   leaf[2]  leaf[3]   leaf[4]   leaf[5]   leaf[6]   leaf[7]  \
Coef.     2.430052  3.080000  2.72586  3.085366  2.934954  3.337793  3.620909   
Std.Err.  0.044477  0.057474  0.01540  0.057637  0.015716  0.014112  0.020111   

           leaf[8]   leaf[9]  
Coef.     4.056150  4.389333  
Std.Err.  0.025086  0.040948   

<span class=" -Color -Color-Bold -Color-Bold-Red">DINING</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.202073  0.608000  0.487989  0.713415  0.469621  0.717391   
Std.Err.  0.029896  0.050668  0.011054  0.037469  0.013947  0.010268   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     0.870000  0.921123  1.013333  
Std.Err.  0.013806  0.014918  0.019791   

<span class=" -Color -Color-Bold -Color-Bold-Red">METRO</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     6.367876  6.600000  4.587376  4.658537  5.609721  5.613294   
Std.Err.  0.131099  0.129515  0.063162  0.226951  0.066153  0.050662   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     5.976364  6.164439  6.402667  
Std.Err.  0.067004  0.074326  0.091031   

<span class=" -Color -Color-Bold -Color-Bold-Red">CRACKS</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.922280  1.944000  1.922280  1.932927  1.954253  1.948997   
Std.Err.  0.019322  0.020648  0.005812  0.019593  0.005588  0.004499   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     1.966364  1.967914  1.978667  
Std.Err.  0.005438  0.006448  0.007472   

<span class=" -Color -Color-Bold -Color-Bold-Red">REGION</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     2.766839  2.960000  2.471032  2.493902  2.869192  2.764214   
Std.Err.  0.045438  0.058365  0.015533  0.051534  0.017560  0.013070   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     2.674545  2.879679  2.826667  
Std.Err.  0.019998  0.022828  0.033958   

<span class=" -Color -Color-Bold -Color-Bold-Red">METRO3</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.891192  2.040000  1.682525  1.646341  2.057898  1.996656   
Std.Err.  0.022473  0.083008  0.021217  0.059137  0.040916  0.028187   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     2.001818  2.060160  2.176000  
Std.Err.  0.035992  0.046038  0.073729   

<span class=" -Color -Color-Bold -Color-Bold-Red">PHONE</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.601036  0.712000  0.672162  0.682927  0.711937  0.682274   
Std.Err.  0.128542  0.142247  0.035345  0.127639  0.040607  0.032536   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     0.684545  0.744652  0.562667  
Std.Err.  0.047985  0.052842  0.095394   

<span class=" -Color -Color-Bold -Color-Bold-Red">KITCHEN</span>            leaf[1]  leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]   leaf[7]  \
Coef.     1.005181    1.008  1.012718  1.006098  1.003574  1.004181  1.001818   
Std.Err.  0.005181    0.008  0.002433  0.006098  0.001596  0.001320  0.001285   

           leaf[8]       leaf[9]  
Coef.     1.005348  1.000000e+00  
Std.Err.  0.002668  1.033549e-16   

<span class=" -Color -Color-Bold -Color-Bold-Red">MOBILTYP</span>            leaf[1]       leaf[2]       leaf[3]       leaf[4]       leaf[5]  \
Coef.     0.979275  2.000000e+00 -1.000000e+00 -1.000000e+00 -1.000000e+00   
Std.Err.  0.014617  7.976078e-17  2.169102e-17  3.478375e-17  1.722204e-16   

               leaf[6]       leaf[7]       leaf[8]       leaf[9]  
Coef.    -1.000000e+00 -1.000000e+00 -1.000000e+00 -1.000000e+00  
Std.Err.  4.541240e-18  1.272609e-16  9.749025e-17  1.033349e-16   

<span class=" -Color -Color-Bold -Color-Bold-Red">WINTEROVEN</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.932642  1.832000  1.943005  1.975610  1.972838  1.973244   
Std.Err.  0.052497  0.112872  0.013459  0.012082  0.012155  0.009126   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     1.968182  1.998663  1.946667  
Std.Err.  0.014886  0.001337  0.034084   

<span class=" -Color -Color-Bold -Color-Bold-Red">WINTERKESP</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.880829  1.776000  1.940650  1.993902  1.964260  1.962375   
Std.Err.  0.054548  0.114082  0.013496  0.006098  0.012389  0.009356   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     1.953636  1.994652  1.936000  
Std.Err.  0.015290  0.002668  0.034452   

<span class=" -Color -Color-Bold -Color-Bold-Red">WINTERELSP</span>            leaf[1]  leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]   leaf[7]  \
Coef.     1.725389   1.6560  1.765897  1.817073  1.814868  1.832776  1.826364   
Std.Err.  0.058875   0.1159  0.015502  0.030281  0.015387  0.011429  0.018003   

           leaf[8]   leaf[9]  
Coef.     1.822193  1.832000  
Std.Err.  0.013989  0.037423   

<span class=" -Color -Color-Bold -Color-Bold-Red">WINTERWOOD</span>            leaf[1]  leaf[2]   leaf[3]       leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.948187  1.84000  1.962789  2.000000e+00  1.977841  1.977843   
Std.Err.  0.051813  0.11268  0.013142  6.956750e-17  0.012014  0.009025   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     1.973636  1.998663  1.949333  
Std.Err.  0.014728  0.001337  0.033990   

<span class=" -Color -Color-Bold -Color-Bold-Red">WINTERNONE</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.243523  1.104000  1.190768  1.182927  1.163688  1.153010   
Std.Err.  0.058209  0.111195  0.015119  0.030281  0.015059  0.011251   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     1.145455  1.183155  1.098667  
Std.Err.  0.017512  0.014152  0.035560   

<span class=" -Color -Color-Bold -Color-Bold-Red">NEWC</span>            leaf[1]   leaf[2]   leaf[3]       leaf[4]   leaf[5]   leaf[6]  \
Coef.    -8.844560 -8.600000 -8.962317 -9.000000e+00 -8.756969 -8.632107   
Std.Err.  0.089275  0.175977  0.013301  1.391350e-16  0.041185  0.038497   

           leaf[7]   leaf[8]   leaf[9]  
Coef.    -8.663636 -8.358289 -8.200000  
Std.Err.  0.054385  0.089662  0.140282   

<span class=" -Color -Color-Bold -Color-Bold-Red">DISH</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.606218  1.224000  1.495525  1.341463  1.105790  1.084448   
Std.Err.  0.035261  0.037441  0.010854  0.037142  0.008226  0.005687   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     1.046364  1.010695  1.005333  
Std.Err.  0.006343  0.003764  0.003766   

<span class=" -Color -Color-Bold -Color-Bold-Red">WASH</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.093264  1.040000  1.053227  1.024390  1.015011  1.007107   
Std.Err.  0.020987  0.017598  0.004873  0.012082  0.003252  0.001718   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     1.003636  1.002674  1.002667  
Std.Err.  0.001816  0.001889  0.002667   

<span class=" -Color -Color-Bold -Color-Bold-Red">DRY</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.155440  1.040000  1.078662  1.060976  1.026447  1.010452   
Std.Err.  0.026148  0.017598  0.005844  0.018742  0.004292  0.002080   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     1.003636  1.002674  1.005333  
Std.Err.  0.001816  0.001889  0.003766   

<span class=" -Color -Color-Bold -Color-Bold-Red">NUNIT2</span>                leaf[1]       leaf[2]   leaf[3]  leaf[4]   leaf[5]   leaf[6]  \
Coef.     4.000000e+00  4.000000e+00  1.163919  1.04878  1.230164  1.076505   
Std.Err.  1.922963e-16  1.595216e-16  0.011039  0.02084  0.014687  0.006598   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     1.025455  1.030749  1.008000  
Std.Err.  0.005701  0.007363  0.005956   

<span class=" -Color -Color-Bold -Color-Bold-Red">BURNER</span>            leaf[1]   leaf[2]   leaf[3]       leaf[4]   leaf[5]   leaf[6]  \
Coef.    -5.803109 -5.816000 -5.914743 -6.000000e+00 -5.962116 -5.976589   
Std.Err.  0.087316  0.105576  0.017701  3.478375e-16  0.014319  0.008838   

          leaf[7]   leaf[8]   leaf[9]  
Coef.    -5.98000 -5.989305 -5.981333  
Std.Err.  0.01156  0.010695  0.018667   

<span class=" -Color -Color-Bold -Color-Bold-Red">COOK</span>            leaf[1]   leaf[2]   leaf[3]       leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.025907  1.024000  1.010834  1.000000e+00  1.005004  1.002926   
Std.Err.  0.011465  0.013744  0.002247  3.478375e-17  0.001887  0.001105   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     1.002727  1.001337  1.002667  
Std.Err.  0.001573  0.001337  0.002667   

<span class=" -Color -Color-Bold -Color-Bold-Red">OVEN</span>            leaf[1]   leaf[2]   leaf[3]       leaf[4]   leaf[5]   leaf[6]  \
Coef.    -5.891192 -5.888000 -5.931700 -6.000000e+00 -5.979271 -5.979515   
Std.Err.  0.062492  0.078876  0.015231  3.478375e-16  0.010372  0.007733   

           leaf[7]   leaf[8]       leaf[9]  
Coef.    -5.993636 -5.989305 -6.000000e+00  
Std.Err.  0.006364  0.010695  2.758533e-16   

<span class=" -Color -Color-Bold -Color-Bold-Red">REFR</span>            leaf[1]  leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]   leaf[7]  \
Coef.     1.005181    1.008  1.006123  1.006098  1.002144  1.001672  1.000909   
Std.Err.  0.005181    0.008  0.001694  0.006098  0.001237  0.000836  0.000909   

           leaf[8]       leaf[9]  
Coef.     1.004011  1.000000e+00  
Std.Err.  0.002312  1.033446e-16   

<span class=" -Color -Color-Bold -Color-Bold-Red">DENS</span>           leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]   leaf[7]  \
Coef.         0.0  0.144000  0.099859  0.189024  0.092924  0.195652  0.272727   
Std.Err.      0.0  0.031529  0.006643  0.035210  0.007895  0.008492  0.014095   

           leaf[8]   leaf[9]  
Coef.     0.328877  0.464000  
Std.Err.  0.018781  0.032404   

<span class=" -Color -Color-Bold -Color-Bold-Red">FAMRM</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.005181  0.072000  0.115874  0.182927  0.105075  0.287625   
Std.Err.  0.005181  0.025843  0.007012  0.030281  0.008447  0.009772   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     0.440909  0.537433  0.752000  
Std.Err.  0.017569  0.022223  0.040841   

<span class=" -Color -Color-Bold -Color-Bold-Red">HALFB</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.186528  0.096000  0.451248  0.676829  0.190136  0.408445   
Std.Err.  0.071847  0.028791  0.011710  0.045717  0.010925  0.010709   

           leaf[7]   leaf[8]  leaf[9]  
Coef.     0.813636  0.568182  0.91200  
Std.Err.  0.015878  0.021217  0.03503   

<span class=" -Color -Color-Bold -Color-Bold-Red">KITCH</span>                leaf[1]  leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.000000e+00    1.008  1.002355  1.006098  1.011437  1.012960   
Std.Err.  4.807407e-17    0.008  0.001052  0.006098  0.002844  0.002313   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     1.009091  1.036096  1.058667  
Std.Err.  0.002863  0.007082  0.012725   

<span class=" -Color -Color-Bold -Color-Bold-Red">LIVING</span>           leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]   leaf[7]  \
Coef.     1.00000  1.072000  1.012718  1.060976  1.017155  1.069816  1.085455   
Std.Err.  0.01039  0.025843  0.003608  0.020642  0.005965  0.006964  0.010913   

           leaf[8]   leaf[9]  
Coef.     1.160428  1.200000  
Std.Err.  0.017986  0.031595   

<span class=" -Color -Color-Bold -Color-Bold-Red">OTHFN</span>            leaf[1]  leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]   leaf[7]  \
Coef.     0.015544      0.0  0.069713  0.109756  0.060758  0.127926  0.223636   
Std.Err.  0.008927      0.0  0.006280  0.028704  0.007494  0.007965  0.015883   

           leaf[8]   leaf[9]  
Coef.     0.201872  0.384000  
Std.Err.  0.019956  0.036681   

<span class=" -Color -Color-Bold -Color-Bold-Red">RECRM</span>           leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]   leaf[7]  \
Coef.         0.0  0.032000  0.038625  0.091463  0.031451  0.070652  0.125455   
Std.Err.      0.0  0.015805  0.004236  0.022579  0.004882  0.005372  0.010397   

           leaf[8]   leaf[9]  
Coef.     0.212567  0.349333  
Std.Err.  0.015897  0.028163   

<span class=" -Color -Color-Bold -Color-Bold-Red">CLIMB</span>                leaf[1]       leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     2.308571e+00  2.308571e+00  2.261089  2.280418  2.253642  2.293378   
Std.Err.  3.204938e-17  3.988039e-17  0.018633  0.019846  0.016414  0.008611   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     2.304161  2.317617  2.302415  
Std.Err.  0.004208  0.012362  0.006156   

<span class=" -Color -Color-Bold -Color-Bold-Red">ELEV</span>                leaf[1]       leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.    -6.000000e+00 -6.000000e+00 -5.567122 -5.908537 -5.551108 -5.880435   
Std.Err.  5.127900e-16  2.392823e-16  0.036987  0.064621  0.046704  0.018774   

           leaf[7]   leaf[8]   leaf[9]  
Coef.    -5.960909 -5.959893 -5.981333  
Std.Err.  0.015944  0.020058  0.018667   

<span class=" -Color -Color-Bold -Color-Bold-Red">DIRAC</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.469150  1.520207  1.462422  1.425796  1.453864  1.402817   
Std.Err.  0.022588  0.024104  0.007016  0.029282  0.009189  0.008810   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     1.303784  1.275952  1.217845  
Std.Err.  0.016260  0.020886  0.030770   

<span class=" -Color -Color-Bold -Color-Bold-Red">PORCH</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.155440  1.088000  1.111163  1.036585  1.067191  1.047241   
Std.Err.  0.026148  0.025441  0.006824  0.014705  0.006696  0.004339   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     1.042727  1.028075  1.018667  
Std.Err.  0.006101  0.006044  0.006999   

<span class=" -Color -Color-Bold -Color-Bold-Red">AIRSYS</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.398964  1.120000  1.297221  1.243902  1.065046  1.068144   
Std.Err.  0.035340  0.029182  0.009921  0.033636  0.006596  0.005153   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     1.041818  1.036096  1.029333  
Std.Err.  0.006038  0.006825  0.008725   

<span class=" -Color -Color-Bold -Color-Bold-Red">WELL</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.    -0.709845 -0.584000 -0.894960 -0.853659 -0.896355 -0.876254   
Std.Err.  0.050833  0.075506  0.010056  0.043459  0.012176  0.010065   

           leaf[7]   leaf[8]   leaf[9]  
Coef.    -0.855455 -0.879679 -0.837333  
Std.Err.  0.015858  0.017400  0.028645   

<span class=" -Color -Color-Bold -Color-Bold-Red">WELDUS</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     4.507772  4.176000  4.762600  4.689024  4.802716  4.763378   
Std.Err.  0.094403  0.144608  0.020549  0.083221  0.022765  0.019461   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     4.699091  4.763369  4.696000  
Std.Err.  0.031655  0.034293  0.054157   

<span class=" -Color -Color-Bold -Color-Bold-Red">STEAM</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.    -0.129534  0.224000 -0.091851 -0.048780 -0.007148  0.035953   
Std.Err.  0.098258  0.132404  0.029913  0.109344  0.037755  0.029171   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     0.035455  0.163102 -0.002667  
Std.Err.  0.043003  0.053480  0.072981   

<span class=" -Color -Color-Bold -Color-Bold-Red">OARSYS</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.    -1.233161  0.952000 -0.409797  0.024390  1.425304  1.349498   
Std.Err.  0.280641  0.231922  0.079011  0.268272  0.052730  0.041123   

           leaf[7]  leaf[8]   leaf[9]  
Coef.     1.505455  1.34492  1.354667  
Std.Err.  0.048562  0.05485  0.070740   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise1</span>            leaf[1]   leaf[2]   leaf[3]  leaf[4]   leaf[5]   leaf[6]   leaf[7]  \
Coef.     0.505618  0.508176  0.501445  0.50980  0.508980  0.498352  0.506366   
Std.Err.  0.019987  0.026851  0.006171  0.02291  0.007757  0.005849  0.008779   

           leaf[8]   leaf[9]  
Coef.     0.498214  0.489948  
Std.Err.  0.010782  0.015011   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise2</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.479330  0.510887  0.490818  0.506432  0.503823  0.491863   
Std.Err.  0.021813  0.024629  0.006199  0.022006  0.007677  0.005916   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     0.519609  0.510273  0.522781  
Std.Err.  0.008804  0.010675  0.014784   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise3</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.531553  0.520926  0.503041  0.513428  0.495849  0.502136   
Std.Err.  0.020642  0.025942  0.006234  0.024108  0.007680  0.005939   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     0.497612  0.507676  0.498867  
Std.Err.  0.008658  0.010623  0.014714   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise4</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.523838  0.508204  0.501693  0.490069  0.491062  0.496855   
Std.Err.  0.020465  0.025079  0.006255  0.023845  0.007588  0.005855   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     0.512434  0.511926  0.499861  
Std.Err.  0.008656  0.010537  0.015507   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise5</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.478568  0.523951  0.499522  0.507624  0.496674  0.500685   
Std.Err.  0.021662  0.025191  0.006281  0.022255  0.007758  0.005944   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     0.503707  0.486443  0.484459  
Std.Err.  0.008934  0.010450  0.015176   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise6</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.476431  0.498303  0.505307  0.499575  0.510473  0.508335   
Std.Err.  0.020660  0.026290  0.006271  0.022161  0.007732  0.005895   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     0.506212  0.497468  0.493743  
Std.Err.  0.008799  0.010308  0.015215   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise7</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.466509  0.532502  0.496970  0.518808  0.498091  0.503088   
Std.Err.  0.020272  0.026065  0.006161  0.023536  0.007586  0.005869   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     0.513389  0.509192  0.506994  
Std.Err.  0.008932  0.010350  0.014708   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise8</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.467514  0.531026  0.500346  0.504115  0.500933  0.495891   
Std.Err.  0.019916  0.023795  0.006282  0.022729  0.007797  0.005931   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     0.491334  0.504812  0.519548  
Std.Err.  0.008564  0.010536  0.014868   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise9</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.468098  0.526925  0.495539  0.529444  0.497031  0.505148   
Std.Err.  0.021758  0.025539  0.006286  0.022109  0.007718  0.005907   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     0.494763  0.507972  0.511789  
Std.Err.  0.008594  0.010442  0.014352   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise10</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.478180  0.500685  0.500060  0.472976  0.500191  0.511017   
Std.Err.  0.020324  0.026903  0.006198  0.020793  0.007517  0.005885   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     0.500824  0.506995  0.499019  
Std.Err.  0.008697  0.010502  0.015238   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise11</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.489795  0.488485  0.492516  0.509673  0.515484  0.508793   
Std.Err.  0.020479  0.026809  0.006266  0.022111  0.007814  0.005892   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     0.502809  0.505733  0.491129  
Std.Err.  0.008781  0.010552  0.014998   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise12</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.496124  0.547064  0.500455  0.471864  0.500168  0.501219   
Std.Err.  0.019757  0.025711  0.006398  0.020903  0.007672  0.005918   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     0.500728  0.521672  0.494793  
Std.Err.  0.008546  0.010593  0.014908   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise13</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.494888  0.535293  0.504892  0.479535  0.498929  0.497969   
Std.Err.  0.020970  0.025177  0.006226  0.022273  0.007703  0.005880   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     0.484956  0.488564  0.525039  
Std.Err.  0.008924  0.010471  0.014502   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise14</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.502771  0.511858  0.492756  0.485773  0.484064  0.506386   
Std.Err.  0.020405  0.026618  0.006238  0.023378  0.007688  0.005918   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     0.491079  0.506982  0.510278  
Std.Err.  0.008691  0.010477  0.015145   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise15</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.472543  0.528618  0.507714  0.515404  0.493721  0.504659   
Std.Err.  0.020178  0.027276  0.006296  0.022529  0.007811  0.005896   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     0.502755  0.505887  0.533510  
Std.Err.  0.008781  0.010675  0.014998   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise16</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.507393  0.484206  0.508391  0.476947  0.508548  0.496021   
Std.Err.  0.020754  0.023525  0.006304  0.024211  0.007776  0.005903   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     0.515021  0.506695  0.484385  
Std.Err.  0.008648  0.010293  0.015362   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise17</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.489426  0.471092  0.506304  0.494265  0.503826  0.502336   
Std.Err.  0.019743  0.026462  0.006370  0.021789  0.007648  0.005893   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     0.491715  0.520289  0.502923  
Std.Err.  0.008787  0.010738  0.014807   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise18</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.517420  0.501618  0.506690  0.514725  0.500758  0.501080   
Std.Err.  0.020522  0.026708  0.006223  0.022547  0.007592  0.005993   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     0.500188  0.506352  0.487323  
Std.Err.  0.008384  0.010230  0.015278   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise19</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.467595  0.508217  0.504449  0.509105  0.511728  0.498591   
Std.Err.  0.021083  0.026840  0.006173  0.022455  0.007697  0.005811   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     0.500900  0.514904  0.492103  
Std.Err.  0.008506  0.010683  0.014820   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise20</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.490588  0.465721  0.508434  0.535436  0.498242  0.507797   
Std.Err.  0.020553  0.024379  0.006266  0.022370  0.007577  0.005956   

           leaf[7]   leaf[8]   leaf[9]  
Coef.     0.503339  0.488032  0.496959  
Std.Err.  0.008697  0.010667  0.015243   
</pre></div>
</div>
</div>
</div>
<p>Finally, as we did in the linear model case, we can use the same code for an annotated version of the same information. Again, we ordered the rows in decreasing order based on an estimate of the relative variance “explained” by leaf membership: <span class="math notranslate nohighlight">\(Var(E[X_i|L_i]) / Var(X_i)\)</span>, where <span class="math notranslate nohighlight">\(L_i\)</span> represents the leaf.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="k">for</span> <span class="n">var_name</span> <span class="ow">in</span> <span class="n">covariates</span><span class="p">:</span>
    <span class="n">form2</span> <span class="o">=</span> <span class="n">var_name</span> <span class="o">+</span> <span class="s2">&quot; ~ &quot;</span> <span class="o">+</span> <span class="s2">&quot;0&quot;</span> <span class="o">+</span> <span class="s2">&quot;+&quot;</span> <span class="o">+</span> <span class="s2">&quot;leaf&quot;</span>
    <span class="n">ols</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="n">form2</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">cov_type</span> <span class="o">=</span> <span class="s1">&#39;HC2&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">summary2</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>
    
    <span class="c1"># Retrieve results</span>
    <span class="n">toget_index</span> <span class="o">=</span> <span class="n">ols</span><span class="p">[</span><span class="s2">&quot;Coef.&quot;</span><span class="p">]</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">toget_index</span><span class="o">.</span><span class="n">index</span>
    <span class="n">cova1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">var_name</span><span class="p">,</span><span class="n">num_leaves</span><span class="p">),</span> <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;covariate&quot;</span><span class="p">)</span>
    <span class="n">avg</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">ols</span><span class="p">[</span><span class="s2">&quot;Coef.&quot;</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;avg&quot;</span><span class="p">)</span>
    <span class="n">stderr</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">ols</span><span class="p">[</span><span class="s2">&quot;Std.Err.&quot;</span><span class="p">],</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;stderr&quot;</span><span class="p">)</span>
    <span class="n">ranking</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">num_leaves</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;ranking&quot;</span><span class="p">)</span>
    <span class="n">scaling</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">((</span><span class="n">avg</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">avg</span><span class="p">))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">avg</span><span class="p">)),</span> <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;scaling&quot;</span><span class="p">)</span>
    <span class="n">data2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span> <span class="n">covariates</span><span class="p">)</span>
    <span class="n">variation1</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">avg</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">data2</span><span class="p">[</span><span class="n">var_name</span><span class="p">])</span>
    <span class="n">variation</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">variation1</span><span class="p">,</span> <span class="n">num_leaves</span><span class="p">),</span> <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;variation&quot;</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">avg</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;str&#39;</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;(&quot;</span> <span class="o">+</span> <span class="nb">round</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;str&#39;</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;)&quot;</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;labels&quot;</span><span class="p">)</span>
    
    <span class="c1"># Tally up results</span>
    <span class="n">df1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">cova1</span><span class="p">,</span> <span class="n">avg</span><span class="p">,</span> <span class="n">stderr</span><span class="p">,</span> <span class="n">ranking</span><span class="p">,</span> <span class="n">scaling</span><span class="p">,</span> <span class="n">variation</span><span class="p">,</span> <span class="n">labels</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">df1</span><span class="p">)</span>

<span class="c1"># a small optional trick to ensure heatmap will be in decreasing order of &#39;variation&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;variation&quot;</span><span class="p">,</span> <span class="s2">&quot;covariate&quot;</span><span class="p">],</span> <span class="n">ascending</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">:(</span><span class="mi">8</span><span class="o">*</span><span class="n">num_leaves</span><span class="p">),</span> <span class="p">:]</span>
<span class="n">df1</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span><span class="n">index</span> <span class="o">=</span> <span class="s2">&quot;covariate&quot;</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="s2">&quot;ranking&quot;</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;scaling&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span>  <span class="n">df</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span><span class="n">index</span> <span class="o">=</span> <span class="s2">&quot;covariate&quot;</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="s2">&quot;ranking&quot;</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="c1"># plot heatmap</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">df1</span><span class="p">,</span> 
                 <span class="n">annot</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
                 <span class="n">annot_kws</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span> <span class="s1">&#39;color&#39;</span><span class="p">:</span><span class="s2">&quot;k&quot;</span><span class="p">},</span>
                 <span class="n">fmt</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span>
                 <span class="n">cmap</span> <span class="o">=</span> <span class="s2">&quot;YlGnBu&quot;</span><span class="p">,</span>
                 <span class="n">linewidths</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">xticklabels</span> <span class="o">=</span> <span class="n">ranking</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">labelrotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">labelrotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Leaf (ordered by prediction, low to high)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Average covariate values within leaf&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span> <span class="n">fontweight</span> <span class="o">=</span> <span class="s2">&quot;bold&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Average covariate values within leaf&#39;)
</pre></div>
</div>
<img alt="../_images/2_introduction_to_machine_learning_92_1.png" src="../_images/2_introduction_to_machine_learning_92_1.png" />
</div>
</div>
</section>
<section id="forest">
<h3><span class="section-number">2.2.3. </span>Forest<a class="headerlink" href="#forest" title="Permalink to this headline">#</a></h3>
<p>Forests are a type of <strong>ensemble</strong> estimators: they aggregate information about many decision trees to compute a new estimate that typically has much smaller variance.</p>
<p>At a high level, the process of fitting a (regression) forest consists of fitting many decision trees, each on a different subsample of the data. The forest prediction for a particular point <span class="math notranslate nohighlight">\(x\)</span> is the average of all tree predictions for that point.</p>
<p>One interesting aspect of forests and many other ensemble methods is that cross-validation can be built into the algorithm itself. Since each tree only uses a subset of the data, the remaining subset is effectively a test set for that tree. We call these observations <strong>out-of-bag</strong> (there were not in the “bag” of training observations). They can be used to evaluate the performance of that tree, and the average of out-of-bag evaluations is evidence of the performance of the forest itself.</p>
<p>For the example below, we’ll use the regression_forest function of the <code class="docutils literal notranslate"><span class="pre">R</span></code> package <code class="docutils literal notranslate"><span class="pre">grf</span></code>. The particular forest implementation in <code class="docutils literal notranslate"><span class="pre">grf</span></code> has interesting properties that are absent from most other packages. For example, trees are build using a certain sample-splitting scheme that ensures that predictions are approximately unbiased and normally distributed for large samples, which in turn allows us to compute valid confidence intervals around those predictions. We’ll have more to say about the importance of these features when we talk about causal estimates in future chapters. See also the grf website for more information.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">permutation_importance</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">forest</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">oob_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1">#x_train, x_test, y_train, y_test = train_test_split(XX.to_numpy() , Y, test_size=.3)</span>
<span class="n">forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c1"># Retrieving forest predictions</span>
<span class="n">rf_pred</span> <span class="o">=</span> <span class="n">forest</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>

<span class="c1"># Evaluation</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">rf_pred</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Forest MSE:&quot;</span><span class="p">,</span> <span class="n">mse</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Forest MSE: 0.5873930432041589
</pre></div>
</div>
</div>
</div>
<p>The fitted attribute <code class="docutils literal notranslate"><span class="pre">feature_importances_</span></code> computes the decrease in node impurity weighted by the probability of reaching that node. The node probability can be calculated by the number of samples that reach the node, divided by the total number of samples. The higher the value the more important the feature.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">feature_importance</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">forest</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">covariates</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span> <span class="p">[</span><span class="s2">&quot;importance&quot;</span><span class="p">])</span>
<span class="n">importance</span> <span class="o">=</span> <span class="n">feature_importance</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;importance&quot;</span><span class="p">],</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">importance</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>UNITSF</th>
      <th>NUNIT2</th>
      <th>BATHS</th>
      <th>MOBILTYP</th>
      <th>LOT</th>
      <th>noise2</th>
      <th>noise10</th>
      <th>noise18</th>
      <th>noise14</th>
      <th>noise17</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>importance</th>
      <td>0.147069</td>
      <td>0.112232</td>
      <td>0.06445</td>
      <td>0.051078</td>
      <td>0.033354</td>
      <td>0.029163</td>
      <td>0.027714</td>
      <td>0.026012</td>
      <td>0.025877</td>
      <td>0.024181</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">importance</span><span class="o">.</span><span class="n">index</span><span class="p">[:</span><span class="mi">10</span><span class="p">],</span><span class="n">importance</span><span class="o">.</span><span class="n">importance</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span> <span class="n">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span> <span class="mi">90</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Importance&quot;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Variable Importance&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Variable Importance&#39;)
</pre></div>
</div>
<img alt="../_images/2_introduction_to_machine_learning_99_1.png" src="../_images/2_introduction_to_machine_learning_99_1.png" />
</div>
</div>
<p>All the caveats about interpretation that we mentioned above apply in a similar to forest output.</p>
</section>
</section>
<section id="further-reading">
<h2><span class="section-number">2.3. </span>Further reading<a class="headerlink" href="#further-reading" title="Permalink to this headline">#</a></h2>
<p>In this tutorial we briefly reviewed some key concepts that we recur later in this tutorial. For readers who are entirely new to this field or interested in learning about it more depth, the first few chapters of the following textbook are an acccessible introduction:</p>
<p>James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). An introduction to statistical learning (Vol. 112, p. 18). New York: springer. Available for free at <a class="reference external" href="https://www.statlearning.com/">the authors’ website</a>.</p>
<p>Some of the discussion in the Lasso section in particular was drawn from <a class="reference external" href="https://www.aeaweb.org/articles?id=10.1257/jep.31.2.87">Mullainathan and Spiess (JEP, 2017)</a>, which contains a good discussion of the interpretability issues discussed here.</p>
<p>There has been a good deal of research on inference in high-dimensional models, Although we won’t be covering in depth it in this tutorial, we refer readers to <a class="reference external" href="http://www.mit.edu/~vchern/papers/JEP.pdf">Belloni, Chernozhukov and Hansen (JEP, 2014)</a>. Also check out the related <code class="docutils literal notranslate"><span class="pre">R</span></code> package <a class="reference external" href="https://cran.r-project.org/web/packages/hdm/hdm.pdf"><code class="docutils literal notranslate"><span class="pre">hdm</span></code></a>, developed by the same authors, along with Philipp Bach and Martin Spindler.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "d2cml-ai/mgtecon634_python",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="1_introduction_1.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">1. </span>Introduction</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="3_average_treatment_effect_1.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">3. </span>ATE I: Binary treatment</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Phd Susan Athey<br/>
  
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>