
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>2. Introduction to Machine Learning &#8212; MGTECON 634 at Stanford (R scripts)</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3. ATE I: Binary treatment" href="3_average_treatment_effect_1.html" />
    <link rel="prev" title="1. Introduction" href="1_introduction_1.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">MGTECON 634 at Stanford (R scripts)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../md/intro.html">
   Machine Learning-Based Causal Inference
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Topics
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1_introduction_1.html">
   1. Introduction
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   2. Introduction to Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3_average_treatment_effect_1.html">
   3. ATE I: Binary treatment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4_heterogeneous_treatment_effect_1.html">
   4. HTE I: Binary treatment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5_policy_evaluation_1.html">
   5. Policy Evaluation I - Binary Treatment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="6_Policy_Learning_1.html">
   6. Policy Learning I - Binary Treatment
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/notebooks/2_introduction_to_Machine_Learning.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/d2cml-ai/mgtecon634_python"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/d2cml-ai/mgtecon634_python/issues/new?title=Issue%20on%20page%20%2Fnotebooks/2_introduction_to_Machine_Learning.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/d2cml-ai/mgtecon634_python/edit/master/_build/jupyter_execute/notebooks/2_introduction_to_Machine_Learning.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/d2cml-ai/mgtecon634_python/master?urlpath=tree/_build/jupyter_execute/notebooks/2_introduction_to_Machine_Learning.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/d2cml-ai/mgtecon634_python/blob/master/_build/jupyter_execute/notebooks/2_introduction_to_Machine_Learning.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#key-concepts">
   2.1. Key concepts
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#common-machine-learning-algorithms">
   2.2. Common machine learning algorithms
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generalized-linear-models">
     2.2.1. Generalized linear models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decision-tree">
     2.2.2. Decision Tree
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#forest">
     2.2.3. Forest
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-reading">
   2.3. Further reading
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Introduction to Machine Learning</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#key-concepts">
   2.1. Key concepts
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#common-machine-learning-algorithms">
   2.2. Common machine learning algorithms
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generalized-linear-models">
     2.2.1. Generalized linear models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decision-tree">
     2.2.2. Decision Tree
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#forest">
     2.2.3. Forest
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-reading">
   2.3. Further reading
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="introduction-to-machine-learning">
<h1><span class="section-number">2. </span>Introduction to Machine Learning<a class="headerlink" href="#introduction-to-machine-learning" title="Permalink to this headline">¶</a></h1>
<p>In this chapter, we’ll briefly review machine learning concepts that will be relevant later. We’ll focus in particular on the problem of <strong>prediction</strong>, that is, to model some output variable as a function of observed input covariates.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># loading relevant packages</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">import</span> <span class="n">figure</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">make_scorer</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">SyncRNG</span> <span class="kn">import</span> <span class="n">SyncRNG</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</div>
<p>In this section, we will use simulated data. In the next section we’ll load a real dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Simulating data</span>

<span class="c1"># Sample size</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">500</span>

<span class="c1"># Generating covariate X ~ Unif[-4, 4]</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="c1">#with linspace we can generate a vector of &quot;n&quot; numbers between a range of numbers</span>

<span class="c1"># Generate outcome</span>
<span class="c1"># if x &lt; 0:</span>
<span class="c1">#   y = cos(2*x) + N(0, 1)</span>
<span class="c1"># else:</span>
<span class="c1">#   y = 1-sin(x) + N(0, 1)</span>
<span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> 
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">),</span> <span class="mi">1</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">+</span> <span class="mi">1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># collecting observations in a data.frame object</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>The following shows how the two variables <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> relate. Note that the relationship is nonlinear.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Data&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">mu</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;Ground truth E[Y|X=x]&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;X&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Outcome y&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Outcome y&#39;)
</pre></div>
</div>
<img alt="../_images/2_introduction_to_Machine_Learning_6_1.png" src="../_images/2_introduction_to_Machine_Learning_6_1.png" />
</div>
</div>
<p>Note: If you’d like to run the code below on a different dataset, you can replace the dataset above with another <code class="docutils literal notranslate"><span class="pre">data.frame</span></code> of your choice, and redefine the key variable identifiers (<code class="docutils literal notranslate"><span class="pre">outcome</span></code>, <code class="docutils literal notranslate"><span class="pre">covariates</span></code>) accordingly. Although we try to make the code as general as possible, you may also need to make a few minor changes to the code below; read the comments carefully.</p>
<div class="section" id="key-concepts">
<h2><span class="section-number">2.1. </span>Key concepts<a class="headerlink" href="#key-concepts" title="Permalink to this headline">¶</a></h2>
<p>The prediction problem is to accurately guess the value of some output variable <span class="math notranslate nohighlight">\(Y_i\)</span> from input variables <span class="math notranslate nohighlight">\(X_i\)</span>. For example, we might want to predict “house prices given house characteristics such as the number of rooms, age of the building, and so on. The relationship between input and output is modeled in very general terms by some function</p>
<div class="math notranslate nohighlight" id="equation-true-model">
<span class="eqno">(2.1)<a class="headerlink" href="#equation-true-model" title="Permalink to this equation">¶</a></span>\[
  Y_i = f(X_i) + \epsilon_i
\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon_i\)</span> represents all that is not captured by information obtained from <span class="math notranslate nohighlight">\(X_i\)</span> via the mapping <span class="math notranslate nohighlight">\(f\)</span>. We say that error <span class="math notranslate nohighlight">\(\epsilon_i\)</span> is irreducible.</p>
<p>We highlight that <a class="reference internal" href="#equation-true-model">(2.1)</a> is <strong>not modeling a causal relationship</strong> between inputs and outputs. For an extreme example, consider taking <span class="math notranslate nohighlight">\(Y_i\)</span> to be “distance from the equator” and <span class="math notranslate nohighlight">\(X_i\)</span> to be “average temperature.” We can still think of the problem of guessing (“predicting”) “distance from the equator” given some information about “average temperature,” even though one would expect the former to cause the latter.</p>
<p>In general, we can’t know the “ground truth”  <span class="math notranslate nohighlight">\(f\)</span>, so we will approximate it from data. Given <span class="math notranslate nohighlight">\(n\)</span> data points <span class="math notranslate nohighlight">\(\{(X_1, Y_1), \cdots, (X_n, Y_n)\}\)</span>, our goal is to obtain an estimated model  <span class="math notranslate nohighlight">\(\hat{f}\)</span> such that our predictions <span class="math notranslate nohighlight">\(\widehat{Y}_i := \hat{f}(X_i)\)</span> are “close” to the true outcome values <span class="math notranslate nohighlight">\(Y_i\)</span> given some criterion. To formalize this, we’ll follow these three steps:</p>
<ul class="simple">
<li><p><strong>Modeling:</strong> Decide on some suitable class of functions that our estimated model may belong to. In machine learning applications the class of functions can be very large and complex (e.g., deep decision trees, forests, high-dimensional linear models, etc). Also, we must decide on a loss function that serves as our criterion to evaluate the quality of our predictions (e.g., mean-squared error).</p></li>
<li><p><strong>Fitting:</strong> Find the estimate <span class="math notranslate nohighlight">\(\hat{f}\)</span> that optimizes the loss function chosen in the previous step (e.g., the tree that minimizes the squared deviation between <span class="math notranslate nohighlight">\(\hat{f}(X_i)\)</span> and <span class="math notranslate nohighlight">\(Y_i\)</span> in our data).</p></li>
<li><p><strong>Evaluation:</strong> Evaluate our fitted model <span class="math notranslate nohighlight">\(\hat{f}\)</span>. That is, if we were given a new, yet unseen, input and output pair <span class="math notranslate nohighlight">\((X',Y')\)</span>, we’d like to know if <span class="math notranslate nohighlight">\(Y' \approx \hat{f}(X_i)\)</span> by some metric.</p></li>
</ul>
<p>For concreteness, let’s work through an example. Let’s say that, given the data simulated above, we’d like to predict <span class="math notranslate nohighlight">\(Y_i\)</span> from the first covariate  <span class="math notranslate nohighlight">\(X_{i1}\)</span> only. Also, let’s say that our model class will be polynomials of degree <span class="math notranslate nohighlight">\(q\)</span> in <span class="math notranslate nohighlight">\(X_{i1}\)</span>, and we’ll evaluate fit based on mean squared error. That is, <span class="math notranslate nohighlight">\(\hat{f}(X_{i1}) = \hat{b}_0 + X_{i1}\hat{b}_1 + \cdots + X_{i1}^q \hat{b}_q\)</span>, where the coefficients are obtained by solving the following problem:</p>
<div class="math notranslate nohighlight">
\[
  \hat{b} = \arg\min_b \sum_{i=1}^m
    \left(Y_i - b_0 - X_{i1}b_1 - \cdots - X_{iq}^q b_q \right)^2
\]</div>
<p>An important question is what is <span class="math notranslate nohighlight">\(q\)</span>, the degree of the polynomial. It controls the complexity of the model. One may imagine that more complex models are better, but that is not always true, because a very flexible model may try to simply interpolate over the data at hand, but fail to generalize well for new data points. We call this <strong>overfitting</strong>. The main feature of overfitting is <strong>high variance</strong>, in the sense that, if we were given a different data set of the same size, we’d likely get a very different model.</p>
<p>To illustrate, in the figure below we let the degree be  <span class="math notranslate nohighlight">\(q=10\)</span> but use only the first few data points. The fitted model is shown in green, and the original data points are in red.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="s1">&#39;x&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Note: this code assumes that the first covariate is continuous.</span>
<span class="c1"># Fitting a flexible model on very little data</span>

<span class="c1"># selecting only a few data points</span>
<span class="n">subset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">30</span><span class="p">)</span>

<span class="c1"># formula for a high-dimensional polynomial regression</span>
<span class="c1"># y ~ 1 + x1 + x1^2 + x1^3 + .... + x1^q</span>
<span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">X_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># linear regression using only a few observations</span>
<span class="n">poly</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_poly</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">lin2</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lin2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_poly</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">30</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">30</span><span class="p">])</span>

<span class="c1"># compute a grid of x1 values we&#39;ll use for prediction</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span>
<span class="n">xgrid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">new_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">xgrid</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>

<span class="c1"># predict</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">lin2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">new_data</span><span class="p">))</span>

<span class="c1"># Visualising the Polynomial Regression results</span>
<span class="c1"># Plotting observations (in red) and model predictions (in green</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">subset</span><span class="p">,</span><span class="s1">&#39;x&#39;</span><span class="p">],</span><span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">subset</span><span class="p">,</span><span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xgrid</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Estimate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Example of overfitting&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Outcome y&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Outcome y&#39;)
</pre></div>
</div>
<img alt="../_images/2_introduction_to_Machine_Learning_10_1.png" src="../_images/2_introduction_to_Machine_Learning_10_1.png" />
</div>
</div>
<p>On the other hand, when <span class="math notranslate nohighlight">\(q\)</span> is too small relative to our data, we permit only very simple models and may suffer from misspecification bias. We call this <strong>underfitting</strong>. The main feature of underfitting is <strong>high bias</strong> – the selected model just isn’t complex enough to accurately capture the relationship between input and output variables.</p>
<p>To illustrate underfitting, in the figure below we set <span class="math notranslate nohighlight">\(q=1\)</span> (a linear fit).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Note: this code assumes that the first covariate is continuous</span>
<span class="c1"># Fitting a very simply model on very little data</span>

<span class="c1"># formula for a linear regression (without taking polynomials of x1)</span>
<span class="c1"># y ~ 1 + x1</span>
<span class="n">lin</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lin</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">30</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">30</span><span class="p">])</span>

<span class="c1"># compute a grid of x1 values we&#39;ll use for prediction</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span>
<span class="n">xgrid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">new_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">xgrid</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>

<span class="c1"># predict</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">lin</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">new_data</span><span class="p">)</span>

<span class="c1"># plotting observations (in red) and model predictions (in green)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">subset</span><span class="p">,</span><span class="s1">&#39;x&#39;</span><span class="p">],</span><span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">subset</span><span class="p">,</span><span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xgrid</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Estimate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Example of underfitting&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Outcome y&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Outcome y&#39;)
</pre></div>
</div>
<img alt="../_images/2_introduction_to_Machine_Learning_12_1.png" src="../_images/2_introduction_to_Machine_Learning_12_1.png" />
</div>
</div>
<p>This tension is called the <strong>bias-variance trade-off</strong>: simpler models underfit and have more bias, more complex models overfit and have more variance.</p>
<p>One data-driven way of deciding an appropriate level of complexity is to divide the available data into a training set (where the model is fit) and the validation set (where the model is evaluated). The next snippet of code uses the first half of the data to fit a polynomial of order <span class="math notranslate nohighlight">\(q\)</span>, and then evaluates that polynomial on the second half. The training MSE estimate decreases monotonically with the polynomial degree, because the model is better able to fit on the training data; the test MSE estimate starts increasing after a while reflecting that the model no longer generalizes well.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># polynomial degrees that we&#39;ll loop over</span>
<span class="n">degrees</span> <span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">21</span><span class="p">)</span>

<span class="c1"># training data observations: 1 to (n/2)</span>
<span class="n">train_mse</span> <span class="o">=</span><span class="p">[]</span>
<span class="n">test_mse</span> <span class="o">=</span><span class="p">[]</span>

<span class="c1"># looping over each polynomial degree</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">degrees</span><span class="p">:</span>
    
    <span class="c1"># formula y ~ 1 + x1 + x1^2 + ... + x1^q</span>
    <span class="c1"># linear regression using the formula above</span>
    <span class="c1"># note we&#39;re fitting only on the training data observations</span>
    <span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span> <span class="o">=</span> <span class="n">d</span><span class="p">,</span> <span class="n">include_bias</span> <span class="o">=</span><span class="kc">False</span>  <span class="p">)</span>
    <span class="n">poly_features</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    
    <span class="c1"># predicting on the training subset</span>
    <span class="c1"># (no need to pass a dataframe)</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">poly_features</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.5</span> <span class="p">,</span> <span class="n">random_state</span><span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Now since we want the valid and test size to be equal (10% each of overall data). </span>
    <span class="c1"># we have to define valid_size=0.5 (that is 50% of remaining data)</span>
    <span class="n">poly_reg_model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
    <span class="n">poly_reg_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    
    <span class="c1"># predicting on the validation subset</span>
    <span class="c1"># (the minus sign in &quot;-train&quot; excludes observations in the training data)</span>
    <span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">poly_reg_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">poly_reg_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    
    <span class="c1"># compute the mse estimate on the validation subset and output it</span>
    <span class="n">mse_train</span><span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">)</span>
    <span class="n">mse_test</span><span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">)</span>
    
    <span class="n">train_mse</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mse_train</span><span class="p">)</span>
    <span class="n">test_mse</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mse_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">degrees</span><span class="p">,</span> <span class="n">train_mse</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;Training&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">degrees</span><span class="p">,</span> <span class="n">test_mse</span><span class="p">,</span><span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;Validation&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;MSE Estimates (train test split)&quot;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span> <span class="o">=</span> <span class="s2">&quot;Polynomial degree&quot;</span><span class="p">,</span> <span class="n">ylabel</span> <span class="o">=</span> <span class="s2">&quot;MSE estimate&quot;</span><span class="p">)</span>
    
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">&quot;Low bias </span><span class="se">\n</span><span class="s2"> High Variance&quot;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mf">1.23</span><span class="p">),</span> <span class="n">xycoords</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mf">1.23</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">,</span>
            <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">&quot;-&gt;&quot;</span><span class="p">,</span><span class="n">connectionstyle</span><span class="o">=</span><span class="s2">&quot;arc3&quot;</span><span class="p">),)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">&quot;High bias </span><span class="se">\n</span><span class="s2"> Low Variance&quot;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">5.3</span><span class="p">,</span> <span class="mf">1.30</span><span class="p">),</span> <span class="n">xycoords</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mf">1.30</span><span class="p">),</span> <span class="n">textcoords</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">,</span>
            <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s2">&quot;-&gt;&quot;</span><span class="p">,</span><span class="n">connectionstyle</span><span class="o">=</span><span class="s2">&quot;arc3&quot;</span><span class="p">),)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(7, 1.3, &#39;High bias \n Low Variance&#39;)
</pre></div>
</div>
<img alt="../_images/2_introduction_to_Machine_Learning_15_1.png" src="../_images/2_introduction_to_Machine_Learning_15_1.png" />
</div>
</div>
<p>To make better use of the data we will often divide the data into <span class="math notranslate nohighlight">\(K\)</span> subsets, or <em>folds</em>. Then one fits <span class="math notranslate nohighlight">\(K\)</span> models, each using  <span class="math notranslate nohighlight">\(K-1\)</span> folds and then evaluation the fitted model on the remaining fold. This is called <strong>k-fold cross-validation</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#cv = KFold(n_splits=10, random_state=1, shuffle=True)</span>
<span class="n">scorer</span> <span class="o">=</span> <span class="n">make_scorer</span>
<span class="n">mse</span> <span class="o">=</span><span class="p">[]</span>

<span class="c1"># looping over polynomial degrees (q)</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">degrees</span><span class="p">:</span> 
    
    <span class="c1"># formula y ~ 1 + x1 + x1^2 + ... + x1^q</span>
    <span class="c1"># polynomial degrees that we&#39;ll loop over to select</span>
    <span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span> <span class="o">=</span> <span class="n">d</span><span class="p">,</span> <span class="n">include_bias</span> <span class="o">=</span><span class="kc">False</span>  <span class="p">)</span>
    <span class="n">poly_features</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    
    <span class="c1"># fit on K-1 folds, leaving out observations in fold.idx</span>
    <span class="c1"># (the minus sign in -fold.idx excludes those observations)</span>
    <span class="n">ols</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
    
    <span class="c1"># cross-validated mse estimate</span>
    <span class="n">scorer</span> <span class="o">=</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">)</span>
    <span class="n">mse_test</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">ols</span><span class="p">,</span> <span class="n">poly_features</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scorer</span><span class="p">,</span> <span class="n">cv</span> <span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">mse</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mse_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">degrees</span><span class="p">,</span> <span class="n">mse</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Polynomial degree&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">21</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;MSE estimate&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;MSE estimate (K-fold cross validation)&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="c1">#different to r, the models in python got a better performance with more training cause by the</span>
<span class="c1">#cross validation and the kfold</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;MSE estimate (K-fold cross validation)&#39;)
</pre></div>
</div>
<img alt="../_images/2_introduction_to_Machine_Learning_18_1.png" src="../_images/2_introduction_to_Machine_Learning_18_1.png" />
</div>
</div>
<p>A final remark is that, in machine learning applications, the complexity of the model often is allowed to increase with the available data. In the example above, even though we weren’t very successful when fitting a high-dimensional model on very little data, if we had much more data perhaps such a model would be appropriate. The next figure again fits a high order polynomial model, but this time on many data points. Note how, at least in data-rich regions, the model is much better behaved, and tracks the average outcome reasonably well without trying to interpolate wildly of the data points.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Note this code assumes that the first covariate is continuous</span>
<span class="c1"># Fitting a flexible model on a lot of data</span>

<span class="c1"># now using much more data</span>
<span class="n">subset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">500</span><span class="p">)</span>


<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="s1">&#39;x&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># formula for high order polynomial regression</span>
<span class="c1"># y ~ 1 + x1 + x1^2 + ... + x1^q</span>
<span class="n">poly</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span> <span class="o">=</span> <span class="mi">15</span><span class="p">)</span>

<span class="c1"># linear regression</span>
<span class="n">X_poly</span> <span class="o">=</span> <span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">poly</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_poly</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">lin2</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lin2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_poly</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">500</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">500</span><span class="p">])</span>

<span class="c1"># compute a grid of x1 values we&#39;ll use for prediction</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span>
<span class="n">xgrid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">new_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">xgrid</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>

<span class="c1"># predict</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">lin2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">poly</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">new_data</span><span class="p">))</span>

<span class="c1"># Visualising the Polynomial Regression results</span>
<span class="c1"># plotting observations (in red) and model predictions (in green)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">subset</span><span class="p">,</span><span class="s1">&#39;x&#39;</span><span class="p">],</span><span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">subset</span><span class="p">,</span><span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xgrid</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Estimate&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">mu</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;Ground truth&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Outcome&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Outcome&#39;)
</pre></div>
</div>
<img alt="../_images/2_introduction_to_Machine_Learning_20_1.png" src="../_images/2_introduction_to_Machine_Learning_20_1.png" />
</div>
</div>
<p>This is one of the benefits of using machine learning-based models: more data implies more flexible modeling, and therefore potentially better predictive power – provided that we carefully avoid overfitting.</p>
<p>The example above based on polynomial regression was used mostly for illustration. In practice, there are often better-performing algorithms. We’ll see some of them next.</p>
</div>
<div class="section" id="common-machine-learning-algorithms">
<h2><span class="section-number">2.2. </span>Common machine learning algorithms<a class="headerlink" href="#common-machine-learning-algorithms" title="Permalink to this headline">¶</a></h2>
<p>Next, we’ll introduce three machine learning algorithms: (regularized) linear models, trees, and forests. Although this isn’t an exhaustive list, these algorithms are common enough that every machine learning practitioner should know about them. They also have convenient <code class="docutils literal notranslate"><span class="pre">R</span></code> packages that allow for easy coding.</p>
<p>In this tutorial, we’ll focus heavily on how to <strong>interpret</strong> the output of machine learning models – or, at least, how not to <em>mis</em>-interpret it. However, in this chapter we won’t be making any causal claims about the relationships between variables yet. But please hang tight, as estimating causal effects will be one of the main topics presented in the next chapters.</p>
<p>For the remainder of the chapter we will use a real dataset. Each row in this data set represents the characteristics of a owner-occupied housing unit. Our goal is to predict the (log) price of the housing unit (<code class="docutils literal notranslate"><span class="pre">LOGVALUE</span></code>, our outcome variable) from features such as the size of the lot (<code class="docutils literal notranslate"><span class="pre">LOT</span></code>) and square feet area (<code class="docutils literal notranslate"><span class="pre">UNITSF</span></code>), number of bedrooms (<code class="docutils literal notranslate"><span class="pre">BEDRMS</span></code>) and bathrooms (<code class="docutils literal notranslate"><span class="pre">BATHS</span></code>), year in which it was built (<code class="docutils literal notranslate"><span class="pre">BUILT</span></code>) etc. This dataset comes from the American Housing Survey and was used in <a class="reference external" href="https://www.aeaweb.org/articles?id=10.1257/jep.31.2.87">Mullainathan and Spiess (2017, JEP)</a>. In addition, we will append to this data columns that are pure noise. Ideally, our fitted model should not take them into acccount.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">io</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load dataset</span>
<span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;https://docs.google.com/uc?id=1qHr-6nN7pCbU8JUtbRDtMzUKqS9ZlZcR&amp;export=download&#39;</span>
<span class="n">urlData</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span><span class="o">.</span><span class="n">content</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">io</span><span class="o">.</span><span class="n">StringIO</span><span class="p">(</span><span class="n">urlData</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)))</span>
<span class="n">data</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;Unnamed: 0&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># outcome variable name</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="s1">&#39;LOGVALUE&#39;</span>

<span class="c1"># covariates</span>
<span class="n">true_covariates</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;LOT&#39;</span><span class="p">,</span><span class="s1">&#39;UNITSF&#39;</span><span class="p">,</span><span class="s1">&#39;BUILT&#39;</span><span class="p">,</span><span class="s1">&#39;BATHS&#39;</span><span class="p">,</span><span class="s1">&#39;BEDRMS&#39;</span><span class="p">,</span><span class="s1">&#39;DINING&#39;</span><span class="p">,</span><span class="s1">&#39;METRO&#39;</span><span class="p">,</span><span class="s1">&#39;CRACKS&#39;</span><span class="p">,</span><span class="s1">&#39;REGION&#39;</span><span class="p">,</span><span class="s1">&#39;METRO3&#39;</span><span class="p">,</span><span class="s1">&#39;PHONE&#39;</span><span class="p">,</span><span class="s1">&#39;KITCHEN&#39;</span><span class="p">,</span><span class="s1">&#39;MOBILTYP&#39;</span><span class="p">,</span><span class="s1">&#39;WINTEROVEN&#39;</span><span class="p">,</span><span class="s1">&#39;WINTERKESP&#39;</span><span class="p">,</span><span class="s1">&#39;WINTERELSP&#39;</span><span class="p">,</span><span class="s1">&#39;WINTERWOOD&#39;</span><span class="p">,</span><span class="s1">&#39;WINTERNONE&#39;</span><span class="p">,</span><span class="s1">&#39;NEWC&#39;</span><span class="p">,</span><span class="s1">&#39;DISH&#39;</span><span class="p">,</span><span class="s1">&#39;WASH&#39;</span><span class="p">,</span><span class="s1">&#39;DRY&#39;</span><span class="p">,</span><span class="s1">&#39;NUNIT2&#39;</span><span class="p">,</span><span class="s1">&#39;BURNER&#39;</span><span class="p">,</span><span class="s1">&#39;COOK&#39;</span><span class="p">,</span><span class="s1">&#39;OVEN&#39;</span><span class="p">,</span><span class="s1">&#39;REFR&#39;</span><span class="p">,</span><span class="s1">&#39;DENS&#39;</span><span class="p">,</span><span class="s1">&#39;FAMRM&#39;</span><span class="p">,</span><span class="s1">&#39;HALFB&#39;</span><span class="p">,</span><span class="s1">&#39;KITCH&#39;</span><span class="p">,</span><span class="s1">&#39;LIVING&#39;</span><span class="p">,</span><span class="s1">&#39;OTHFN&#39;</span><span class="p">,</span><span class="s1">&#39;RECRM&#39;</span><span class="p">,</span><span class="s1">&#39;CLIMB&#39;</span><span class="p">,</span><span class="s1">&#39;ELEV&#39;</span><span class="p">,</span><span class="s1">&#39;DIRAC&#39;</span><span class="p">,</span><span class="s1">&#39;PORCH&#39;</span><span class="p">,</span><span class="s1">&#39;AIRSYS&#39;</span><span class="p">,</span><span class="s1">&#39;WELL&#39;</span><span class="p">,</span><span class="s1">&#39;WELDUS&#39;</span><span class="p">,</span><span class="s1">&#39;STEAM&#39;</span><span class="p">,</span><span class="s1">&#39;OARSYS&#39;</span><span class="p">]</span>
<span class="n">p_true</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">true_covariates</span><span class="p">)</span>

<span class="c1"># noise covariates added for didactic reasons</span>
<span class="n">p_noise</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">noise_covariates</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p_noise</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">noise_covariates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;noise</span><span class="si">{0}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">covariates</span> <span class="o">=</span> <span class="n">true_covariates</span> <span class="o">+</span> <span class="n">noise_covariates</span>
<span class="n">x_noise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">p_noise</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28727</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span>
<span class="n">x_noise</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">x_noise</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">noise_covariates</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">data</span><span class="p">,</span> <span class="n">x_noise</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># sample size</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># total number of covariates</span>
<span class="n">p</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">covariates</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here’s the correlation between the first few covariates. Note how, most variables are positively correlated, which is expected since houses with more bedrooms will usually also have more bathrooms, larger area, etc.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">covariates</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">8</span><span class="p">]]</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>LOT</th>
      <th>UNITSF</th>
      <th>BUILT</th>
      <th>BATHS</th>
      <th>BEDRMS</th>
      <th>DINING</th>
      <th>METRO</th>
      <th>CRACKS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>LOT</th>
      <td>1.000000</td>
      <td>0.064841</td>
      <td>0.044639</td>
      <td>0.057325</td>
      <td>0.009626</td>
      <td>-0.015348</td>
      <td>0.136258</td>
      <td>0.016851</td>
    </tr>
    <tr>
      <th>UNITSF</th>
      <td>0.064841</td>
      <td>1.000000</td>
      <td>0.143201</td>
      <td>0.428723</td>
      <td>0.361165</td>
      <td>0.214030</td>
      <td>0.057441</td>
      <td>0.033548</td>
    </tr>
    <tr>
      <th>BUILT</th>
      <td>0.044639</td>
      <td>0.143201</td>
      <td>1.000000</td>
      <td>0.434519</td>
      <td>0.215109</td>
      <td>0.037468</td>
      <td>0.323703</td>
      <td>0.092390</td>
    </tr>
    <tr>
      <th>BATHS</th>
      <td>0.057325</td>
      <td>0.428723</td>
      <td>0.434519</td>
      <td>1.000000</td>
      <td>0.540230</td>
      <td>0.259457</td>
      <td>0.189812</td>
      <td>0.062819</td>
    </tr>
    <tr>
      <th>BEDRMS</th>
      <td>0.009626</td>
      <td>0.361165</td>
      <td>0.215109</td>
      <td>0.540230</td>
      <td>1.000000</td>
      <td>0.281846</td>
      <td>0.121331</td>
      <td>0.026779</td>
    </tr>
    <tr>
      <th>DINING</th>
      <td>-0.015348</td>
      <td>0.214030</td>
      <td>0.037468</td>
      <td>0.259457</td>
      <td>0.281846</td>
      <td>1.000000</td>
      <td>0.022026</td>
      <td>0.021270</td>
    </tr>
    <tr>
      <th>METRO</th>
      <td>0.136258</td>
      <td>0.057441</td>
      <td>0.323703</td>
      <td>0.189812</td>
      <td>0.121331</td>
      <td>0.022026</td>
      <td>1.000000</td>
      <td>0.057545</td>
    </tr>
    <tr>
      <th>CRACKS</th>
      <td>0.016851</td>
      <td>0.033548</td>
      <td>0.092390</td>
      <td>0.062819</td>
      <td>0.026779</td>
      <td>0.021270</td>
      <td>0.057545</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="section" id="generalized-linear-models">
<h3><span class="section-number">2.2.1. </span>Generalized linear models<a class="headerlink" href="#generalized-linear-models" title="Permalink to this headline">¶</a></h3>
<p>This class of models extends common methods such as linear and logistic regression by adding a penalty to the magnitude of the coefficients. <strong>Lasso</strong> penalizes the absolute value of slope coefficients. For regression problems, it becomes</p>
<div class="math notranslate nohighlight" id="equation-lasso">
<span class="eqno">(2.2)<a class="headerlink" href="#equation-lasso" title="Permalink to this equation">¶</a></span>\[
  \hat{b}_{Lasso} = \arg\min_b \sum_{i=1}^m
    \left( Y_i - b_0 - X_{i1}b_1 - \cdots - X_{ip}b_p \right)^2
    - \lambda \sum_{j=1}^p |b_j|
\]</div>
<p>Similarly, in a regression problem <strong>Ridge</strong> penalizes the sum of squares of the slope coefficients,</p>
<div class="math notranslate nohighlight" id="equation-ridge">
<span class="eqno">(2.3)<a class="headerlink" href="#equation-ridge" title="Permalink to this equation">¶</a></span>\[
  \hat{b}_{Ridge} = \arg\min_b \sum_{i=1}^m
    \left( Y_i - b_0 - X_{i1}b_1 - \cdots - X_{ip}b_p \right)^2
    - \lambda \sum_{j=1}^p b_j^2
\]</div>
<p>Also, there exists the <strong>Elastic Net</strong> penalization which consists of a convex combination between the other two. In all cases, the scalar parameter
<span class="math notranslate nohighlight">\(\lambda\)</span> controls the complexity of the model. For <span class="math notranslate nohighlight">\(\lambda=0\)</span>, the problem reduces to the “usual” linear regression. As <span class="math notranslate nohighlight">\(\lambda\)</span> increases, we favor simpler models. As we’ll see below, the optimal parameter <span class="math notranslate nohighlight">\(\lambda\)</span> is selected via cross-validation.</p>
<p>An important feature of Lasso-type penalization is that it promotes <strong>sparsity</strong> – that is, it forces many coefficients to be exactly zero. This is different from Ridge-type penalization, which forces coefficients to be small.</p>
<p>Another interesting property of these models is that, even though they are called “linear” models, this should actually be understood as <strong>linear in transformations</strong> of the covariates. For example, we could use polynomials or splines (continuous piecewise polynomials) of the covariates and allow for much more flexible models.</p>
<p>In fact, because of the penalization term, problems <a class="reference internal" href="#equation-lasso">(2.2)</a> and <a class="reference internal" href="#equation-ridge">(2.3)</a> remain well-defined and have a unique solution even in <strong>high-dimensional</strong> problems in which the number of coefficients <span class="math notranslate nohighlight">\(p\)</span> is larger than the sample size <span class="math notranslate nohighlight">\(n\)</span> – that is, our data is “fat” with more columns than rows. These situations can arise either naturally (e.g. genomics problems in which we have hundreds of thousands of gene expression information for a few individuals) or because we are including many transformations of a smaller set of covariates.</p>
<p>Finally, although here we are focusing on regression problems, other generalized linear models such as logistic regression can also be similarly modified by adding a Lasso, Ridge, or Elastic Net-type penalty to similar consequences.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">covariates</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">outcome</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># A formula of type &quot;~ x1 + x2 + ...&quot; (right-hand side only) to</span>
<span class="c1"># indicate how covariates should enter the model. If you&#39;d like to add, e.g.,</span>
<span class="c1"># third-order polynomials in x1, you could do so here by modifying the formula</span>
<span class="c1"># to be something like  &quot;~ poly(x1, 3) + x2 + ...&quot;</span>
<span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">()</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="mf">1e-8</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="mf">1e-1</span><span class="p">),</span> <span class="mi">100</span><span class="p">)</span>

<span class="n">tuned_parameters</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="n">alphas</span><span class="p">}]</span>
<span class="n">n_folds</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">scorer</span> <span class="o">=</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">)</span>

<span class="c1"># Use this formula instead if you&#39;d like to fit on piecewise polynomials</span>
<span class="c1"># fmla &lt;- formula(paste(&quot; ~ 0 + &quot;, paste0(&quot;bs(&quot;, covariates, &quot;, df=5)&quot;, collapse=&quot; + &quot;)))</span>

<span class="c1"># Function model.matrix selects the covariates according to the formula</span>
<span class="c1"># above and expands the covariates accordingly. In addition, if any column</span>
<span class="c1"># is a factor, then this creates dummies (one-hot encoding) as well.</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">lasso</span><span class="p">,</span> <span class="n">tuned_parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">n_folds</span><span class="p">,</span> <span class="n">refit</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="n">scorer</span><span class="p">)</span>

<span class="c1"># Fit a lasso model.</span>
<span class="c1"># Note this automatically performs cross-validation.</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s2">&quot;mean_test_score&quot;</span><span class="p">]</span>
<span class="n">scores_std</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s2">&quot;std_test_score&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>The next figure plots the average estimated MSE for each lambda. The red dots are the averages across all folds, and the error bars are based on the variability of mse estimates across folds. The vertical dashed lines show the (log) lambda with smallest estimated MSE (left) and the one whose mse is at most one standard error from the first (right).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_lasso</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span> <span class="s2">&quot;alphas&quot;</span><span class="p">),</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;scores&quot;</span><span class="p">)])</span><span class="o">.</span><span class="n">T</span>
<span class="n">best</span> <span class="o">=</span> <span class="n">data_lasso</span><span class="p">[</span><span class="n">data_lasso</span><span class="p">[</span><span class="s2">&quot;scores&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">data_lasso</span><span class="p">[</span><span class="s2">&quot;scores&quot;</span><span class="p">])]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">semilogx</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;red&quot;</span><span class="p">)</span>

<span class="c1"># plot error lines showing +/- std. errors of the scores</span>
<span class="n">std_error</span> <span class="o">=</span> <span class="n">scores_std</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_folds</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">semilogx</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">std_error</span><span class="p">,</span> <span class="s2">&quot;b--&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">semilogx</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">scores</span> <span class="o">-</span> <span class="n">std_error</span><span class="p">,</span> <span class="s2">&quot;b--&quot;</span><span class="p">)</span>

<span class="c1"># alpha=0.2 controls the translucency of the fill color</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">std_error</span><span class="p">,</span> <span class="n">scores</span> <span class="o">-</span> <span class="n">std_error</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;CV score +/- std error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;alpha&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">best</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;.5&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">alphas</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">alphas</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(1e-08, 0.1)
</pre></div>
</div>
<img alt="../_images/2_introduction_to_Machine_Learning_35_1.png" src="../_images/2_introduction_to_Machine_Learning_35_1.png" />
</div>
</div>
<p>Here are the first few estimated coefficients at the <span class="math notranslate nohighlight">\(\lambda\)</span> value that minimizes cross-validated MSE. Note that many estimated coefficients them are exactly zero.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Estimated coefficients at the lambda value that minimized cross-validated MSE</span>
<span class="n">lasso</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">best</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
<span class="n">table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">table</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">lasso</span><span class="o">.</span><span class="n">intercept_</span>
<span class="n">table</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">table</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">table</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">table</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;(Intercept)&#39;</span><span class="p">,</span><span class="s1">&#39;LOT&#39;</span><span class="p">,</span><span class="s1">&#39;UNITSF&#39;</span><span class="p">,</span><span class="s1">&#39;BUILT&#39;</span><span class="p">,</span><span class="s1">&#39;BATHS&#39;</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Coef.&#39;</span><span class="p">])</span> <span class="c1"># showing only first coefficients</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>(Intercept)</th>
      <th>LOT</th>
      <th>UNITSF</th>
      <th>BUILT</th>
      <th>BATHS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Coef.</th>
      <td>11.595941</td>
      <td>3.506156e-07</td>
      <td>0.000023</td>
      <td>0.000246</td>
      <td>0.247006</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of nonzero coefficients at optimal lambda:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">]),</span> <span class="s2">&quot;out of &quot;</span> <span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of nonzero coefficients at optimal lambda: 45 out of  63
</pre></div>
</div>
</div>
</div>
<p>Predictions and estimated MSE for the selected model are retrieved as follows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Retrieve predictions at best lambda regularization parameter</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">lasso</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Get k-fold cross validation</span>
<span class="n">mse_lasso</span>  <span class="o">=</span> <span class="n">best</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;glmnet MSE estimate (k-fold cross-validation):&quot;</span><span class="p">,</span> <span class="n">mse_lasso</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>glmnet MSE estimate (k-fold cross-validation): 0.6159603583713001
</pre></div>
</div>
</div>
</div>
<p>The next command plots estimated coefficients as a function of the regularization parameter <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coefs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
    <span class="n">lasso</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">a</span><span class="p">)</span>
    <span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
    <span class="n">coefs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">coefs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;tight&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Standardized Coefficients&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Lasso coefficients as a function of alpha&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_introduction_to_Machine_Learning_43_0.png" src="../_images/2_introduction_to_Machine_Learning_43_0.png" />
</div>
</div>
<p>It’s tempting to try to interpret the coefficients obtained via Lasso. Unfortunately, that can be very difficult, because by dropping covariates Lasso introduces a form of <strong>omitted variable bias</strong> (<a class="reference external" href="https://en.wikipedia.org/wiki/Omitted-variable_bias">wikipedia</a>). To understand this form of bias, consider the following toy example. We have two positively correlated independent variables, <code class="docutils literal notranslate"><span class="pre">x.1</span></code> and <code class="docutils literal notranslate"><span class="pre">x.2</span></code>, that are linearly related to the outcome <code class="docutils literal notranslate"><span class="pre">y</span></code>. Linear regression of <code class="docutils literal notranslate"><span class="pre">y</span></code> on <code class="docutils literal notranslate"><span class="pre">x1</span></code> and <code class="docutils literal notranslate"><span class="pre">x2</span></code> gives us the correct coefficients. However, if we <em>omit</em> <code class="docutils literal notranslate"><span class="pre">x2</span></code> from the estimation model, the coefficient on <code class="docutils literal notranslate"><span class="pre">x1</span></code> increases. This is because <code class="docutils literal notranslate"><span class="pre">x1</span></code> is now “picking up” the effect of the variable that was left out. In other words, the effect of <code class="docutils literal notranslate"><span class="pre">x1</span></code> seems stronger because we aren’t controlling for some other confounding variable. Note that the second model this still works for prediction, but we cannot interpret the coefficient as a measure of strength of the causal relationship between <code class="docutils literal notranslate"><span class="pre">x1</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generating some data </span>
<span class="c1"># y = 1 + 2*x1 + 3*x2 + noise, where corr(x1, x2) = .5</span>
<span class="c1"># note the sample size is very large -- this isn&#39;t solved by big data!</span>
<span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">]</span>
<span class="n">cov</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">1.5</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mf">1.5</span><span class="p">]]</span>

<span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="mi">100000</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x1</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">x2</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100000</span><span class="p">)</span>
<span class="n">data_sim</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">y</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span><span class="s1">&#39;x2&#39;</span><span class="p">,</span><span class="s1">&#39;y&#39;</span><span class="p">]</span> <span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Correct Model&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Correct Model
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;y ~ x1 + x2&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">data_sim</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.997
Model:                            OLS   Adj. R-squared:                  0.997
Method:                 Least Squares   F-statistic:                 1.896e+07
Date:                Fri, 22 Jul 2022   Prob (F-statistic):               0.00
Time:                        14:24:42   Log-Likelihood:                -17612.
No. Observations:              100000   AIC:                         3.523e+04
Df Residuals:                   99997   BIC:                         3.526e+04
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      1.4995      0.001   1643.183      0.000       1.498       1.501
x1             1.9996      0.001   1995.935      0.000       1.998       2.002
x2             3.0014      0.001   3008.677      0.000       2.999       3.003
==============================================================================
Omnibus:                    85479.613   Durbin-Watson:                   2.003
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5960.497
Skew:                           0.001   Prob(JB):                         0.00
Kurtosis:                       1.804   Cond. No.                         2.24
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model with omitted variable bias&quot;</span><span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;y ~ x1&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">data_sim</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model with omitted variable bias
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.759
Model:                            OLS   Adj. R-squared:                  0.759
Method:                 Least Squares   F-statistic:                 3.155e+05
Date:                Fri, 22 Jul 2022   Prob (F-statistic):               0.00
Time:                        14:24:42   Log-Likelihood:            -2.4344e+05
No. Observations:              100000   AIC:                         4.869e+05
Df Residuals:                   99998   BIC:                         4.869e+05
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      1.4911      0.009    170.803      0.000       1.474       1.508
x1             4.0103      0.007    561.652      0.000       3.996       4.024
==============================================================================
Omnibus:                        0.365   Durbin-Watson:                   2.008
Prob(Omnibus):                  0.833   Jarque-Bera (JB):                0.355
Skew:                           0.000   Prob(JB):                        0.837
Kurtosis:                       3.009   Cond. No.                         1.22
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>
</div>
</div>
</div>
<p>The phenomenon above occurs in Lasso and in any other sparsity-promoting method when correlated covariates are present since, by forcing coefficients to be zero, Lasso is effectively dropping them from the model. And as we have seen, as a variable gets dropped, a different variable that is correlated with it can “pick up” its effect, which in turn can cause bias. Once <span class="math notranslate nohighlight">\(\lambda\)</span> grows sufficiently large, the penalization term overwhelms any benefit of having that variable in the model, so that variable finally decreases to zero too.</p>
<p>One may instead consider using Lasso to select a subset of variables, and then regressing the outcome on the subset of selected variables via OLS (without any penalization). This method is often called <strong>post-lasso</strong>. Although it has desirable properties in terms of model fit (see e.g., <a class="reference external" href="https://arxiv.org/pdf/1001.0188.pdf">Belloni and Chernozhukov, 2013</a>), this procedure does not solve the omitted variable issue we mentioned above.</p>
<p>We illustrate this next. We observe the path of the estimated coefficient on the number of bathroooms (<code class="docutils literal notranslate"><span class="pre">BATHS</span></code>) as we increase <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># prepare data</span>
<span class="n">scale_X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1">###############################################</span>
<span class="c1"># fit ols model</span>
<span class="n">ols</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">ols</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scale_X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>
<span class="n">ols_coef</span> <span class="o">=</span> <span class="n">ols</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
<span class="n">lamdas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">0.4</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="c1"># retrieve ols coefficients</span>
<span class="n">coef_ols</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">ols_coef</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>

<span class="c1">###############################################</span>
<span class="c1"># fit lasso model</span>
<span class="n">lasso_bath_coef</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># retrieve lasso coefficients</span>
<span class="n">lasso_coefs</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">lamdas</span><span class="p">:</span>
    <span class="n">lasso</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">a</span><span class="p">,</span><span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scale_X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
    <span class="n">lasso_bath_coef</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
    <span class="n">lasso_coefs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
    
<span class="c1">#################################################   </span>
<span class="c1"># fit ridge model</span>
<span class="n">ridge_bath_coef</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># retrieve ridge coefficients</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">lamdas</span><span class="p">:</span>
    <span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">a</span><span class="p">,</span><span class="n">normalize</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scale_X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
    <span class="n">ridge_bath_coef</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ridge</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
    
<span class="c1">####################################################</span>
<span class="c1"># fit post-lasso model</span>
<span class="n">poslasso_coef</span> <span class="o">=</span> <span class="p">[</span> <span class="p">]</span>

<span class="c1">#loop over lasso coefficients and re-fit OLS to get post-lasso coefficients</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    
    <span class="c1"># which slopes are non-zero</span>
    <span class="n">scale_X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">(</span><span class="n">lasso_coefs</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">!=</span>  <span class="mi">0</span><span class="p">)])</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">(</span><span class="n">lasso_coefs</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">!=</span>  <span class="mi">0</span><span class="p">)])</span>

    <span class="c1"># if there are any non zero coefficients, estimate OLS</span>
    <span class="n">ols</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
    <span class="n">ols</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scale_X</span><span class="p">,</span><span class="n">Y</span><span class="p">)</span>  

    <span class="c1"># populate post-lasso coefficients</span>
    <span class="n">post_coef</span> <span class="o">=</span> <span class="n">ols</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">(</span><span class="n">lasso_coefs</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">!=</span>  <span class="mi">0</span><span class="p">)]</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">get_loc</span><span class="p">(</span><span class="s1">&#39;BATHS&#39;</span><span class="p">)]</span>                             
    <span class="n">poslasso_coef</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">post_coef</span> <span class="p">)</span>    
    
<span class="c1">#################################################</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamdas</span><span class="p">,</span> <span class="n">ridge_bath_coef</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Ridge&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;:&#39;</span><span class="p">,</span><span class="n">markevery</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamdas</span><span class="p">,</span> <span class="n">lasso_bath_coef</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;Lasso&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;^&#39;</span><span class="p">,</span><span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;dashed&#39;</span><span class="p">,</span><span class="n">markevery</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamdas</span><span class="p">,</span> <span class="n">coef_ols</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;OLS&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span><span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;dashed&#39;</span><span class="p">,</span><span class="n">markevery</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamdas</span><span class="p">,</span> <span class="n">poslasso_coef</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;postlasso&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span><span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span><span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;dashed&#39;</span><span class="p">,</span><span class="n">markevery</span><span class="o">=</span><span class="mi">8</span> <span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Coefficient estimate on Baths&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Coef&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;lambda&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 0, &#39;lambda&#39;)
</pre></div>
</div>
<img alt="../_images/2_introduction_to_Machine_Learning_49_1.png" src="../_images/2_introduction_to_Machine_Learning_49_1.png" />
</div>
</div>
<p>The OLS coefficients are not penalized, so they remain constant. Ridge estimates decrease monotonically as <span class="math notranslate nohighlight">\(\lambda\)</span> grows. Also, for this dataset, Lasso estimates first increase and then decrease. Meanwhile, the post-lasso coefficient estimates seem to behave somewhat erratically with <span class="math notranslate nohighlight">\(lambda\)</span>. To understand this behavior, let’s see what happens to the magnitude of other selected variables that are correlated with <code class="docutils literal notranslate"><span class="pre">BATHS</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scale_X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">UNITSF_coef</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">BEDRMS_coef</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">DINING_coef</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">lamdas</span><span class="p">:</span>
    <span class="n">lasso</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">a</span><span class="p">,</span><span class="n">normalize</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">lasso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scale_X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
    <span class="n">UNITSF_coef</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">BEDRMS_coef</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
    <span class="n">DINING_coef</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lasso</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamdas</span><span class="p">,</span> <span class="n">UNITSF_coef</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;UNITSF&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;black&#39;</span> <span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamdas</span><span class="p">,</span> <span class="n">BEDRMS_coef</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;BEDRMS&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span>  <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamdas</span><span class="p">,</span> <span class="n">DINING_coef</span><span class="p">,</span><span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;DINING&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span><span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;dotted&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Coef&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;lambda&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 0, &#39;lambda&#39;)
</pre></div>
</div>
<img alt="../_images/2_introduction_to_Machine_Learning_52_1.png" src="../_images/2_introduction_to_Machine_Learning_52_1.png" />
</div>
</div>
<p>Note how the discrete jumps in magnitude for the <code class="docutils literal notranslate"><span class="pre">BATHS</span></code> coefficient in the first coincide with, for example, variables <code class="docutils literal notranslate"><span class="pre">DINING</span></code> and <code class="docutils literal notranslate"><span class="pre">BEDRMS</span></code> being exactly zero. As these variables got dropped from the model, the coefficient on <code class="docutils literal notranslate"><span class="pre">BATHS</span></code> increased to pick up their effect.</p>
<p>Another problem with Lasso coefficients is their instability. When multiple variables are highly correlated we may spuriously drop several of them. To get a sense of the amount of variability, in the next snippet we fix <span class="math notranslate nohighlight">\(\lambda\)</span> and then look at the lasso coefficients estimated during cross-validation. We see that by simply removing one fold we can get a very different set of coefficients (nonzero coefficients are in black in the heatmap below). This is because there may be many choices of coefficients with similar predictive power, so the set of nonzero coefficients we end up with can be quite unstable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">itertools</span>

<span class="c1"># Fixing lambda. This choice is not very important; the same occurs any intermediate lambda value.</span>
<span class="n">nobs</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">nfold</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># Define folds indices </span>
<span class="n">list_1</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nfold</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span><span class="o">*</span><span class="n">nobs</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">nobs</span><span class="p">,</span><span class="n">nobs</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">foldid</span> <span class="o">=</span> <span class="p">[</span><span class="n">list_1</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">sample</span><span class="p">]</span>

<span class="c1"># Create split function(similar to R)</span>
<span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
    <span class="n">count</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span> <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">compress</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">el</span> <span class="o">==</span> <span class="n">i</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">f</span><span class="p">)))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">count</span><span class="p">)</span> <span class="p">)</span> 

<span class="c1"># Split observation indices into folds </span>
<span class="n">list_2</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nobs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
<span class="n">I</span> <span class="o">=</span> <span class="n">split</span><span class="p">(</span><span class="n">list_2</span><span class="p">,</span> <span class="n">foldid</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LassoCV</span>

<span class="n">scale_X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">lasso_coef_fold</span><span class="o">=</span><span class="p">[]</span>

<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">I</span><span class="p">)):</span>
    
        <span class="c1"># Split data - index to keep are in mask as booleans</span>
        <span class="n">include_idx</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">I</span><span class="p">[</span><span class="n">b</span><span class="p">])</span>  <span class="c1">#Here should go I[b] Set is more efficient, but doesn&#39;t reorder your elements if that is desireable</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([(</span><span class="n">i</span> <span class="ow">in</span> <span class="n">include_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))])</span>

        <span class="c1"># Lasso regression, excluding folds selected </span>
        
        <span class="n">lassocv</span> <span class="o">=</span> <span class="n">LassoCV</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">lassocv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scale_X</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">])</span>
        <span class="n">lasso_coef_fold</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lassocv</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">index_val</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Fold-1&#39;</span><span class="p">,</span><span class="s1">&#39;Fold-2&#39;</span><span class="p">,</span><span class="s1">&#39;Fold-3&#39;</span><span class="p">,</span><span class="s1">&#39;Fold-4&#39;</span><span class="p">,</span><span class="s1">&#39;Fold-5&#39;</span><span class="p">,</span><span class="s1">&#39;Fold-6&#39;</span><span class="p">,</span><span class="s1">&#39;Fold-7&#39;</span><span class="p">,</span><span class="s1">&#39;Fold-8&#39;</span><span class="p">,</span><span class="s1">&#39;Fold-9&#39;</span><span class="p">,</span><span class="s1">&#39;Fold-10&#39;</span><span class="p">]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span> <span class="n">lasso_coef_fold</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="n">index_val</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="n">df</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">applymap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s2">&quot;background-color: white&quot;</span> <span class="k">if</span> <span class="n">x</span><span class="o">==</span><span class="mi">0</span> <span class="k">else</span> <span class="s2">&quot;background-color: black&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style type="text/css">
#T_41201_row0_col0, #T_41201_row0_col1, #T_41201_row0_col2, #T_41201_row0_col3, #T_41201_row0_col4, #T_41201_row0_col5, #T_41201_row0_col6, #T_41201_row0_col7, #T_41201_row0_col8, #T_41201_row0_col9, #T_41201_row1_col0, #T_41201_row1_col1, #T_41201_row1_col2, #T_41201_row1_col3, #T_41201_row1_col4, #T_41201_row1_col5, #T_41201_row1_col6, #T_41201_row1_col7, #T_41201_row1_col8, #T_41201_row1_col9, #T_41201_row2_col0, #T_41201_row2_col1, #T_41201_row2_col3, #T_41201_row2_col4, #T_41201_row2_col5, #T_41201_row2_col6, #T_41201_row2_col7, #T_41201_row2_col8, #T_41201_row2_col9, #T_41201_row3_col0, #T_41201_row3_col1, #T_41201_row3_col2, #T_41201_row3_col3, #T_41201_row3_col4, #T_41201_row3_col5, #T_41201_row3_col6, #T_41201_row3_col7, #T_41201_row3_col8, #T_41201_row3_col9, #T_41201_row4_col0, #T_41201_row4_col1, #T_41201_row4_col2, #T_41201_row4_col3, #T_41201_row4_col4, #T_41201_row4_col5, #T_41201_row4_col6, #T_41201_row4_col7, #T_41201_row4_col8, #T_41201_row4_col9, #T_41201_row5_col0, #T_41201_row5_col1, #T_41201_row5_col2, #T_41201_row5_col3, #T_41201_row5_col4, #T_41201_row5_col5, #T_41201_row5_col6, #T_41201_row5_col7, #T_41201_row5_col8, #T_41201_row5_col9, #T_41201_row6_col1, #T_41201_row6_col3, #T_41201_row6_col5, #T_41201_row6_col6, #T_41201_row6_col9, #T_41201_row7_col0, #T_41201_row7_col1, #T_41201_row7_col2, #T_41201_row7_col3, #T_41201_row7_col4, #T_41201_row7_col5, #T_41201_row7_col6, #T_41201_row7_col7, #T_41201_row7_col8, #T_41201_row7_col9, #T_41201_row8_col0, #T_41201_row8_col1, #T_41201_row8_col2, #T_41201_row8_col3, #T_41201_row8_col4, #T_41201_row8_col5, #T_41201_row8_col6, #T_41201_row8_col7, #T_41201_row8_col8, #T_41201_row8_col9, #T_41201_row9_col0, #T_41201_row9_col1, #T_41201_row9_col2, #T_41201_row9_col3, #T_41201_row9_col4, #T_41201_row9_col5, #T_41201_row9_col6, #T_41201_row9_col7, #T_41201_row9_col8, #T_41201_row9_col9, #T_41201_row10_col0, #T_41201_row10_col1, #T_41201_row10_col2, #T_41201_row10_col5, #T_41201_row10_col8, #T_41201_row10_col9, #T_41201_row11_col0, #T_41201_row11_col1, #T_41201_row11_col3, #T_41201_row11_col4, #T_41201_row11_col6, #T_41201_row11_col7, #T_41201_row11_col8, #T_41201_row11_col9, #T_41201_row12_col0, #T_41201_row12_col1, #T_41201_row12_col2, #T_41201_row12_col3, #T_41201_row12_col4, #T_41201_row12_col5, #T_41201_row12_col6, #T_41201_row12_col7, #T_41201_row12_col8, #T_41201_row12_col9, #T_41201_row15_col0, #T_41201_row15_col1, #T_41201_row15_col2, #T_41201_row15_col3, #T_41201_row15_col4, #T_41201_row15_col5, #T_41201_row15_col6, #T_41201_row15_col7, #T_41201_row15_col8, #T_41201_row15_col9, #T_41201_row17_col0, #T_41201_row17_col2, #T_41201_row17_col3, #T_41201_row17_col5, #T_41201_row17_col6, #T_41201_row17_col7, #T_41201_row17_col8, #T_41201_row18_col0, #T_41201_row18_col1, #T_41201_row18_col2, #T_41201_row18_col3, #T_41201_row18_col4, #T_41201_row18_col5, #T_41201_row18_col6, #T_41201_row18_col7, #T_41201_row18_col8, #T_41201_row18_col9, #T_41201_row19_col0, #T_41201_row19_col1, #T_41201_row19_col2, #T_41201_row19_col3, #T_41201_row19_col4, #T_41201_row19_col5, #T_41201_row19_col6, #T_41201_row19_col7, #T_41201_row19_col8, #T_41201_row19_col9, #T_41201_row20_col0, #T_41201_row20_col1, #T_41201_row20_col2, #T_41201_row20_col3, #T_41201_row20_col4, #T_41201_row20_col6, #T_41201_row20_col7, #T_41201_row20_col8, #T_41201_row20_col9, #T_41201_row21_col0, #T_41201_row21_col1, #T_41201_row21_col2, #T_41201_row21_col3, #T_41201_row21_col4, #T_41201_row21_col5, #T_41201_row21_col6, #T_41201_row21_col7, #T_41201_row21_col8, #T_41201_row21_col9, #T_41201_row22_col0, #T_41201_row22_col1, #T_41201_row22_col2, #T_41201_row22_col3, #T_41201_row22_col4, #T_41201_row22_col5, #T_41201_row22_col6, #T_41201_row22_col7, #T_41201_row22_col8, #T_41201_row22_col9, #T_41201_row26_col1, #T_41201_row27_col0, #T_41201_row27_col1, #T_41201_row27_col2, #T_41201_row27_col3, #T_41201_row27_col4, #T_41201_row27_col5, #T_41201_row27_col6, #T_41201_row27_col7, #T_41201_row27_col8, #T_41201_row27_col9, #T_41201_row28_col0, #T_41201_row28_col1, #T_41201_row28_col2, #T_41201_row28_col3, #T_41201_row28_col4, #T_41201_row28_col5, #T_41201_row28_col6, #T_41201_row28_col7, #T_41201_row28_col8, #T_41201_row28_col9, #T_41201_row29_col0, #T_41201_row29_col1, #T_41201_row29_col2, #T_41201_row29_col3, #T_41201_row29_col4, #T_41201_row29_col5, #T_41201_row29_col6, #T_41201_row29_col7, #T_41201_row29_col8, #T_41201_row29_col9, #T_41201_row30_col0, #T_41201_row30_col1, #T_41201_row30_col2, #T_41201_row30_col3, #T_41201_row30_col4, #T_41201_row30_col5, #T_41201_row30_col6, #T_41201_row30_col7, #T_41201_row30_col8, #T_41201_row30_col9, #T_41201_row31_col1, #T_41201_row31_col2, #T_41201_row31_col3, #T_41201_row31_col4, #T_41201_row31_col5, #T_41201_row31_col6, #T_41201_row31_col7, #T_41201_row31_col8, #T_41201_row31_col9, #T_41201_row32_col0, #T_41201_row32_col1, #T_41201_row32_col2, #T_41201_row32_col3, #T_41201_row32_col4, #T_41201_row32_col5, #T_41201_row32_col6, #T_41201_row32_col7, #T_41201_row32_col8, #T_41201_row32_col9, #T_41201_row33_col0, #T_41201_row33_col1, #T_41201_row33_col2, #T_41201_row33_col3, #T_41201_row33_col4, #T_41201_row33_col5, #T_41201_row33_col6, #T_41201_row33_col7, #T_41201_row33_col8, #T_41201_row33_col9, #T_41201_row34_col0, #T_41201_row34_col1, #T_41201_row34_col2, #T_41201_row34_col3, #T_41201_row34_col4, #T_41201_row34_col5, #T_41201_row34_col6, #T_41201_row34_col7, #T_41201_row34_col8, #T_41201_row34_col9, #T_41201_row35_col0, #T_41201_row35_col1, #T_41201_row35_col2, #T_41201_row35_col3, #T_41201_row35_col4, #T_41201_row35_col5, #T_41201_row35_col6, #T_41201_row35_col7, #T_41201_row35_col8, #T_41201_row35_col9, #T_41201_row36_col0, #T_41201_row36_col1, #T_41201_row36_col2, #T_41201_row36_col3, #T_41201_row36_col4, #T_41201_row36_col5, #T_41201_row36_col6, #T_41201_row36_col7, #T_41201_row36_col8, #T_41201_row36_col9, #T_41201_row37_col0, #T_41201_row37_col1, #T_41201_row37_col2, #T_41201_row37_col3, #T_41201_row37_col4, #T_41201_row37_col5, #T_41201_row37_col6, #T_41201_row37_col7, #T_41201_row37_col8, #T_41201_row37_col9, #T_41201_row38_col0, #T_41201_row38_col1, #T_41201_row38_col2, #T_41201_row38_col3, #T_41201_row38_col4, #T_41201_row38_col5, #T_41201_row38_col6, #T_41201_row38_col7, #T_41201_row38_col8, #T_41201_row38_col9, #T_41201_row40_col0, #T_41201_row40_col1, #T_41201_row40_col2, #T_41201_row40_col3, #T_41201_row40_col4, #T_41201_row40_col5, #T_41201_row40_col6, #T_41201_row40_col7, #T_41201_row40_col8, #T_41201_row40_col9, #T_41201_row41_col0, #T_41201_row41_col3, #T_41201_row41_col4, #T_41201_row41_col5, #T_41201_row41_col8, #T_41201_row41_col9, #T_41201_row45_col0, #T_41201_row45_col1, #T_41201_row45_col2, #T_41201_row45_col3, #T_41201_row45_col4, #T_41201_row45_col5, #T_41201_row45_col6, #T_41201_row45_col7, #T_41201_row45_col8, #T_41201_row45_col9, #T_41201_row46_col5, #T_41201_row48_col0, #T_41201_row48_col1, #T_41201_row48_col2, #T_41201_row48_col3, #T_41201_row48_col4, #T_41201_row48_col5, #T_41201_row48_col6, #T_41201_row48_col7, #T_41201_row48_col8, #T_41201_row48_col9, #T_41201_row51_col1, #T_41201_row51_col2, #T_41201_row51_col3, #T_41201_row52_col5, #T_41201_row52_col8, #T_41201_row53_col0, #T_41201_row53_col1, #T_41201_row53_col3, #T_41201_row53_col5, #T_41201_row53_col6, #T_41201_row54_col4, #T_41201_row54_col5, #T_41201_row54_col6, #T_41201_row54_col7, #T_41201_row54_col9, #T_41201_row56_col7, #T_41201_row56_col9, #T_41201_row57_col1, #T_41201_row57_col5, #T_41201_row59_col0, #T_41201_row59_col1, #T_41201_row59_col2, #T_41201_row59_col5, #T_41201_row59_col6, #T_41201_row59_col7, #T_41201_row59_col8, #T_41201_row59_col9, #T_41201_row61_col5 {
  background-color: black;
}
#T_41201_row2_col2, #T_41201_row6_col0, #T_41201_row6_col2, #T_41201_row6_col4, #T_41201_row6_col7, #T_41201_row6_col8, #T_41201_row10_col3, #T_41201_row10_col4, #T_41201_row10_col6, #T_41201_row10_col7, #T_41201_row11_col2, #T_41201_row11_col5, #T_41201_row13_col0, #T_41201_row13_col1, #T_41201_row13_col2, #T_41201_row13_col3, #T_41201_row13_col4, #T_41201_row13_col5, #T_41201_row13_col6, #T_41201_row13_col7, #T_41201_row13_col8, #T_41201_row13_col9, #T_41201_row14_col0, #T_41201_row14_col1, #T_41201_row14_col2, #T_41201_row14_col3, #T_41201_row14_col4, #T_41201_row14_col5, #T_41201_row14_col6, #T_41201_row14_col7, #T_41201_row14_col8, #T_41201_row14_col9, #T_41201_row16_col0, #T_41201_row16_col1, #T_41201_row16_col2, #T_41201_row16_col3, #T_41201_row16_col4, #T_41201_row16_col5, #T_41201_row16_col6, #T_41201_row16_col7, #T_41201_row16_col8, #T_41201_row16_col9, #T_41201_row17_col1, #T_41201_row17_col4, #T_41201_row17_col9, #T_41201_row20_col5, #T_41201_row23_col0, #T_41201_row23_col1, #T_41201_row23_col2, #T_41201_row23_col3, #T_41201_row23_col4, #T_41201_row23_col5, #T_41201_row23_col6, #T_41201_row23_col7, #T_41201_row23_col8, #T_41201_row23_col9, #T_41201_row24_col0, #T_41201_row24_col1, #T_41201_row24_col2, #T_41201_row24_col3, #T_41201_row24_col4, #T_41201_row24_col5, #T_41201_row24_col6, #T_41201_row24_col7, #T_41201_row24_col8, #T_41201_row24_col9, #T_41201_row25_col0, #T_41201_row25_col1, #T_41201_row25_col2, #T_41201_row25_col3, #T_41201_row25_col4, #T_41201_row25_col5, #T_41201_row25_col6, #T_41201_row25_col7, #T_41201_row25_col8, #T_41201_row25_col9, #T_41201_row26_col0, #T_41201_row26_col2, #T_41201_row26_col3, #T_41201_row26_col4, #T_41201_row26_col5, #T_41201_row26_col6, #T_41201_row26_col7, #T_41201_row26_col8, #T_41201_row26_col9, #T_41201_row31_col0, #T_41201_row39_col0, #T_41201_row39_col1, #T_41201_row39_col2, #T_41201_row39_col3, #T_41201_row39_col4, #T_41201_row39_col5, #T_41201_row39_col6, #T_41201_row39_col7, #T_41201_row39_col8, #T_41201_row39_col9, #T_41201_row41_col1, #T_41201_row41_col2, #T_41201_row41_col6, #T_41201_row41_col7, #T_41201_row42_col0, #T_41201_row42_col1, #T_41201_row42_col2, #T_41201_row42_col3, #T_41201_row42_col4, #T_41201_row42_col5, #T_41201_row42_col6, #T_41201_row42_col7, #T_41201_row42_col8, #T_41201_row42_col9, #T_41201_row43_col0, #T_41201_row43_col1, #T_41201_row43_col2, #T_41201_row43_col3, #T_41201_row43_col4, #T_41201_row43_col5, #T_41201_row43_col6, #T_41201_row43_col7, #T_41201_row43_col8, #T_41201_row43_col9, #T_41201_row44_col0, #T_41201_row44_col1, #T_41201_row44_col2, #T_41201_row44_col3, #T_41201_row44_col4, #T_41201_row44_col5, #T_41201_row44_col6, #T_41201_row44_col7, #T_41201_row44_col8, #T_41201_row44_col9, #T_41201_row46_col0, #T_41201_row46_col1, #T_41201_row46_col2, #T_41201_row46_col3, #T_41201_row46_col4, #T_41201_row46_col6, #T_41201_row46_col7, #T_41201_row46_col8, #T_41201_row46_col9, #T_41201_row47_col0, #T_41201_row47_col1, #T_41201_row47_col2, #T_41201_row47_col3, #T_41201_row47_col4, #T_41201_row47_col5, #T_41201_row47_col6, #T_41201_row47_col7, #T_41201_row47_col8, #T_41201_row47_col9, #T_41201_row49_col0, #T_41201_row49_col1, #T_41201_row49_col2, #T_41201_row49_col3, #T_41201_row49_col4, #T_41201_row49_col5, #T_41201_row49_col6, #T_41201_row49_col7, #T_41201_row49_col8, #T_41201_row49_col9, #T_41201_row50_col0, #T_41201_row50_col1, #T_41201_row50_col2, #T_41201_row50_col3, #T_41201_row50_col4, #T_41201_row50_col5, #T_41201_row50_col6, #T_41201_row50_col7, #T_41201_row50_col8, #T_41201_row50_col9, #T_41201_row51_col0, #T_41201_row51_col4, #T_41201_row51_col5, #T_41201_row51_col6, #T_41201_row51_col7, #T_41201_row51_col8, #T_41201_row51_col9, #T_41201_row52_col0, #T_41201_row52_col1, #T_41201_row52_col2, #T_41201_row52_col3, #T_41201_row52_col4, #T_41201_row52_col6, #T_41201_row52_col7, #T_41201_row52_col9, #T_41201_row53_col2, #T_41201_row53_col4, #T_41201_row53_col7, #T_41201_row53_col8, #T_41201_row53_col9, #T_41201_row54_col0, #T_41201_row54_col1, #T_41201_row54_col2, #T_41201_row54_col3, #T_41201_row54_col8, #T_41201_row55_col0, #T_41201_row55_col1, #T_41201_row55_col2, #T_41201_row55_col3, #T_41201_row55_col4, #T_41201_row55_col5, #T_41201_row55_col6, #T_41201_row55_col7, #T_41201_row55_col8, #T_41201_row55_col9, #T_41201_row56_col0, #T_41201_row56_col1, #T_41201_row56_col2, #T_41201_row56_col3, #T_41201_row56_col4, #T_41201_row56_col5, #T_41201_row56_col6, #T_41201_row56_col8, #T_41201_row57_col0, #T_41201_row57_col2, #T_41201_row57_col3, #T_41201_row57_col4, #T_41201_row57_col6, #T_41201_row57_col7, #T_41201_row57_col8, #T_41201_row57_col9, #T_41201_row58_col0, #T_41201_row58_col1, #T_41201_row58_col2, #T_41201_row58_col3, #T_41201_row58_col4, #T_41201_row58_col5, #T_41201_row58_col6, #T_41201_row58_col7, #T_41201_row58_col8, #T_41201_row58_col9, #T_41201_row59_col3, #T_41201_row59_col4, #T_41201_row60_col0, #T_41201_row60_col1, #T_41201_row60_col2, #T_41201_row60_col3, #T_41201_row60_col4, #T_41201_row60_col5, #T_41201_row60_col6, #T_41201_row60_col7, #T_41201_row60_col8, #T_41201_row60_col9, #T_41201_row61_col0, #T_41201_row61_col1, #T_41201_row61_col2, #T_41201_row61_col3, #T_41201_row61_col4, #T_41201_row61_col6, #T_41201_row61_col7, #T_41201_row61_col8, #T_41201_row61_col9, #T_41201_row62_col0, #T_41201_row62_col1, #T_41201_row62_col2, #T_41201_row62_col3, #T_41201_row62_col4, #T_41201_row62_col5, #T_41201_row62_col6, #T_41201_row62_col7, #T_41201_row62_col8, #T_41201_row62_col9 {
  background-color: white;
}
</style>
<table id="T_41201_">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th class="col_heading level0 col0" >Fold-1</th>
      <th class="col_heading level0 col1" >Fold-2</th>
      <th class="col_heading level0 col2" >Fold-3</th>
      <th class="col_heading level0 col3" >Fold-4</th>
      <th class="col_heading level0 col4" >Fold-5</th>
      <th class="col_heading level0 col5" >Fold-6</th>
      <th class="col_heading level0 col6" >Fold-7</th>
      <th class="col_heading level0 col7" >Fold-8</th>
      <th class="col_heading level0 col8" >Fold-9</th>
      <th class="col_heading level0 col9" >Fold-10</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_41201_level0_row0" class="row_heading level0 row0" >LOT</th>
      <td id="T_41201_row0_col0" class="data row0 col0" >0.038375</td>
      <td id="T_41201_row0_col1" class="data row0 col1" >0.038462</td>
      <td id="T_41201_row0_col2" class="data row0 col2" >0.035620</td>
      <td id="T_41201_row0_col3" class="data row0 col3" >0.040026</td>
      <td id="T_41201_row0_col4" class="data row0 col4" >0.037944</td>
      <td id="T_41201_row0_col5" class="data row0 col5" >0.037086</td>
      <td id="T_41201_row0_col6" class="data row0 col6" >0.042715</td>
      <td id="T_41201_row0_col7" class="data row0 col7" >0.041485</td>
      <td id="T_41201_row0_col8" class="data row0 col8" >0.037108</td>
      <td id="T_41201_row0_col9" class="data row0 col9" >0.037315</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row1" class="row_heading level0 row1" >UNITSF</th>
      <td id="T_41201_row1_col0" class="data row1 col0" >0.045816</td>
      <td id="T_41201_row1_col1" class="data row1 col1" >0.044881</td>
      <td id="T_41201_row1_col2" class="data row1 col2" >0.046355</td>
      <td id="T_41201_row1_col3" class="data row1 col3" >0.048169</td>
      <td id="T_41201_row1_col4" class="data row1 col4" >0.043296</td>
      <td id="T_41201_row1_col5" class="data row1 col5" >0.051749</td>
      <td id="T_41201_row1_col6" class="data row1 col6" >0.047423</td>
      <td id="T_41201_row1_col7" class="data row1 col7" >0.044882</td>
      <td id="T_41201_row1_col8" class="data row1 col8" >0.046010</td>
      <td id="T_41201_row1_col9" class="data row1 col9" >0.044185</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row2" class="row_heading level0 row2" >BUILT</th>
      <td id="T_41201_row2_col0" class="data row2 col0" >0.001890</td>
      <td id="T_41201_row2_col1" class="data row2 col1" >0.006764</td>
      <td id="T_41201_row2_col2" class="data row2 col2" >0.000000</td>
      <td id="T_41201_row2_col3" class="data row2 col3" >0.004193</td>
      <td id="T_41201_row2_col4" class="data row2 col4" >0.003072</td>
      <td id="T_41201_row2_col5" class="data row2 col5" >0.001698</td>
      <td id="T_41201_row2_col6" class="data row2 col6" >0.004246</td>
      <td id="T_41201_row2_col7" class="data row2 col7" >0.004488</td>
      <td id="T_41201_row2_col8" class="data row2 col8" >0.005278</td>
      <td id="T_41201_row2_col9" class="data row2 col9" >0.002369</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row3" class="row_heading level0 row3" >BATHS</th>
      <td id="T_41201_row3_col0" class="data row3 col0" >0.198848</td>
      <td id="T_41201_row3_col1" class="data row3 col1" >0.197141</td>
      <td id="T_41201_row3_col2" class="data row3 col2" >0.199362</td>
      <td id="T_41201_row3_col3" class="data row3 col3" >0.197134</td>
      <td id="T_41201_row3_col4" class="data row3 col4" >0.198838</td>
      <td id="T_41201_row3_col5" class="data row3 col5" >0.199768</td>
      <td id="T_41201_row3_col6" class="data row3 col6" >0.197858</td>
      <td id="T_41201_row3_col7" class="data row3 col7" >0.197430</td>
      <td id="T_41201_row3_col8" class="data row3 col8" >0.199601</td>
      <td id="T_41201_row3_col9" class="data row3 col9" >0.199168</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row4" class="row_heading level0 row4" >BEDRMS</th>
      <td id="T_41201_row4_col0" class="data row4 col0" >0.053390</td>
      <td id="T_41201_row4_col1" class="data row4 col1" >0.057200</td>
      <td id="T_41201_row4_col2" class="data row4 col2" >0.055561</td>
      <td id="T_41201_row4_col3" class="data row4 col3" >0.055013</td>
      <td id="T_41201_row4_col4" class="data row4 col4" >0.056977</td>
      <td id="T_41201_row4_col5" class="data row4 col5" >0.049937</td>
      <td id="T_41201_row4_col6" class="data row4 col6" >0.054089</td>
      <td id="T_41201_row4_col7" class="data row4 col7" >0.055285</td>
      <td id="T_41201_row4_col8" class="data row4 col8" >0.053701</td>
      <td id="T_41201_row4_col9" class="data row4 col9" >0.055501</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row5" class="row_heading level0 row5" >DINING</th>
      <td id="T_41201_row5_col0" class="data row5 col0" >0.047148</td>
      <td id="T_41201_row5_col1" class="data row5 col1" >0.047023</td>
      <td id="T_41201_row5_col2" class="data row5 col2" >0.046705</td>
      <td id="T_41201_row5_col3" class="data row5 col3" >0.045837</td>
      <td id="T_41201_row5_col4" class="data row5 col4" >0.044675</td>
      <td id="T_41201_row5_col5" class="data row5 col5" >0.045737</td>
      <td id="T_41201_row5_col6" class="data row5 col6" >0.045932</td>
      <td id="T_41201_row5_col7" class="data row5 col7" >0.046118</td>
      <td id="T_41201_row5_col8" class="data row5 col8" >0.047773</td>
      <td id="T_41201_row5_col9" class="data row5 col9" >0.043836</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row6" class="row_heading level0 row6" >METRO</th>
      <td id="T_41201_row6_col0" class="data row6 col0" >0.000000</td>
      <td id="T_41201_row6_col1" class="data row6 col1" >0.001396</td>
      <td id="T_41201_row6_col2" class="data row6 col2" >0.000000</td>
      <td id="T_41201_row6_col3" class="data row6 col3" >0.000264</td>
      <td id="T_41201_row6_col4" class="data row6 col4" >0.000000</td>
      <td id="T_41201_row6_col5" class="data row6 col5" >0.003026</td>
      <td id="T_41201_row6_col6" class="data row6 col6" >0.001408</td>
      <td id="T_41201_row6_col7" class="data row6 col7" >0.000000</td>
      <td id="T_41201_row6_col8" class="data row6 col8" >0.000000</td>
      <td id="T_41201_row6_col9" class="data row6 col9" >0.001433</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row7" class="row_heading level0 row7" >CRACKS</th>
      <td id="T_41201_row7_col0" class="data row7 col0" >0.019067</td>
      <td id="T_41201_row7_col1" class="data row7 col1" >0.018613</td>
      <td id="T_41201_row7_col2" class="data row7 col2" >0.018821</td>
      <td id="T_41201_row7_col3" class="data row7 col3" >0.018442</td>
      <td id="T_41201_row7_col4" class="data row7 col4" >0.020744</td>
      <td id="T_41201_row7_col5" class="data row7 col5" >0.019275</td>
      <td id="T_41201_row7_col6" class="data row7 col6" >0.020786</td>
      <td id="T_41201_row7_col7" class="data row7 col7" >0.019331</td>
      <td id="T_41201_row7_col8" class="data row7 col8" >0.019138</td>
      <td id="T_41201_row7_col9" class="data row7 col9" >0.016787</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row8" class="row_heading level0 row8" >REGION</th>
      <td id="T_41201_row8_col0" class="data row8 col0" >0.082000</td>
      <td id="T_41201_row8_col1" class="data row8 col1" >0.080013</td>
      <td id="T_41201_row8_col2" class="data row8 col2" >0.079299</td>
      <td id="T_41201_row8_col3" class="data row8 col3" >0.081996</td>
      <td id="T_41201_row8_col4" class="data row8 col4" >0.081520</td>
      <td id="T_41201_row8_col5" class="data row8 col5" >0.087518</td>
      <td id="T_41201_row8_col6" class="data row8 col6" >0.078733</td>
      <td id="T_41201_row8_col7" class="data row8 col7" >0.078410</td>
      <td id="T_41201_row8_col8" class="data row8 col8" >0.081466</td>
      <td id="T_41201_row8_col9" class="data row8 col9" >0.078341</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row9" class="row_heading level0 row9" >METRO3</th>
      <td id="T_41201_row9_col0" class="data row9 col0" >0.008603</td>
      <td id="T_41201_row9_col1" class="data row9 col1" >0.006385</td>
      <td id="T_41201_row9_col2" class="data row9 col2" >0.009752</td>
      <td id="T_41201_row9_col3" class="data row9 col3" >0.008559</td>
      <td id="T_41201_row9_col4" class="data row9 col4" >0.006691</td>
      <td id="T_41201_row9_col5" class="data row9 col5" >0.005384</td>
      <td id="T_41201_row9_col6" class="data row9 col6" >0.008682</td>
      <td id="T_41201_row9_col7" class="data row9 col7" >0.009053</td>
      <td id="T_41201_row9_col8" class="data row9 col8" >0.007605</td>
      <td id="T_41201_row9_col9" class="data row9 col9" >0.009206</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row10" class="row_heading level0 row10" >PHONE</th>
      <td id="T_41201_row10_col0" class="data row10 col0" >0.000793</td>
      <td id="T_41201_row10_col1" class="data row10 col1" >0.003709</td>
      <td id="T_41201_row10_col2" class="data row10 col2" >0.003386</td>
      <td id="T_41201_row10_col3" class="data row10 col3" >0.000000</td>
      <td id="T_41201_row10_col4" class="data row10 col4" >0.000000</td>
      <td id="T_41201_row10_col5" class="data row10 col5" >0.003124</td>
      <td id="T_41201_row10_col6" class="data row10 col6" >0.000000</td>
      <td id="T_41201_row10_col7" class="data row10 col7" >0.000000</td>
      <td id="T_41201_row10_col8" class="data row10 col8" >0.001588</td>
      <td id="T_41201_row10_col9" class="data row10 col9" >0.004091</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row11" class="row_heading level0 row11" >KITCHEN</th>
      <td id="T_41201_row11_col0" class="data row11 col0" >-0.002009</td>
      <td id="T_41201_row11_col1" class="data row11 col1" >-0.002742</td>
      <td id="T_41201_row11_col2" class="data row11 col2" >-0.000000</td>
      <td id="T_41201_row11_col3" class="data row11 col3" >-0.001563</td>
      <td id="T_41201_row11_col4" class="data row11 col4" >-0.001156</td>
      <td id="T_41201_row11_col5" class="data row11 col5" >-0.000000</td>
      <td id="T_41201_row11_col6" class="data row11 col6" >-0.004483</td>
      <td id="T_41201_row11_col7" class="data row11 col7" >-0.002042</td>
      <td id="T_41201_row11_col8" class="data row11 col8" >-0.004884</td>
      <td id="T_41201_row11_col9" class="data row11 col9" >-0.002683</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row12" class="row_heading level0 row12" >MOBILTYP</th>
      <td id="T_41201_row12_col0" class="data row12 col0" >-0.108854</td>
      <td id="T_41201_row12_col1" class="data row12 col1" >-0.112842</td>
      <td id="T_41201_row12_col2" class="data row12 col2" >-0.108748</td>
      <td id="T_41201_row12_col3" class="data row12 col3" >-0.116270</td>
      <td id="T_41201_row12_col4" class="data row12 col4" >-0.115956</td>
      <td id="T_41201_row12_col5" class="data row12 col5" >-0.118131</td>
      <td id="T_41201_row12_col6" class="data row12 col6" >-0.112344</td>
      <td id="T_41201_row12_col7" class="data row12 col7" >-0.112582</td>
      <td id="T_41201_row12_col8" class="data row12 col8" >-0.119828</td>
      <td id="T_41201_row12_col9" class="data row12 col9" >-0.112318</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row13" class="row_heading level0 row13" >WINTEROVEN</th>
      <td id="T_41201_row13_col0" class="data row13 col0" >0.000000</td>
      <td id="T_41201_row13_col1" class="data row13 col1" >0.000000</td>
      <td id="T_41201_row13_col2" class="data row13 col2" >0.000000</td>
      <td id="T_41201_row13_col3" class="data row13 col3" >0.000000</td>
      <td id="T_41201_row13_col4" class="data row13 col4" >0.000000</td>
      <td id="T_41201_row13_col5" class="data row13 col5" >0.000000</td>
      <td id="T_41201_row13_col6" class="data row13 col6" >0.000000</td>
      <td id="T_41201_row13_col7" class="data row13 col7" >0.000000</td>
      <td id="T_41201_row13_col8" class="data row13 col8" >0.000000</td>
      <td id="T_41201_row13_col9" class="data row13 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row14" class="row_heading level0 row14" >WINTERKESP</th>
      <td id="T_41201_row14_col0" class="data row14 col0" >0.000000</td>
      <td id="T_41201_row14_col1" class="data row14 col1" >0.000000</td>
      <td id="T_41201_row14_col2" class="data row14 col2" >0.000000</td>
      <td id="T_41201_row14_col3" class="data row14 col3" >-0.000000</td>
      <td id="T_41201_row14_col4" class="data row14 col4" >0.000000</td>
      <td id="T_41201_row14_col5" class="data row14 col5" >0.000000</td>
      <td id="T_41201_row14_col6" class="data row14 col6" >0.000000</td>
      <td id="T_41201_row14_col7" class="data row14 col7" >0.000000</td>
      <td id="T_41201_row14_col8" class="data row14 col8" >0.000000</td>
      <td id="T_41201_row14_col9" class="data row14 col9" >-0.000000</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row15" class="row_heading level0 row15" >WINTERELSP</th>
      <td id="T_41201_row15_col0" class="data row15 col0" >0.020949</td>
      <td id="T_41201_row15_col1" class="data row15 col1" >0.029671</td>
      <td id="T_41201_row15_col2" class="data row15 col2" >0.026198</td>
      <td id="T_41201_row15_col3" class="data row15 col3" >0.020406</td>
      <td id="T_41201_row15_col4" class="data row15 col4" >0.024588</td>
      <td id="T_41201_row15_col5" class="data row15 col5" >0.029165</td>
      <td id="T_41201_row15_col6" class="data row15 col6" >0.026714</td>
      <td id="T_41201_row15_col7" class="data row15 col7" >0.021579</td>
      <td id="T_41201_row15_col8" class="data row15 col8" >0.022521</td>
      <td id="T_41201_row15_col9" class="data row15 col9" >0.026878</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row16" class="row_heading level0 row16" >WINTERWOOD</th>
      <td id="T_41201_row16_col0" class="data row16 col0" >0.000000</td>
      <td id="T_41201_row16_col1" class="data row16 col1" >0.000000</td>
      <td id="T_41201_row16_col2" class="data row16 col2" >0.000000</td>
      <td id="T_41201_row16_col3" class="data row16 col3" >-0.000000</td>
      <td id="T_41201_row16_col4" class="data row16 col4" >0.000000</td>
      <td id="T_41201_row16_col5" class="data row16 col5" >0.000000</td>
      <td id="T_41201_row16_col6" class="data row16 col6" >0.000000</td>
      <td id="T_41201_row16_col7" class="data row16 col7" >0.000000</td>
      <td id="T_41201_row16_col8" class="data row16 col8" >0.000000</td>
      <td id="T_41201_row16_col9" class="data row16 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row17" class="row_heading level0 row17" >WINTERNONE</th>
      <td id="T_41201_row17_col0" class="data row17 col0" >-0.005689</td>
      <td id="T_41201_row17_col1" class="data row17 col1" >-0.000000</td>
      <td id="T_41201_row17_col2" class="data row17 col2" >-0.003018</td>
      <td id="T_41201_row17_col3" class="data row17 col3" >-0.004438</td>
      <td id="T_41201_row17_col4" class="data row17 col4" >-0.000000</td>
      <td id="T_41201_row17_col5" class="data row17 col5" >-0.004586</td>
      <td id="T_41201_row17_col6" class="data row17 col6" >-0.007006</td>
      <td id="T_41201_row17_col7" class="data row17 col7" >-0.001980</td>
      <td id="T_41201_row17_col8" class="data row17 col8" >-0.001477</td>
      <td id="T_41201_row17_col9" class="data row17 col9" >-0.000000</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row18" class="row_heading level0 row18" >NEWC</th>
      <td id="T_41201_row18_col0" class="data row18 col0" >0.028393</td>
      <td id="T_41201_row18_col1" class="data row18 col1" >0.027592</td>
      <td id="T_41201_row18_col2" class="data row18 col2" >0.027837</td>
      <td id="T_41201_row18_col3" class="data row18 col3" >0.027809</td>
      <td id="T_41201_row18_col4" class="data row18 col4" >0.024305</td>
      <td id="T_41201_row18_col5" class="data row18 col5" >0.026748</td>
      <td id="T_41201_row18_col6" class="data row18 col6" >0.030938</td>
      <td id="T_41201_row18_col7" class="data row18 col7" >0.027100</td>
      <td id="T_41201_row18_col8" class="data row18 col8" >0.026399</td>
      <td id="T_41201_row18_col9" class="data row18 col9" >0.030795</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row19" class="row_heading level0 row19" >DISH</th>
      <td id="T_41201_row19_col0" class="data row19 col0" >-0.095896</td>
      <td id="T_41201_row19_col1" class="data row19 col1" >-0.095016</td>
      <td id="T_41201_row19_col2" class="data row19 col2" >-0.097825</td>
      <td id="T_41201_row19_col3" class="data row19 col3" >-0.095791</td>
      <td id="T_41201_row19_col4" class="data row19 col4" >-0.100828</td>
      <td id="T_41201_row19_col5" class="data row19 col5" >-0.093902</td>
      <td id="T_41201_row19_col6" class="data row19 col6" >-0.096623</td>
      <td id="T_41201_row19_col7" class="data row19 col7" >-0.096014</td>
      <td id="T_41201_row19_col8" class="data row19 col8" >-0.093249</td>
      <td id="T_41201_row19_col9" class="data row19 col9" >-0.098660</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row20" class="row_heading level0 row20" >WASH</th>
      <td id="T_41201_row20_col0" class="data row20 col0" >-0.005077</td>
      <td id="T_41201_row20_col1" class="data row20 col1" >-0.008160</td>
      <td id="T_41201_row20_col2" class="data row20 col2" >-0.016008</td>
      <td id="T_41201_row20_col3" class="data row20 col3" >-0.008782</td>
      <td id="T_41201_row20_col4" class="data row20 col4" >-0.008514</td>
      <td id="T_41201_row20_col5" class="data row20 col5" >-0.000000</td>
      <td id="T_41201_row20_col6" class="data row20 col6" >-0.004817</td>
      <td id="T_41201_row20_col7" class="data row20 col7" >-0.006111</td>
      <td id="T_41201_row20_col8" class="data row20 col8" >-0.008849</td>
      <td id="T_41201_row20_col9" class="data row20 col9" >-0.007913</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row21" class="row_heading level0 row21" >DRY</th>
      <td id="T_41201_row21_col0" class="data row21 col0" >-0.033092</td>
      <td id="T_41201_row21_col1" class="data row21 col1" >-0.032523</td>
      <td id="T_41201_row21_col2" class="data row21 col2" >-0.021707</td>
      <td id="T_41201_row21_col3" class="data row21 col3" >-0.031459</td>
      <td id="T_41201_row21_col4" class="data row21 col4" >-0.029623</td>
      <td id="T_41201_row21_col5" class="data row21 col5" >-0.036820</td>
      <td id="T_41201_row21_col6" class="data row21 col6" >-0.035534</td>
      <td id="T_41201_row21_col7" class="data row21 col7" >-0.028991</td>
      <td id="T_41201_row21_col8" class="data row21 col8" >-0.025354</td>
      <td id="T_41201_row21_col9" class="data row21 col9" >-0.031353</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row22" class="row_heading level0 row22" >NUNIT2</th>
      <td id="T_41201_row22_col0" class="data row22 col0" >-0.223423</td>
      <td id="T_41201_row22_col1" class="data row22 col1" >-0.222433</td>
      <td id="T_41201_row22_col2" class="data row22 col2" >-0.224703</td>
      <td id="T_41201_row22_col3" class="data row22 col3" >-0.218324</td>
      <td id="T_41201_row22_col4" class="data row22 col4" >-0.208873</td>
      <td id="T_41201_row22_col5" class="data row22 col5" >-0.209463</td>
      <td id="T_41201_row22_col6" class="data row22 col6" >-0.225208</td>
      <td id="T_41201_row22_col7" class="data row22 col7" >-0.219855</td>
      <td id="T_41201_row22_col8" class="data row22 col8" >-0.212460</td>
      <td id="T_41201_row22_col9" class="data row22 col9" >-0.217099</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row23" class="row_heading level0 row23" >BURNER</th>
      <td id="T_41201_row23_col0" class="data row23 col0" >-0.000000</td>
      <td id="T_41201_row23_col1" class="data row23 col1" >-0.000000</td>
      <td id="T_41201_row23_col2" class="data row23 col2" >-0.000000</td>
      <td id="T_41201_row23_col3" class="data row23 col3" >-0.000000</td>
      <td id="T_41201_row23_col4" class="data row23 col4" >-0.000000</td>
      <td id="T_41201_row23_col5" class="data row23 col5" >0.000000</td>
      <td id="T_41201_row23_col6" class="data row23 col6" >-0.000000</td>
      <td id="T_41201_row23_col7" class="data row23 col7" >0.000000</td>
      <td id="T_41201_row23_col8" class="data row23 col8" >-0.000000</td>
      <td id="T_41201_row23_col9" class="data row23 col9" >-0.000000</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row24" class="row_heading level0 row24" >COOK</th>
      <td id="T_41201_row24_col0" class="data row24 col0" >-0.000000</td>
      <td id="T_41201_row24_col1" class="data row24 col1" >-0.000000</td>
      <td id="T_41201_row24_col2" class="data row24 col2" >-0.000000</td>
      <td id="T_41201_row24_col3" class="data row24 col3" >-0.000000</td>
      <td id="T_41201_row24_col4" class="data row24 col4" >-0.000000</td>
      <td id="T_41201_row24_col5" class="data row24 col5" >0.000000</td>
      <td id="T_41201_row24_col6" class="data row24 col6" >-0.000000</td>
      <td id="T_41201_row24_col7" class="data row24 col7" >0.000000</td>
      <td id="T_41201_row24_col8" class="data row24 col8" >-0.000000</td>
      <td id="T_41201_row24_col9" class="data row24 col9" >-0.000000</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row25" class="row_heading level0 row25" >OVEN</th>
      <td id="T_41201_row25_col0" class="data row25 col0" >-0.000000</td>
      <td id="T_41201_row25_col1" class="data row25 col1" >0.000000</td>
      <td id="T_41201_row25_col2" class="data row25 col2" >-0.000000</td>
      <td id="T_41201_row25_col3" class="data row25 col3" >-0.000000</td>
      <td id="T_41201_row25_col4" class="data row25 col4" >-0.000000</td>
      <td id="T_41201_row25_col5" class="data row25 col5" >0.000000</td>
      <td id="T_41201_row25_col6" class="data row25 col6" >-0.000000</td>
      <td id="T_41201_row25_col7" class="data row25 col7" >0.000000</td>
      <td id="T_41201_row25_col8" class="data row25 col8" >-0.000000</td>
      <td id="T_41201_row25_col9" class="data row25 col9" >-0.000000</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row26" class="row_heading level0 row26" >REFR</th>
      <td id="T_41201_row26_col0" class="data row26 col0" >-0.000000</td>
      <td id="T_41201_row26_col1" class="data row26 col1" >-0.001651</td>
      <td id="T_41201_row26_col2" class="data row26 col2" >0.000000</td>
      <td id="T_41201_row26_col3" class="data row26 col3" >-0.000000</td>
      <td id="T_41201_row26_col4" class="data row26 col4" >-0.000000</td>
      <td id="T_41201_row26_col5" class="data row26 col5" >-0.000000</td>
      <td id="T_41201_row26_col6" class="data row26 col6" >-0.000000</td>
      <td id="T_41201_row26_col7" class="data row26 col7" >-0.000000</td>
      <td id="T_41201_row26_col8" class="data row26 col8" >-0.000000</td>
      <td id="T_41201_row26_col9" class="data row26 col9" >-0.000000</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row27" class="row_heading level0 row27" >DENS</th>
      <td id="T_41201_row27_col0" class="data row27 col0" >0.046345</td>
      <td id="T_41201_row27_col1" class="data row27 col1" >0.046745</td>
      <td id="T_41201_row27_col2" class="data row27 col2" >0.045410</td>
      <td id="T_41201_row27_col3" class="data row27 col3" >0.049517</td>
      <td id="T_41201_row27_col4" class="data row27 col4" >0.047755</td>
      <td id="T_41201_row27_col5" class="data row27 col5" >0.046103</td>
      <td id="T_41201_row27_col6" class="data row27 col6" >0.048747</td>
      <td id="T_41201_row27_col7" class="data row27 col7" >0.050443</td>
      <td id="T_41201_row27_col8" class="data row27 col8" >0.048496</td>
      <td id="T_41201_row27_col9" class="data row27 col9" >0.047664</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row28" class="row_heading level0 row28" >FAMRM</th>
      <td id="T_41201_row28_col0" class="data row28 col0" >0.055224</td>
      <td id="T_41201_row28_col1" class="data row28 col1" >0.058181</td>
      <td id="T_41201_row28_col2" class="data row28 col2" >0.057038</td>
      <td id="T_41201_row28_col3" class="data row28 col3" >0.054745</td>
      <td id="T_41201_row28_col4" class="data row28 col4" >0.056458</td>
      <td id="T_41201_row28_col5" class="data row28 col5" >0.060480</td>
      <td id="T_41201_row28_col6" class="data row28 col6" >0.057968</td>
      <td id="T_41201_row28_col7" class="data row28 col7" >0.057576</td>
      <td id="T_41201_row28_col8" class="data row28 col8" >0.057494</td>
      <td id="T_41201_row28_col9" class="data row28 col9" >0.057115</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row29" class="row_heading level0 row29" >HALFB</th>
      <td id="T_41201_row29_col0" class="data row29 col0" >0.107623</td>
      <td id="T_41201_row29_col1" class="data row29 col1" >0.103015</td>
      <td id="T_41201_row29_col2" class="data row29 col2" >0.102481</td>
      <td id="T_41201_row29_col3" class="data row29 col3" >0.105910</td>
      <td id="T_41201_row29_col4" class="data row29 col4" >0.106388</td>
      <td id="T_41201_row29_col5" class="data row29 col5" >0.109803</td>
      <td id="T_41201_row29_col6" class="data row29 col6" >0.104619</td>
      <td id="T_41201_row29_col7" class="data row29 col7" >0.105997</td>
      <td id="T_41201_row29_col8" class="data row29 col8" >0.105421</td>
      <td id="T_41201_row29_col9" class="data row29 col9" >0.102579</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row30" class="row_heading level0 row30" >KITCH</th>
      <td id="T_41201_row30_col0" class="data row30 col0" >-0.015594</td>
      <td id="T_41201_row30_col1" class="data row30 col1" >-0.015677</td>
      <td id="T_41201_row30_col2" class="data row30 col2" >-0.013482</td>
      <td id="T_41201_row30_col3" class="data row30 col3" >-0.016436</td>
      <td id="T_41201_row30_col4" class="data row30 col4" >-0.013020</td>
      <td id="T_41201_row30_col5" class="data row30 col5" >-0.014695</td>
      <td id="T_41201_row30_col6" class="data row30 col6" >-0.015417</td>
      <td id="T_41201_row30_col7" class="data row30 col7" >-0.014978</td>
      <td id="T_41201_row30_col8" class="data row30 col8" >-0.014721</td>
      <td id="T_41201_row30_col9" class="data row30 col9" >-0.016384</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row31" class="row_heading level0 row31" >LIVING</th>
      <td id="T_41201_row31_col0" class="data row31 col0" >0.000000</td>
      <td id="T_41201_row31_col1" class="data row31 col1" >0.004826</td>
      <td id="T_41201_row31_col2" class="data row31 col2" >0.007038</td>
      <td id="T_41201_row31_col3" class="data row31 col3" >0.002092</td>
      <td id="T_41201_row31_col4" class="data row31 col4" >0.002826</td>
      <td id="T_41201_row31_col5" class="data row31 col5" >0.003802</td>
      <td id="T_41201_row31_col6" class="data row31 col6" >0.004665</td>
      <td id="T_41201_row31_col7" class="data row31 col7" >0.002367</td>
      <td id="T_41201_row31_col8" class="data row31 col8" >0.002364</td>
      <td id="T_41201_row31_col9" class="data row31 col9" >0.006095</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row32" class="row_heading level0 row32" >OTHFN</th>
      <td id="T_41201_row32_col0" class="data row32 col0" >0.033794</td>
      <td id="T_41201_row32_col1" class="data row32 col1" >0.038993</td>
      <td id="T_41201_row32_col2" class="data row32 col2" >0.036988</td>
      <td id="T_41201_row32_col3" class="data row32 col3" >0.036415</td>
      <td id="T_41201_row32_col4" class="data row32 col4" >0.038120</td>
      <td id="T_41201_row32_col5" class="data row32 col5" >0.037899</td>
      <td id="T_41201_row32_col6" class="data row32 col6" >0.034072</td>
      <td id="T_41201_row32_col7" class="data row32 col7" >0.035835</td>
      <td id="T_41201_row32_col8" class="data row32 col8" >0.035931</td>
      <td id="T_41201_row32_col9" class="data row32 col9" >0.035770</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row33" class="row_heading level0 row33" >RECRM</th>
      <td id="T_41201_row33_col0" class="data row33 col0" >0.018683</td>
      <td id="T_41201_row33_col1" class="data row33 col1" >0.024274</td>
      <td id="T_41201_row33_col2" class="data row33 col2" >0.020155</td>
      <td id="T_41201_row33_col3" class="data row33 col3" >0.022219</td>
      <td id="T_41201_row33_col4" class="data row33 col4" >0.020117</td>
      <td id="T_41201_row33_col5" class="data row33 col5" >0.021063</td>
      <td id="T_41201_row33_col6" class="data row33 col6" >0.022285</td>
      <td id="T_41201_row33_col7" class="data row33 col7" >0.022064</td>
      <td id="T_41201_row33_col8" class="data row33 col8" >0.021186</td>
      <td id="T_41201_row33_col9" class="data row33 col9" >0.020107</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row34" class="row_heading level0 row34" >CLIMB</th>
      <td id="T_41201_row34_col0" class="data row34 col0" >0.010067</td>
      <td id="T_41201_row34_col1" class="data row34 col1" >0.014124</td>
      <td id="T_41201_row34_col2" class="data row34 col2" >0.009234</td>
      <td id="T_41201_row34_col3" class="data row34 col3" >0.012496</td>
      <td id="T_41201_row34_col4" class="data row34 col4" >0.012085</td>
      <td id="T_41201_row34_col5" class="data row34 col5" >0.013762</td>
      <td id="T_41201_row34_col6" class="data row34 col6" >0.010369</td>
      <td id="T_41201_row34_col7" class="data row34 col7" >0.007627</td>
      <td id="T_41201_row34_col8" class="data row34 col8" >0.011953</td>
      <td id="T_41201_row34_col9" class="data row34 col9" >0.011397</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row35" class="row_heading level0 row35" >ELEV</th>
      <td id="T_41201_row35_col0" class="data row35 col0" >0.082366</td>
      <td id="T_41201_row35_col1" class="data row35 col1" >0.079229</td>
      <td id="T_41201_row35_col2" class="data row35 col2" >0.083064</td>
      <td id="T_41201_row35_col3" class="data row35 col3" >0.075883</td>
      <td id="T_41201_row35_col4" class="data row35 col4" >0.071149</td>
      <td id="T_41201_row35_col5" class="data row35 col5" >0.074561</td>
      <td id="T_41201_row35_col6" class="data row35 col6" >0.081478</td>
      <td id="T_41201_row35_col7" class="data row35 col7" >0.075854</td>
      <td id="T_41201_row35_col8" class="data row35 col8" >0.078286</td>
      <td id="T_41201_row35_col9" class="data row35 col9" >0.079350</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row36" class="row_heading level0 row36" >DIRAC</th>
      <td id="T_41201_row36_col0" class="data row36 col0" >-0.000806</td>
      <td id="T_41201_row36_col1" class="data row36 col1" >-0.003574</td>
      <td id="T_41201_row36_col2" class="data row36 col2" >-0.002243</td>
      <td id="T_41201_row36_col3" class="data row36 col3" >-0.002077</td>
      <td id="T_41201_row36_col4" class="data row36 col4" >-0.003693</td>
      <td id="T_41201_row36_col5" class="data row36 col5" >-0.002800</td>
      <td id="T_41201_row36_col6" class="data row36 col6" >-0.000997</td>
      <td id="T_41201_row36_col7" class="data row36 col7" >-0.002300</td>
      <td id="T_41201_row36_col8" class="data row36 col8" >-0.002925</td>
      <td id="T_41201_row36_col9" class="data row36 col9" >-0.002027</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row37" class="row_heading level0 row37" >PORCH</th>
      <td id="T_41201_row37_col0" class="data row37 col0" >-0.013102</td>
      <td id="T_41201_row37_col1" class="data row37 col1" >-0.017889</td>
      <td id="T_41201_row37_col2" class="data row37 col2" >-0.011512</td>
      <td id="T_41201_row37_col3" class="data row37 col3" >-0.014472</td>
      <td id="T_41201_row37_col4" class="data row37 col4" >-0.017069</td>
      <td id="T_41201_row37_col5" class="data row37 col5" >-0.014086</td>
      <td id="T_41201_row37_col6" class="data row37 col6" >-0.014184</td>
      <td id="T_41201_row37_col7" class="data row37 col7" >-0.016777</td>
      <td id="T_41201_row37_col8" class="data row37 col8" >-0.013121</td>
      <td id="T_41201_row37_col9" class="data row37 col9" >-0.018314</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row38" class="row_heading level0 row38" >AIRSYS</th>
      <td id="T_41201_row38_col0" class="data row38 col0" >-0.052512</td>
      <td id="T_41201_row38_col1" class="data row38 col1" >-0.052436</td>
      <td id="T_41201_row38_col2" class="data row38 col2" >-0.054964</td>
      <td id="T_41201_row38_col3" class="data row38 col3" >-0.049350</td>
      <td id="T_41201_row38_col4" class="data row38 col4" >-0.051686</td>
      <td id="T_41201_row38_col5" class="data row38 col5" >-0.048910</td>
      <td id="T_41201_row38_col6" class="data row38 col6" >-0.050005</td>
      <td id="T_41201_row38_col7" class="data row38 col7" >-0.051392</td>
      <td id="T_41201_row38_col8" class="data row38 col8" >-0.052633</td>
      <td id="T_41201_row38_col9" class="data row38 col9" >-0.049923</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row39" class="row_heading level0 row39" >WELL</th>
      <td id="T_41201_row39_col0" class="data row39 col0" >-0.000000</td>
      <td id="T_41201_row39_col1" class="data row39 col1" >-0.000000</td>
      <td id="T_41201_row39_col2" class="data row39 col2" >-0.000000</td>
      <td id="T_41201_row39_col3" class="data row39 col3" >-0.000000</td>
      <td id="T_41201_row39_col4" class="data row39 col4" >0.000000</td>
      <td id="T_41201_row39_col5" class="data row39 col5" >-0.000000</td>
      <td id="T_41201_row39_col6" class="data row39 col6" >0.000000</td>
      <td id="T_41201_row39_col7" class="data row39 col7" >0.000000</td>
      <td id="T_41201_row39_col8" class="data row39 col8" >-0.000000</td>
      <td id="T_41201_row39_col9" class="data row39 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row40" class="row_heading level0 row40" >WELDUS</th>
      <td id="T_41201_row40_col0" class="data row40 col0" >-0.024787</td>
      <td id="T_41201_row40_col1" class="data row40 col1" >-0.023321</td>
      <td id="T_41201_row40_col2" class="data row40 col2" >-0.023290</td>
      <td id="T_41201_row40_col3" class="data row40 col3" >-0.019169</td>
      <td id="T_41201_row40_col4" class="data row40 col4" >-0.025551</td>
      <td id="T_41201_row40_col5" class="data row40 col5" >-0.026668</td>
      <td id="T_41201_row40_col6" class="data row40 col6" >-0.023939</td>
      <td id="T_41201_row40_col7" class="data row40 col7" >-0.021478</td>
      <td id="T_41201_row40_col8" class="data row40 col8" >-0.022485</td>
      <td id="T_41201_row40_col9" class="data row40 col9" >-0.023493</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row41" class="row_heading level0 row41" >STEAM</th>
      <td id="T_41201_row41_col0" class="data row41 col0" >0.000349</td>
      <td id="T_41201_row41_col1" class="data row41 col1" >0.000000</td>
      <td id="T_41201_row41_col2" class="data row41 col2" >0.000000</td>
      <td id="T_41201_row41_col3" class="data row41 col3" >0.000228</td>
      <td id="T_41201_row41_col4" class="data row41 col4" >0.000943</td>
      <td id="T_41201_row41_col5" class="data row41 col5" >0.002759</td>
      <td id="T_41201_row41_col6" class="data row41 col6" >0.000000</td>
      <td id="T_41201_row41_col7" class="data row41 col7" >0.000000</td>
      <td id="T_41201_row41_col8" class="data row41 col8" >0.000012</td>
      <td id="T_41201_row41_col9" class="data row41 col9" >0.001267</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row42" class="row_heading level0 row42" >OARSYS</th>
      <td id="T_41201_row42_col0" class="data row42 col0" >0.000000</td>
      <td id="T_41201_row42_col1" class="data row42 col1" >0.000000</td>
      <td id="T_41201_row42_col2" class="data row42 col2" >0.000000</td>
      <td id="T_41201_row42_col3" class="data row42 col3" >0.000000</td>
      <td id="T_41201_row42_col4" class="data row42 col4" >0.000000</td>
      <td id="T_41201_row42_col5" class="data row42 col5" >0.000000</td>
      <td id="T_41201_row42_col6" class="data row42 col6" >0.000000</td>
      <td id="T_41201_row42_col7" class="data row42 col7" >0.000000</td>
      <td id="T_41201_row42_col8" class="data row42 col8" >0.000000</td>
      <td id="T_41201_row42_col9" class="data row42 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row43" class="row_heading level0 row43" >noise1</th>
      <td id="T_41201_row43_col0" class="data row43 col0" >0.000000</td>
      <td id="T_41201_row43_col1" class="data row43 col1" >0.000000</td>
      <td id="T_41201_row43_col2" class="data row43 col2" >0.000000</td>
      <td id="T_41201_row43_col3" class="data row43 col3" >0.000000</td>
      <td id="T_41201_row43_col4" class="data row43 col4" >0.000000</td>
      <td id="T_41201_row43_col5" class="data row43 col5" >-0.000000</td>
      <td id="T_41201_row43_col6" class="data row43 col6" >0.000000</td>
      <td id="T_41201_row43_col7" class="data row43 col7" >0.000000</td>
      <td id="T_41201_row43_col8" class="data row43 col8" >0.000000</td>
      <td id="T_41201_row43_col9" class="data row43 col9" >-0.000000</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row44" class="row_heading level0 row44" >noise2</th>
      <td id="T_41201_row44_col0" class="data row44 col0" >-0.000000</td>
      <td id="T_41201_row44_col1" class="data row44 col1" >0.000000</td>
      <td id="T_41201_row44_col2" class="data row44 col2" >-0.000000</td>
      <td id="T_41201_row44_col3" class="data row44 col3" >-0.000000</td>
      <td id="T_41201_row44_col4" class="data row44 col4" >0.000000</td>
      <td id="T_41201_row44_col5" class="data row44 col5" >0.000000</td>
      <td id="T_41201_row44_col6" class="data row44 col6" >-0.000000</td>
      <td id="T_41201_row44_col7" class="data row44 col7" >-0.000000</td>
      <td id="T_41201_row44_col8" class="data row44 col8" >-0.000000</td>
      <td id="T_41201_row44_col9" class="data row44 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row45" class="row_heading level0 row45" >noise3</th>
      <td id="T_41201_row45_col0" class="data row45 col0" >-0.007188</td>
      <td id="T_41201_row45_col1" class="data row45 col1" >-0.004911</td>
      <td id="T_41201_row45_col2" class="data row45 col2" >-0.004338</td>
      <td id="T_41201_row45_col3" class="data row45 col3" >-0.002588</td>
      <td id="T_41201_row45_col4" class="data row45 col4" >-0.004051</td>
      <td id="T_41201_row45_col5" class="data row45 col5" >-0.005647</td>
      <td id="T_41201_row45_col6" class="data row45 col6" >-0.004283</td>
      <td id="T_41201_row45_col7" class="data row45 col7" >-0.007424</td>
      <td id="T_41201_row45_col8" class="data row45 col8" >-0.004337</td>
      <td id="T_41201_row45_col9" class="data row45 col9" >-0.003884</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row46" class="row_heading level0 row46" >noise4</th>
      <td id="T_41201_row46_col0" class="data row46 col0" >0.000000</td>
      <td id="T_41201_row46_col1" class="data row46 col1" >0.000000</td>
      <td id="T_41201_row46_col2" class="data row46 col2" >-0.000000</td>
      <td id="T_41201_row46_col3" class="data row46 col3" >0.000000</td>
      <td id="T_41201_row46_col4" class="data row46 col4" >0.000000</td>
      <td id="T_41201_row46_col5" class="data row46 col5" >0.000968</td>
      <td id="T_41201_row46_col6" class="data row46 col6" >-0.000000</td>
      <td id="T_41201_row46_col7" class="data row46 col7" >-0.000000</td>
      <td id="T_41201_row46_col8" class="data row46 col8" >-0.000000</td>
      <td id="T_41201_row46_col9" class="data row46 col9" >-0.000000</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row47" class="row_heading level0 row47" >noise5</th>
      <td id="T_41201_row47_col0" class="data row47 col0" >-0.000000</td>
      <td id="T_41201_row47_col1" class="data row47 col1" >-0.000000</td>
      <td id="T_41201_row47_col2" class="data row47 col2" >-0.000000</td>
      <td id="T_41201_row47_col3" class="data row47 col3" >-0.000000</td>
      <td id="T_41201_row47_col4" class="data row47 col4" >-0.000000</td>
      <td id="T_41201_row47_col5" class="data row47 col5" >-0.000000</td>
      <td id="T_41201_row47_col6" class="data row47 col6" >-0.000000</td>
      <td id="T_41201_row47_col7" class="data row47 col7" >-0.000000</td>
      <td id="T_41201_row47_col8" class="data row47 col8" >-0.000000</td>
      <td id="T_41201_row47_col9" class="data row47 col9" >-0.000000</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row48" class="row_heading level0 row48" >noise6</th>
      <td id="T_41201_row48_col0" class="data row48 col0" >0.003824</td>
      <td id="T_41201_row48_col1" class="data row48 col1" >0.003678</td>
      <td id="T_41201_row48_col2" class="data row48 col2" >0.004244</td>
      <td id="T_41201_row48_col3" class="data row48 col3" >0.004028</td>
      <td id="T_41201_row48_col4" class="data row48 col4" >0.002491</td>
      <td id="T_41201_row48_col5" class="data row48 col5" >0.005234</td>
      <td id="T_41201_row48_col6" class="data row48 col6" >0.004747</td>
      <td id="T_41201_row48_col7" class="data row48 col7" >0.002530</td>
      <td id="T_41201_row48_col8" class="data row48 col8" >0.003875</td>
      <td id="T_41201_row48_col9" class="data row48 col9" >0.002433</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row49" class="row_heading level0 row49" >noise7</th>
      <td id="T_41201_row49_col0" class="data row49 col0" >0.000000</td>
      <td id="T_41201_row49_col1" class="data row49 col1" >0.000000</td>
      <td id="T_41201_row49_col2" class="data row49 col2" >-0.000000</td>
      <td id="T_41201_row49_col3" class="data row49 col3" >0.000000</td>
      <td id="T_41201_row49_col4" class="data row49 col4" >0.000000</td>
      <td id="T_41201_row49_col5" class="data row49 col5" >0.000000</td>
      <td id="T_41201_row49_col6" class="data row49 col6" >0.000000</td>
      <td id="T_41201_row49_col7" class="data row49 col7" >0.000000</td>
      <td id="T_41201_row49_col8" class="data row49 col8" >0.000000</td>
      <td id="T_41201_row49_col9" class="data row49 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row50" class="row_heading level0 row50" >noise8</th>
      <td id="T_41201_row50_col0" class="data row50 col0" >0.000000</td>
      <td id="T_41201_row50_col1" class="data row50 col1" >-0.000000</td>
      <td id="T_41201_row50_col2" class="data row50 col2" >-0.000000</td>
      <td id="T_41201_row50_col3" class="data row50 col3" >-0.000000</td>
      <td id="T_41201_row50_col4" class="data row50 col4" >0.000000</td>
      <td id="T_41201_row50_col5" class="data row50 col5" >0.000000</td>
      <td id="T_41201_row50_col6" class="data row50 col6" >0.000000</td>
      <td id="T_41201_row50_col7" class="data row50 col7" >-0.000000</td>
      <td id="T_41201_row50_col8" class="data row50 col8" >-0.000000</td>
      <td id="T_41201_row50_col9" class="data row50 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row51" class="row_heading level0 row51" >noise9</th>
      <td id="T_41201_row51_col0" class="data row51 col0" >0.000000</td>
      <td id="T_41201_row51_col1" class="data row51 col1" >0.001210</td>
      <td id="T_41201_row51_col2" class="data row51 col2" >0.000766</td>
      <td id="T_41201_row51_col3" class="data row51 col3" >0.000735</td>
      <td id="T_41201_row51_col4" class="data row51 col4" >0.000000</td>
      <td id="T_41201_row51_col5" class="data row51 col5" >0.000000</td>
      <td id="T_41201_row51_col6" class="data row51 col6" >0.000000</td>
      <td id="T_41201_row51_col7" class="data row51 col7" >0.000000</td>
      <td id="T_41201_row51_col8" class="data row51 col8" >0.000000</td>
      <td id="T_41201_row51_col9" class="data row51 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row52" class="row_heading level0 row52" >noise10</th>
      <td id="T_41201_row52_col0" class="data row52 col0" >-0.000000</td>
      <td id="T_41201_row52_col1" class="data row52 col1" >-0.000000</td>
      <td id="T_41201_row52_col2" class="data row52 col2" >-0.000000</td>
      <td id="T_41201_row52_col3" class="data row52 col3" >-0.000000</td>
      <td id="T_41201_row52_col4" class="data row52 col4" >-0.000000</td>
      <td id="T_41201_row52_col5" class="data row52 col5" >-0.000817</td>
      <td id="T_41201_row52_col6" class="data row52 col6" >-0.000000</td>
      <td id="T_41201_row52_col7" class="data row52 col7" >-0.000000</td>
      <td id="T_41201_row52_col8" class="data row52 col8" >-0.000206</td>
      <td id="T_41201_row52_col9" class="data row52 col9" >-0.000000</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row53" class="row_heading level0 row53" >noise11</th>
      <td id="T_41201_row53_col0" class="data row53 col0" >0.001982</td>
      <td id="T_41201_row53_col1" class="data row53 col1" >0.000358</td>
      <td id="T_41201_row53_col2" class="data row53 col2" >0.000000</td>
      <td id="T_41201_row53_col3" class="data row53 col3" >0.000526</td>
      <td id="T_41201_row53_col4" class="data row53 col4" >0.000000</td>
      <td id="T_41201_row53_col5" class="data row53 col5" >0.002401</td>
      <td id="T_41201_row53_col6" class="data row53 col6" >0.003991</td>
      <td id="T_41201_row53_col7" class="data row53 col7" >0.000000</td>
      <td id="T_41201_row53_col8" class="data row53 col8" >0.000000</td>
      <td id="T_41201_row53_col9" class="data row53 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row54" class="row_heading level0 row54" >noise12</th>
      <td id="T_41201_row54_col0" class="data row54 col0" >-0.000000</td>
      <td id="T_41201_row54_col1" class="data row54 col1" >-0.000000</td>
      <td id="T_41201_row54_col2" class="data row54 col2" >-0.000000</td>
      <td id="T_41201_row54_col3" class="data row54 col3" >-0.000000</td>
      <td id="T_41201_row54_col4" class="data row54 col4" >-0.000168</td>
      <td id="T_41201_row54_col5" class="data row54 col5" >-0.000970</td>
      <td id="T_41201_row54_col6" class="data row54 col6" >-0.000110</td>
      <td id="T_41201_row54_col7" class="data row54 col7" >-0.000715</td>
      <td id="T_41201_row54_col8" class="data row54 col8" >-0.000000</td>
      <td id="T_41201_row54_col9" class="data row54 col9" >-0.000165</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row55" class="row_heading level0 row55" >noise13</th>
      <td id="T_41201_row55_col0" class="data row55 col0" >-0.000000</td>
      <td id="T_41201_row55_col1" class="data row55 col1" >-0.000000</td>
      <td id="T_41201_row55_col2" class="data row55 col2" >-0.000000</td>
      <td id="T_41201_row55_col3" class="data row55 col3" >0.000000</td>
      <td id="T_41201_row55_col4" class="data row55 col4" >-0.000000</td>
      <td id="T_41201_row55_col5" class="data row55 col5" >-0.000000</td>
      <td id="T_41201_row55_col6" class="data row55 col6" >-0.000000</td>
      <td id="T_41201_row55_col7" class="data row55 col7" >-0.000000</td>
      <td id="T_41201_row55_col8" class="data row55 col8" >-0.000000</td>
      <td id="T_41201_row55_col9" class="data row55 col9" >-0.000000</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row56" class="row_heading level0 row56" >noise14</th>
      <td id="T_41201_row56_col0" class="data row56 col0" >0.000000</td>
      <td id="T_41201_row56_col1" class="data row56 col1" >0.000000</td>
      <td id="T_41201_row56_col2" class="data row56 col2" >0.000000</td>
      <td id="T_41201_row56_col3" class="data row56 col3" >0.000000</td>
      <td id="T_41201_row56_col4" class="data row56 col4" >0.000000</td>
      <td id="T_41201_row56_col5" class="data row56 col5" >0.000000</td>
      <td id="T_41201_row56_col6" class="data row56 col6" >0.000000</td>
      <td id="T_41201_row56_col7" class="data row56 col7" >0.000734</td>
      <td id="T_41201_row56_col8" class="data row56 col8" >0.000000</td>
      <td id="T_41201_row56_col9" class="data row56 col9" >0.000154</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row57" class="row_heading level0 row57" >noise15</th>
      <td id="T_41201_row57_col0" class="data row57 col0" >0.000000</td>
      <td id="T_41201_row57_col1" class="data row57 col1" >0.000847</td>
      <td id="T_41201_row57_col2" class="data row57 col2" >0.000000</td>
      <td id="T_41201_row57_col3" class="data row57 col3" >0.000000</td>
      <td id="T_41201_row57_col4" class="data row57 col4" >0.000000</td>
      <td id="T_41201_row57_col5" class="data row57 col5" >0.000757</td>
      <td id="T_41201_row57_col6" class="data row57 col6" >0.000000</td>
      <td id="T_41201_row57_col7" class="data row57 col7" >0.000000</td>
      <td id="T_41201_row57_col8" class="data row57 col8" >0.000000</td>
      <td id="T_41201_row57_col9" class="data row57 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row58" class="row_heading level0 row58" >noise16</th>
      <td id="T_41201_row58_col0" class="data row58 col0" >-0.000000</td>
      <td id="T_41201_row58_col1" class="data row58 col1" >-0.000000</td>
      <td id="T_41201_row58_col2" class="data row58 col2" >-0.000000</td>
      <td id="T_41201_row58_col3" class="data row58 col3" >-0.000000</td>
      <td id="T_41201_row58_col4" class="data row58 col4" >-0.000000</td>
      <td id="T_41201_row58_col5" class="data row58 col5" >-0.000000</td>
      <td id="T_41201_row58_col6" class="data row58 col6" >-0.000000</td>
      <td id="T_41201_row58_col7" class="data row58 col7" >-0.000000</td>
      <td id="T_41201_row58_col8" class="data row58 col8" >-0.000000</td>
      <td id="T_41201_row58_col9" class="data row58 col9" >-0.000000</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row59" class="row_heading level0 row59" >noise17</th>
      <td id="T_41201_row59_col0" class="data row59 col0" >-0.002192</td>
      <td id="T_41201_row59_col1" class="data row59 col1" >-0.003576</td>
      <td id="T_41201_row59_col2" class="data row59 col2" >-0.000737</td>
      <td id="T_41201_row59_col3" class="data row59 col3" >-0.000000</td>
      <td id="T_41201_row59_col4" class="data row59 col4" >-0.000000</td>
      <td id="T_41201_row59_col5" class="data row59 col5" >-0.007291</td>
      <td id="T_41201_row59_col6" class="data row59 col6" >-0.000295</td>
      <td id="T_41201_row59_col7" class="data row59 col7" >-0.001651</td>
      <td id="T_41201_row59_col8" class="data row59 col8" >-0.000164</td>
      <td id="T_41201_row59_col9" class="data row59 col9" >-0.002472</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row60" class="row_heading level0 row60" >noise18</th>
      <td id="T_41201_row60_col0" class="data row60 col0" >-0.000000</td>
      <td id="T_41201_row60_col1" class="data row60 col1" >-0.000000</td>
      <td id="T_41201_row60_col2" class="data row60 col2" >-0.000000</td>
      <td id="T_41201_row60_col3" class="data row60 col3" >-0.000000</td>
      <td id="T_41201_row60_col4" class="data row60 col4" >0.000000</td>
      <td id="T_41201_row60_col5" class="data row60 col5" >0.000000</td>
      <td id="T_41201_row60_col6" class="data row60 col6" >-0.000000</td>
      <td id="T_41201_row60_col7" class="data row60 col7" >-0.000000</td>
      <td id="T_41201_row60_col8" class="data row60 col8" >0.000000</td>
      <td id="T_41201_row60_col9" class="data row60 col9" >0.000000</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row61" class="row_heading level0 row61" >noise19</th>
      <td id="T_41201_row61_col0" class="data row61 col0" >-0.000000</td>
      <td id="T_41201_row61_col1" class="data row61 col1" >-0.000000</td>
      <td id="T_41201_row61_col2" class="data row61 col2" >-0.000000</td>
      <td id="T_41201_row61_col3" class="data row61 col3" >-0.000000</td>
      <td id="T_41201_row61_col4" class="data row61 col4" >-0.000000</td>
      <td id="T_41201_row61_col5" class="data row61 col5" >-0.002338</td>
      <td id="T_41201_row61_col6" class="data row61 col6" >-0.000000</td>
      <td id="T_41201_row61_col7" class="data row61 col7" >-0.000000</td>
      <td id="T_41201_row61_col8" class="data row61 col8" >-0.000000</td>
      <td id="T_41201_row61_col9" class="data row61 col9" >-0.000000</td>
    </tr>
    <tr>
      <th id="T_41201_level0_row62" class="row_heading level0 row62" >noise20</th>
      <td id="T_41201_row62_col0" class="data row62 col0" >-0.000000</td>
      <td id="T_41201_row62_col1" class="data row62 col1" >-0.000000</td>
      <td id="T_41201_row62_col2" class="data row62 col2" >-0.000000</td>
      <td id="T_41201_row62_col3" class="data row62 col3" >0.000000</td>
      <td id="T_41201_row62_col4" class="data row62 col4" >-0.000000</td>
      <td id="T_41201_row62_col5" class="data row62 col5" >-0.000000</td>
      <td id="T_41201_row62_col6" class="data row62 col6" >0.000000</td>
      <td id="T_41201_row62_col7" class="data row62 col7" >0.000000</td>
      <td id="T_41201_row62_col8" class="data row62 col8" >-0.000000</td>
      <td id="T_41201_row62_col9" class="data row62 col9" >-0.000000</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>As we have seen above, any interpretation needs to take into account the joint distribution of covariates. One possible heuristic is to consider <strong>data-driven subgroups</strong>. For example, we can analyze what differentiates observations whose predictions are high from those whose predictions are low. The following code estimates a flexible Lasso model with splines, ranks the observations into a few subgroups according to their predicted outcomes, and then estimates the average covariate value for each subgroup.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">itertools</span>

<span class="c1"># Number of data-driven subgroups.</span>
<span class="n">nobs</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Fold indices</span>
<span class="n">nfold</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># Define folds indices </span>
<span class="n">list_1</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nfold</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span><span class="o">*</span><span class="n">nobs</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">nobs</span><span class="p">,</span><span class="n">nobs</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">foldid</span> <span class="o">=</span> <span class="p">[</span><span class="n">list_1</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">sample</span><span class="p">]</span>

<span class="c1"># Create split function(similar to R)</span>
<span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
    <span class="n">count</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span> <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">compress</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">el</span> <span class="o">==</span> <span class="n">i</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">f</span><span class="p">)))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">count</span><span class="p">)</span> <span class="p">)</span> 

<span class="c1"># Split observation indices into folds </span>
<span class="n">list_2</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nobs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
<span class="n">I</span> <span class="o">=</span> <span class="n">split</span><span class="p">(</span><span class="n">list_2</span><span class="p">,</span> <span class="n">foldid</span><span class="p">)</span>

<span class="c1"># Fit a lasso model.</span>
<span class="c1"># Passing foldid argument so we know which observations are in each fold.</span>
<span class="n">lasso_coef_rank</span><span class="o">=</span><span class="p">[]</span>
<span class="n">lasso_pred</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">I</span><span class="p">)):</span>
        <span class="c1"># Split data - index to keep are in mask as booleans</span>
        <span class="n">include_idx</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">I</span><span class="p">[</span><span class="n">b</span><span class="p">])</span>  <span class="c1">#Here should go I[b] Set is more efficient, but doesn&#39;t reorder your elements if that is desireable</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([(</span><span class="n">i</span> <span class="ow">in</span> <span class="n">include_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))])</span>

        <span class="c1"># Lasso regression, excluding folds selected </span>
        <span class="n">lassocv</span> <span class="o">=</span> <span class="n">LassoCV</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">lassocv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scale_X</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">])</span>
        <span class="n">lasso_coef_rank</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lassocv</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
        <span class="n">lasso_pred</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lassocv</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">scale_X</span><span class="p">[</span><span class="n">mask</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">lasso_pred</span>

<span class="n">df_1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]:</span>
    <span class="n">df_2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">y_hat</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    
    <span class="n">b</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">df_2</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">bins</span> <span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">df_2</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">df_2</span><span class="p">,</span><span class="mi">25</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">df_2</span><span class="p">,</span><span class="mi">50</span><span class="p">),</span>
           <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">df_2</span><span class="p">,</span><span class="mi">75</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">df_2</span><span class="p">,</span><span class="mi">100</span><span class="p">)],</span> <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
    
    <span class="n">df_1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_1</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>
<span class="n">df_1</span> <span class="o">=</span> <span class="n">df_1</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">factorize</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">df_1</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span><span class="s1">&#39;ranking&#39;</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df_1</span> <span class="o">=</span> <span class="n">df_1</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;index&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">X</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">df_1</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Ranking observations.</span>
<span class="n">y</span><span class="p">[</span><span class="s1">&#39;ranking&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Estimate expected covariate per subgroup</span>
<span class="n">data_frame</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="k">for</span> <span class="n">var_name</span> <span class="ow">in</span> <span class="n">covariates</span><span class="p">:</span>
    <span class="n">form</span> <span class="o">=</span> <span class="n">var_name</span> <span class="o">+</span> <span class="s2">&quot; ~ &quot;</span> <span class="o">+</span> <span class="s2">&quot;0&quot;</span> <span class="o">+</span> <span class="s2">&quot;+&quot;</span> <span class="o">+</span> <span class="s2">&quot;C(ranking)&quot;</span>
    <span class="n">df1</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="n">form</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">cov_type</span> <span class="o">=</span> <span class="s1">&#39;HC2&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">summary2</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">5</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="c1">#iloc to stay with rankings 0,1,2,3</span>
    <span class="n">df1</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;covariate&#39;</span><span class="p">,</span> <span class="n">var_name</span><span class="p">)</span>
    <span class="n">df1</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s1">&#39;ranking&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;G1&#39;</span><span class="p">,</span><span class="s1">&#39;G2&#39;</span><span class="p">,</span><span class="s1">&#39;G3&#39;</span><span class="p">,</span><span class="s1">&#39;G4&#39;</span><span class="p">])</span>
    <span class="n">df1</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;scaling&#39;</span><span class="p">,</span>
               <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">((</span><span class="n">df1</span><span class="p">[</span><span class="s1">&#39;Coef.&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">df1</span><span class="p">[</span><span class="s1">&#39;Coef.&#39;</span><span class="p">]))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">df1</span><span class="p">[</span><span class="s1">&#39;Coef.&#39;</span><span class="p">]))))</span>
    <span class="n">df1</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;variation&#39;</span><span class="p">,</span>
               <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">df1</span><span class="p">[</span><span class="s1">&#39;Coef.&#39;</span><span class="p">])</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">var_name</span><span class="p">]))</span>
    <span class="n">label</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">):</span>
        <span class="n">label</span> <span class="o">+=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">df1</span><span class="p">[</span><span class="s1">&#39;Coef.&#39;</span><span class="p">][</span><span class="n">j</span><span class="p">],</span><span class="mi">3</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot; (&quot;</span> 
                  <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">df1</span><span class="p">[</span><span class="s1">&#39;Std.Err.&#39;</span><span class="p">][</span><span class="n">j</span><span class="p">],</span><span class="mi">3</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;)&quot;</span><span class="p">]</span>
    <span class="n">df1</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">df1</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;index&#39;</span><span class="p">])</span>
    <span class="n">index</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">):</span>
        <span class="n">index</span> <span class="o">+=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">df1</span><span class="p">[</span><span class="s1">&#39;covariate&#39;</span><span class="p">][</span><span class="n">m</span><span class="p">])</span> <span class="o">+</span> <span class="s2">&quot;_&quot;</span> <span class="o">+</span> <span class="s2">&quot;ranking&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">m</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Index</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
    <span class="n">df1</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
    <span class="n">data_frame</span> <span class="o">=</span> <span class="n">data_frame</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">df1</span><span class="p">)</span>
<span class="n">data_frame</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">labels_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">df_mask</span> <span class="o">=</span> <span class="n">data_frame</span><span class="p">[</span><span class="s1">&#39;ranking&#39;</span><span class="p">]</span><span class="o">==</span><span class="sa">f</span><span class="s2">&quot;G</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">filtered_df</span> <span class="o">=</span> <span class="n">data_frame</span><span class="p">[</span><span class="n">df_mask</span><span class="p">]</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;index&#39;</span><span class="p">])</span>
    <span class="n">labels_data</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;ranking</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">filtered_df</span><span class="p">[[</span><span class="s1">&#39;labels&#39;</span><span class="p">]]</span>
<span class="n">labels_data</span> <span class="o">=</span> <span class="n">labels_data</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Index</span><span class="p">(</span><span class="n">covariates</span><span class="p">))</span>
<span class="n">labels_data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ranking1</th>
      <th>ranking2</th>
      <th>ranking3</th>
      <th>ranking4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>LOT</th>
      <td>50086.346 (1463.95)</td>
      <td>48126.298 (1442.051)</td>
      <td>46548.555 (1398.259)</td>
      <td>46837.464 (1380.26)</td>
    </tr>
    <tr>
      <th>UNITSF</th>
      <td>2434.736 (24.766)</td>
      <td>2420.282 (24.145)</td>
      <td>2424.834 (24.308)</td>
      <td>2440.076 (25.71)</td>
    </tr>
    <tr>
      <th>BUILT</th>
      <td>1972.797 (0.299)</td>
      <td>1974.743 (0.293)</td>
      <td>1973.712 (0.299)</td>
      <td>1972.645 (0.301)</td>
    </tr>
    <tr>
      <th>BATHS</th>
      <td>1.943 (0.009)</td>
      <td>1.964 (0.009)</td>
      <td>1.948 (0.009)</td>
      <td>1.912 (0.009)</td>
    </tr>
    <tr>
      <th>BEDRMS</th>
      <td>3.252 (0.01)</td>
      <td>3.241 (0.01)</td>
      <td>3.246 (0.01)</td>
      <td>3.23 (0.01)</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>noise16</th>
      <td>0.505 (0.003)</td>
      <td>0.503 (0.003)</td>
      <td>0.5 (0.003)</td>
      <td>0.498 (0.003)</td>
    </tr>
    <tr>
      <th>noise17</th>
      <td>0.504 (0.003)</td>
      <td>0.497 (0.003)</td>
      <td>0.499 (0.003)</td>
      <td>0.502 (0.003)</td>
    </tr>
    <tr>
      <th>noise18</th>
      <td>0.504 (0.003)</td>
      <td>0.498 (0.003)</td>
      <td>0.501 (0.003)</td>
      <td>0.5 (0.003)</td>
    </tr>
    <tr>
      <th>noise19</th>
      <td>0.499 (0.003)</td>
      <td>0.504 (0.003)</td>
      <td>0.504 (0.003)</td>
      <td>0.5 (0.003)</td>
    </tr>
    <tr>
      <th>noise20</th>
      <td>0.506 (0.003)</td>
      <td>0.496 (0.003)</td>
      <td>0.502 (0.003)</td>
      <td>0.499 (0.003)</td>
    </tr>
  </tbody>
</table>
<p>63 rows × 4 columns</p>
</div></div></div>
</div>
<p>The next heatmap visualizes the results. Note how observations ranked higher (i.e., were predicted to have higher prices) have more bedrooms and baths, were built more recently, have fewer cracks, and so on. The next snippet of code displays the average covariate per group along with each standard errors. The rows are ordered according to <span class="math notranslate nohighlight">\(Var(E[X_{ij} | G_i) / Var(X_i)\)</span>, where <span class="math notranslate nohighlight">\(G_i\)</span> denotes the ranking. This is a rough normalized measure of how much variation is “explained” by group membership <span class="math notranslate nohighlight">\(G_i\)</span>. Brighter colors indicate larger values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">new_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">df_mask</span> <span class="o">=</span> <span class="n">data_frame</span><span class="p">[</span><span class="s1">&#39;ranking&#39;</span><span class="p">]</span><span class="o">==</span><span class="sa">f</span><span class="s2">&quot;G</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">filtered_df</span> <span class="o">=</span> <span class="n">data_frame</span><span class="p">[</span><span class="n">df_mask</span><span class="p">]</span>
    <span class="n">new_data</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="sa">f</span><span class="s2">&quot;G</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">filtered_df</span><span class="p">[[</span><span class="s1">&#39;scaling&#39;</span><span class="p">]])</span>
<span class="n">new_data</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot heatmap</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">covariates</span>
<span class="n">ranks</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;G1&#39;</span><span class="p">,</span><span class="s1">&#39;G2&#39;</span><span class="p">,</span><span class="s1">&#39;G3&#39;</span><span class="p">,</span><span class="s1">&#39;G4&#39;</span><span class="p">]</span>
<span class="n">harvest</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">new_data</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">labels_hm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">labels_data</span><span class="p">))</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>

<span class="c1"># getting the original colormap using cm.get_cmap() function</span>
<span class="n">orig_map</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;copper&#39;</span><span class="p">)</span>
  
<span class="c1"># reversing the original colormap using reversed() function</span>
<span class="n">reversed_map</span> <span class="o">=</span> <span class="n">orig_map</span><span class="o">.</span><span class="n">reversed</span><span class="p">()</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">harvest</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">reversed_map</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>

<span class="c1"># make bar</span>
<span class="n">bar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
  
<span class="c1"># show plot with labels</span>
<span class="n">bar</span><span class="o">.</span><span class="n">set_label</span><span class="p">(</span><span class="s1">&#39;scaling&#39;</span><span class="p">)</span>

<span class="c1"># Setting the labels</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ranks</span><span class="p">)))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)))</span>
<span class="c1"># labeling respective list entries</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">ranks</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

<span class="c1"># Rotate the tick labels and set their alignment.</span>
<span class="n">plt</span><span class="o">.</span><span class="n">setp</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">get_xticklabels</span><span class="p">(),</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;right&quot;</span><span class="p">,</span>
         <span class="n">rotation_mode</span><span class="o">=</span><span class="s2">&quot;anchor&quot;</span><span class="p">)</span>

<span class="c1"># Creating text annotations by using for loop</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ranks</span><span class="p">)):</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">labels_hm</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span>
                       <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;w&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Average covariate values within group (based on prediction ranking)&quot;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2_introduction_to_Machine_Learning_65_0.png" src="../_images/2_introduction_to_Machine_Learning_65_0.png" />
</div>
</div>
<p>As we just saw above, houses that have, e.g., been built more recently (<code class="docutils literal notranslate"><span class="pre">BUILT</span></code>), have more baths (<code class="docutils literal notranslate"><span class="pre">BATHS</span></code>) are associated with larger price predictions.</p>
<p>This sort of interpretation exercise did not rely on reading any coefficients, and in fact it could also be done using any other flexible method, including decisions trees and forests.</p>
</div>
<div class="section" id="decision-tree">
<h3><span class="section-number">2.2.2. </span>Decision Tree<a class="headerlink" href="#decision-tree" title="Permalink to this headline">¶</a></h3>
<p>This next class of algorithms divides the covariate space into “regions” and estimates a constant prediction within each region.</p>
<p>To estimate a decision tree, we following a recursive partition algorithm. At each stage, we select one variable <span class="math notranslate nohighlight">\(j\)</span> and one split point
<span class="math notranslate nohighlight">\(s\)</span>, and divide the observations into “left” and “right” subsets, depending on whether <span class="math notranslate nohighlight">\(X_{ij} \leq s\)</span> or <span class="math notranslate nohighlight">\(X_{ij} &gt; s\)</span>. For regression problems, the variable and split points are often selected so that the sum of the variances of the outcome variable in each “child” subset is smallest. For classification problems, we split to separate the classes. Then, for each child, we separately repeat the process of finding variables and split points. This continues until a minimum subset size is reached, or improvement falls below some threshold.</p>
<p>At prediction time, to find the predictions for some point <span class="math notranslate nohighlight">\(x\)</span>, we just follow the tree we just built, going left or right according to the selected variables and split points, until we reach a terminal node. Then, for regression problems, the predicted value at some point <span class="math notranslate nohighlight">\(x\)</span> is the average outcome of the observations in the same partition as the point <span class="math notranslate nohighlight">\(x\)</span>. For classification problems, we output the majority class in the node.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">import</span> <span class="nn">graphviz</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">export_graphviz</span> 
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">Series</span>
<span class="kn">from</span> <span class="nn">simple_colors</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="o">~</span>\<span class="n">AppData</span>\<span class="n">Local</span>\<span class="n">Temp</span><span class="o">/</span><span class="n">ipykernel_14752</span><span class="o">/</span><span class="mf">2063672304.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">Series</span>
<span class="ne">----&gt; </span><span class="mi">7</span> <span class="kn">from</span> <span class="nn">simple_colors</span> <span class="kn">import</span> <span class="o">*</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span> <span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="g g-Whitespace">      </span><span class="mi">9</span> <span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;simple_colors&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit tree without pruning first</span>
<span class="n">XX</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">covariates</span><span class="p">]</span>
<span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">ccp_alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span> <span class="mi">15</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">XX</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">(),</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">.3</span><span class="p">)</span>
<span class="n">tree1</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>At this point, we have not constrained the complexity of the tree in any way, so it’s likely too deep and probably overfits. Here’s a plot of what we have so far (without bothering to label the splits to avoid clutter).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">tree</span><span class="o">.</span><span class="n">plot_tree</span><span class="p">(</span><span class="n">dt</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Text(0.6649812429236953, 0.96875, &#39;X[22] &lt;= 3.5\nsquared_error = 0.953\nsamples = 20108\nvalue = 11.817&#39;),
 Text(0.41149011719399203, 0.90625, &#39;X[1] &lt;= 2436.5\nsquared_error = 0.765\nsamples = 19394\nvalue = 11.888&#39;),
 Text(0.1962294981926209, 0.84375, &#39;X[3] &lt;= 1.5\nsquared_error = 0.641\nsamples = 13894\nvalue = 11.685&#39;),
 Text(0.08274342487950806, 0.78125, &#39;X[19] &lt;= 1.5\nsquared_error = 0.692\nsamples = 5053\nvalue = 11.39&#39;),
 Text(0.032513347598471, 0.71875, &#39;X[54] &lt;= 0.001\nsquared_error = 0.585\nsamples = 2640\nvalue = 11.546&#39;),
 Text(0.032180956041216555, 0.65625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 0.0&#39;),
 Text(0.032845739155725445, 0.65625, &#39;X[34] &lt;= 2.154\nsquared_error = 0.534\nsamples = 2639\nvalue = 11.55&#39;),
 Text(0.004731386072793751, 0.59375, &#39;X[53] &lt;= 0.008\nsquared_error = 1.991\nsamples = 164\nvalue = 11.158&#39;),
 Text(0.0006647831145088915, 0.53125, &#39;X[38] &lt;= 1.5\nsquared_error = 35.102\nsamples = 2\nvalue = 5.925&#39;),
 Text(0.00033239155725444574, 0.46875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 0.0&#39;),
 Text(0.0009971746717633372, 0.46875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.849&#39;),
 Text(0.008797989031078611, 0.53125, &#39;X[49] &lt;= 0.109\nsquared_error = 1.24\nsamples = 162\nvalue = 11.223&#39;),
 Text(0.0016619577862722287, 0.46875, &#39;X[58] &lt;= 0.159\nsquared_error = 7.439\nsamples = 16\nvalue = 9.978&#39;),
 Text(0.001329566229017783, 0.40625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 0.0&#39;),
 Text(0.0019943493435266744, 0.40625, &#39;X[57] &lt;= 0.284\nsquared_error = 0.856\nsamples = 15\nvalue = 10.643&#39;),
 Text(0.0006647831145088915, 0.34375, &#39;X[9] &lt;= 1.5\nsquared_error = 0.159\nsamples = 2\nvalue = 8.811&#39;),
 Text(0.00033239155725444574, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.21&#39;),
 Text(0.0009971746717633372, 0.28125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 8.412&#39;),
 Text(0.0033239155725444574, 0.34375, &#39;X[52] &lt;= 0.229\nsquared_error = 0.367\nsamples = 13\nvalue = 10.925&#39;),
 Text(0.0016619577862722287, 0.28125, &#39;X[62] &lt;= 0.531\nsquared_error = 0.116\nsamples = 4\nvalue = 10.244&#39;),
 Text(0.001329566229017783, 0.21875, &#39;squared_error = 0.0\nsamples = 2\nvalue = 9.903&#39;),
 Text(0.0019943493435266744, 0.21875, &#39;X[52] &lt;= 0.109\nsquared_error = 0.0\nsamples = 2\nvalue = 10.584&#39;),
 Text(0.0016619577862722287, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.597&#39;),
 Text(0.00232674090078112, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.571&#39;),
 Text(0.0049858733588166865, 0.28125, &#39;X[58] &lt;= 0.621\nsquared_error = 0.181\nsamples = 9\nvalue = 11.227&#39;),
 Text(0.003656307129798903, 0.21875, &#39;X[5] &lt;= 0.5\nsquared_error = 0.045\nsamples = 5\nvalue = 11.499&#39;),
 Text(0.0029915240152900116, 0.15625, &#39;X[2] &lt;= 1987.5\nsquared_error = 0.009\nsamples = 3\nvalue = 11.648&#39;),
 Text(0.002659132458035566, 0.09375, &#39;X[29] &lt;= 0.5\nsquared_error = 0.0\nsamples = 2\nvalue = 11.716&#39;),
 Text(0.00232674090078112, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.695&#39;),
 Text(0.0029915240152900116, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.736&#39;),
 Text(0.0033239155725444574, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.513&#39;),
 Text(0.004321090244307795, 0.15625, &#39;X[49] &lt;= 0.044\nsquared_error = 0.014\nsamples = 2\nvalue = 11.276&#39;),
 Text(0.003988698687053349, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.396&#39;),
 Text(0.00465348180156224, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.156&#39;),
 Text(0.006315439587834469, 0.21875, &#39;X[55] &lt;= 0.655\nsquared_error = 0.143\nsamples = 4\nvalue = 10.887&#39;),
 Text(0.005650656473325578, 0.15625, &#39;X[51] &lt;= 0.511\nsquared_error = 0.004\nsamples = 2\nvalue = 10.53&#39;),
 Text(0.005318264916071132, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.463&#39;),
 Text(0.005983048030580023, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.597&#39;),
 Text(0.006980222702343361, 0.15625, &#39;X[60] &lt;= 0.468\nsquared_error = 0.026\nsamples = 2\nvalue = 11.245&#39;),
 Text(0.006647831145088915, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.082&#39;),
 Text(0.007312614259597806, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.408&#39;),
 Text(0.015934020275884992, 0.46875, &#39;X[49] &lt;= 0.506\nsquared_error = 0.372\nsamples = 146\nvalue = 11.359&#39;),
 Text(0.011592155559248795, 0.40625, &#39;X[62] &lt;= 0.297\nsquared_error = 0.457\nsamples = 54\nvalue = 11.149&#39;),
 Text(0.008891474156556424, 0.34375, &#39;X[46] &lt;= 0.257\nsquared_error = 0.181\nsamples = 10\nvalue = 11.81&#39;),
 Text(0.007977397374106698, 0.28125, &#39;X[34] &lt;= 1.5\nsquared_error = 0.042\nsamples = 2\nvalue = 11.174&#39;),
 Text(0.007645005816852252, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.968&#39;),
 Text(0.008309788931361143, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.379&#39;),
 Text(0.009805550939006149, 0.28125, &#39;X[1] &lt;= 1525.0\nsquared_error = 0.09\nsamples = 8\nvalue = 11.97&#39;),
 Text(0.008974572045870035, 0.21875, &#39;X[45] &lt;= 0.765\nsquared_error = 0.036\nsamples = 6\nvalue = 12.11&#39;),
 Text(0.008309788931361143, 0.15625, &#39;X[1] &lt;= 1175.0\nsquared_error = 0.013\nsamples = 3\nvalue = 12.28&#39;),
 Text(0.007977397374106698, 0.09375, &#39;X[54] &lt;= 0.243\nsquared_error = 0.003\nsamples = 2\nvalue = 12.205&#39;),
 Text(0.007645005816852252, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.255&#39;),
 Text(0.008309788931361143, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.155&#39;),
 Text(0.00864218048861559, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 12.429&#39;),
 Text(0.009639355160378926, 0.15625, &#39;X[54] &lt;= 0.637\nsquared_error = 0.002\nsamples = 3\nvalue = 11.94&#39;),
 Text(0.00930696360312448, 0.09375, &#39;X[4] &lt;= 1.5\nsquared_error = 0.0\nsamples = 2\nvalue = 11.967&#39;),
 Text(0.008974572045870035, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.983&#39;),
 Text(0.009639355160378926, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.951&#39;),
 Text(0.009971746717633373, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.884&#39;),
 Text(0.010636529832142264, 0.21875, &#39;X[60] &lt;= 0.523\nsquared_error = 0.014\nsamples = 2\nvalue = 11.55&#39;),
 Text(0.010304138274887818, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.43&#39;),
 Text(0.010968921389396709, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.67&#39;),
 Text(0.014292836961941167, 0.34375, &#39;X[43] &lt;= 0.031\nsquared_error = 0.398\nsamples = 44\nvalue = 10.999&#39;),
 Text(0.013960445404686722, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 8.923&#39;),
 Text(0.014625228519195612, 0.28125, &#39;X[2] &lt;= 1987.5\nsquared_error = 0.305\nsamples = 43\nvalue = 11.048&#39;),
 Text(0.012963270732923384, 0.21875, &#39;X[53] &lt;= 0.538\nsquared_error = 0.266\nsamples = 39\nvalue = 10.97&#39;),
 Text(0.011633704503905601, 0.15625, &#39;X[52] &lt;= 0.264\nsquared_error = 0.196\nsamples = 19\nvalue = 10.728&#39;),
 Text(0.010968921389396709, 0.09375, &#39;X[54] &lt;= 0.598\nsquared_error = 0.081\nsamples = 5\nvalue = 11.179&#39;),
 Text(0.010636529832142264, 0.03125, &#39;squared_error = 0.034\nsamples = 4\nvalue = 11.295&#39;),
 Text(0.011301312946651156, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.714&#39;),
 Text(0.012298487618414492, 0.09375, &#39;X[46] &lt;= 0.946\nsquared_error = 0.138\nsamples = 14\nvalue = 10.567&#39;),
 Text(0.011966096061160046, 0.03125, &#39;squared_error = 0.083\nsamples = 11\nvalue = 10.427&#39;),
 Text(0.012630879175668939, 0.03125, &#39;squared_error = 0.004\nsamples = 3\nvalue = 11.08&#39;),
 Text(0.014292836961941167, 0.15625, &#39;X[58] &lt;= 0.343\nsquared_error = 0.224\nsamples = 20\nvalue = 11.2&#39;),
 Text(0.013628053847432275, 0.09375, &#39;X[54] &lt;= 0.983\nsquared_error = 0.063\nsamples = 5\nvalue = 11.752&#39;),
 Text(0.01329566229017783, 0.03125, &#39;squared_error = 0.012\nsamples = 4\nvalue = 11.868&#39;),
 Text(0.013960445404686722, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.29&#39;),
 Text(0.014957620076450058, 0.09375, &#39;X[41] &lt;= 0.5\nsquared_error = 0.142\nsamples = 15\nvalue = 11.016&#39;),
 Text(0.014625228519195612, 0.03125, &#39;squared_error = 0.082\nsamples = 12\nvalue = 10.883&#39;),
 Text(0.015290011633704505, 0.03125, &#39;squared_error = 0.031\nsamples = 3\nvalue = 11.546&#39;),
 Text(0.01628718630546784, 0.21875, &#39;X[47] &lt;= 0.564\nsquared_error = 0.05\nsamples = 4\nvalue = 11.805&#39;),
 Text(0.015954794748213395, 0.15625, &#39;X[62] &lt;= 0.659\nsquared_error = 0.013\nsamples = 3\nvalue = 11.689&#39;),
 Text(0.01562240319095895, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.849&#39;),
 Text(0.01628718630546784, 0.09375, &#39;squared_error = -0.0\nsamples = 2\nvalue = 11.608&#39;),
 Text(0.016619577862722286, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.155&#39;),
 Text(0.02027588499252119, 0.40625, &#39;X[56] &lt;= 0.087\nsquared_error = 0.281\nsamples = 92\nvalue = 11.483&#39;),
 Text(0.01794914409174007, 0.34375, &#39;X[54] &lt;= 0.387\nsquared_error = 0.184\nsamples = 6\nvalue = 12.157&#39;),
 Text(0.01728436097723118, 0.28125, &#39;X[34] &lt;= 0.5\nsquared_error = 0.025\nsamples = 3\nvalue = 12.541&#39;),
 Text(0.01695196941997673, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.766&#39;),
 Text(0.017616752534485625, 0.21875, &#39;squared_error = 0.0\nsamples = 2\nvalue = 12.429&#39;),
 Text(0.01861392720624896, 0.28125, &#39;X[48] &lt;= 0.793\nsquared_error = 0.047\nsamples = 3\nvalue = 11.772&#39;),
 Text(0.018281535648994516, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.044&#39;),
 Text(0.018946318763503407, 0.21875, &#39;X[60] &lt;= 0.592\nsquared_error = 0.015\nsamples = 2\nvalue = 11.636&#39;),
 Text(0.01861392720624896, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.513&#39;),
 Text(0.019278710320757852, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.76&#39;),
 Text(0.022602625893302312, 0.34375, &#39;X[46] &lt;= 0.014\nsquared_error = 0.254\nsamples = 86\nvalue = 11.436&#39;),
 Text(0.022270234336047863, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.999&#39;),
 Text(0.022935017450556757, 0.28125, &#39;X[47] &lt;= 0.941\nsquared_error = 0.232\nsamples = 85\nvalue = 11.453&#39;),
 Text(0.021273059664284527, 0.21875, &#39;X[46] &lt;= 0.054\nsquared_error = 0.212\nsamples = 82\nvalue = 11.481&#39;),
 Text(0.019943493435266746, 0.15625, &#39;X[54] &lt;= 0.707\nsquared_error = 0.146\nsamples = 4\nvalue = 12.15&#39;),
 Text(0.019278710320757852, 0.09375, &#39;X[57] &lt;= 0.608\nsquared_error = 0.035\nsamples = 2\nvalue = 11.796&#39;),
 Text(0.018946318763503407, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.608&#39;),
 Text(0.019611101878012297, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.983&#39;),
 Text(0.020608276549775636, 0.09375, &#39;X[50] &lt;= 0.304\nsquared_error = 0.006\nsamples = 2\nvalue = 12.503&#39;),
 Text(0.02027588499252119, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.429&#39;),
 Text(0.020940668107030082, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.578&#39;),
 Text(0.022602625893302312, 0.15625, &#39;X[29] &lt;= 1.5\nsquared_error = 0.191\nsamples = 78\nvalue = 11.447&#39;),
 Text(0.021937842778793418, 0.09375, &#39;X[58] &lt;= 0.048\nsquared_error = 0.171\nsamples = 74\nvalue = 11.414&#39;),
 Text(0.021605451221538972, 0.03125, &#39;squared_error = 0.011\nsamples = 2\nvalue = 10.493&#39;),
 Text(0.022270234336047863, 0.03125, &#39;squared_error = 0.151\nsamples = 72\nvalue = 11.439&#39;),
 Text(0.023267409007811202, 0.09375, &#39;X[58] &lt;= 0.357\nsquared_error = 0.157\nsamples = 4\nvalue = 12.065&#39;),
 Text(0.022935017450556757, 0.03125, &#39;squared_error = 0.001\nsamples = 2\nvalue = 12.458&#39;),
 Text(0.023599800565065648, 0.03125, &#39;squared_error = 0.004\nsamples = 2\nvalue = 11.672&#39;),
 Text(0.024596975236828984, 0.21875, &#39;X[7] &lt;= 1.5\nsquared_error = 0.16\nsamples = 3\nvalue = 10.666&#39;),
 Text(0.02426458367957454, 0.15625, &#39;X[47] &lt;= 0.948\nsquared_error = 0.006\nsamples = 2\nvalue = 10.386&#39;),
 Text(0.023932192122320093, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.309&#39;),
 Text(0.024596975236828984, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.463&#39;),
 Text(0.02492936679408343, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.225&#39;),
 Text(0.060960092238657136, 0.59375, &#39;X[8] &lt;= 3.5\nsquared_error = 0.427\nsamples = 2475\nvalue = 11.576&#39;),
 Text(0.04463914741565564, 0.53125, &#39;X[28] &lt;= 0.5\nsquared_error = 0.414\nsamples = 2258\nvalue = 11.548&#39;),
 Text(0.031556423466843946, 0.46875, &#39;X[46] &lt;= 0.008\nsquared_error = 0.452\nsamples = 1871\nvalue = 11.51&#39;),
 Text(0.0254279541299651, 0.40625, &#39;X[50] &lt;= 0.205\nsquared_error = 9.283\nsamples = 7\nvalue = 9.735&#39;),
 Text(0.024596975236828984, 0.34375, &#39;X[61] &lt;= 0.388\nsquared_error = 0.022\nsamples = 2\nvalue = 4.927&#39;),
 Text(0.02426458367957454, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 4.779&#39;),
 Text(0.02492936679408343, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 5.075&#39;),
 Text(0.026258933023101214, 0.34375, &#39;X[60] &lt;= 0.731\nsquared_error = 0.041\nsamples = 5\nvalue = 11.659&#39;),
 Text(0.025594149908592323, 0.28125, &#39;X[49] &lt;= 0.499\nsquared_error = 0.007\nsamples = 2\nvalue = 11.432&#39;),
 Text(0.025261758351337878, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.35&#39;),
 Text(0.02592654146584677, 0.21875, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.513&#39;),
 Text(0.026923716137610104, 0.28125, &#39;X[8] &lt;= 2.5\nsquared_error = 0.006\nsamples = 3\nvalue = 11.81&#39;),
 Text(0.02659132458035566, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.918&#39;),
 Text(0.02725610769486455, 0.21875, &#39;X[44] &lt;= 0.704\nsquared_error = 0.0\nsamples = 2\nvalue = 11.756&#39;),
 Text(0.026923716137610104, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.775&#39;),
 Text(0.027588499252118995, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.736&#39;),
 Text(0.03768489280372279, 0.40625, &#39;X[1] &lt;= 2415.0\nsquared_error = 0.407\nsamples = 1864\nvalue = 11.516&#39;),
 Text(0.03357154728269902, 0.34375, &#39;X[1] &lt;= 1277.0\nsquared_error = 0.404\nsamples = 1608\nvalue = 11.55&#39;),
 Text(0.03066312115672262, 0.28125, &#39;X[58] &lt;= 0.956\nsquared_error = 0.315\nsamples = 709\nvalue = 11.446&#39;),
 Text(0.029084261259764002, 0.21875, &#39;X[1] &lt;= 1073.0\nsquared_error = 0.262\nsamples = 668\nvalue = 11.467&#39;),
 Text(0.02825328236662789, 0.15625, &#39;X[55] &lt;= 0.019\nsquared_error = 0.314\nsamples = 308\nvalue = 11.376&#39;),
 Text(0.027588499252118995, 0.09375, &#39;X[2] &lt;= 1972.5\nsquared_error = 3.618\nsamples = 11\nvalue = 10.657&#39;),
 Text(0.02725610769486455, 0.03125, &#39;squared_error = 0.293\nsamples = 10\nvalue = 11.236&#39;),
 Text(0.027920890809373444, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 4.868&#39;),
 Text(0.02891806548113678, 0.09375, &#39;X[42] &lt;= -2.5\nsquared_error = 0.171\nsamples = 297\nvalue = 11.403&#39;),
 Text(0.028585673923882334, 0.03125, &#39;squared_error = 0.166\nsamples = 50\nvalue = 11.159&#39;),
 Text(0.029250457038391225, 0.03125, &#39;squared_error = 0.158\nsamples = 247\nvalue = 11.452&#39;),
 Text(0.029915240152900115, 0.15625, &#39;X[47] &lt;= 0.006\nsquared_error = 0.205\nsamples = 360\nvalue = 11.544&#39;),
 Text(0.02958284859564567, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 8.517&#39;),
 Text(0.03024763171015456, 0.09375, &#39;X[45] &lt;= 0.015\nsquared_error = 0.18\nsamples = 359\nvalue = 11.553&#39;),
 Text(0.029915240152900115, 0.03125, &#39;squared_error = 0.181\nsamples = 3\nvalue = 12.551&#39;),
 Text(0.03058002326740901, 0.03125, &#39;squared_error = 0.171\nsamples = 356\nvalue = 11.544&#39;),
 Text(0.032241981053681236, 0.21875, &#39;X[57] &lt;= 0.927\nsquared_error = 1.054\nsamples = 41\nvalue = 11.102&#39;),
 Text(0.03190958949642679, 0.15625, &#39;X[54] &lt;= 0.964\nsquared_error = 0.149\nsamples = 40\nvalue = 11.253&#39;),
 Text(0.031577197939172345, 0.09375, &#39;X[6] &lt;= 1.5\nsquared_error = 0.109\nsamples = 39\nvalue = 11.286&#39;),
 Text(0.0312448063819179, 0.03125, &#39;squared_error = 0.099\nsamples = 10\nvalue = 10.998&#39;),
 Text(0.03190958949642679, 0.03125, &#39;squared_error = 0.073\nsamples = 29\nvalue = 11.386&#39;),
 Text(0.032241981053681236, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 9.952&#39;),
 Text(0.03257437261093568, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 5.075&#39;),
 Text(0.03647997340867542, 0.28125, &#39;X[45] &lt;= 0.989\nsquared_error = 0.459\nsamples = 899\nvalue = 11.632&#39;),
 Text(0.034402526175835134, 0.21875, &#39;X[44] &lt;= 0.043\nsquared_error = 0.349\nsamples = 892\nvalue = 11.644&#39;),
 Text(0.03323915572544457, 0.15625, &#39;X[56] &lt;= 0.037\nsquared_error = 2.439\nsamples = 37\nvalue = 11.191&#39;),
 Text(0.03290676416819013, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 4.754&#39;),
 Text(0.03357154728269902, 0.09375, &#39;X[58] &lt;= 0.037\nsquared_error = 1.323\nsamples = 36\nvalue = 11.37&#39;),
 Text(0.03323915572544457, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 5.298&#39;),
 Text(0.03390393883995346, 0.03125, &#39;squared_error = 0.278\nsamples = 35\nvalue = 11.543&#39;),
 Text(0.035565896626225696, 0.15625, &#39;X[29] &lt;= 1.5\nsquared_error = 0.249\nsamples = 855\nvalue = 11.663&#39;),
 Text(0.034901113511716805, 0.09375, &#39;X[46] &lt;= 0.999\nsquared_error = 0.236\nsamples = 825\nvalue = 11.646&#39;),
 Text(0.03456872195446236, 0.03125, &#39;squared_error = 0.229\nsamples = 824\nvalue = 11.649&#39;),
 Text(0.03523350506897125, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 9.21&#39;),
 Text(0.036230679740734587, 0.09375, &#39;X[33] &lt;= 0.5\nsquared_error = 0.37\nsamples = 30\nvalue = 12.144&#39;),
 Text(0.03589828818348014, 0.03125, &#39;squared_error = 0.23\nsamples = 28\nvalue = 12.041&#39;),
 Text(0.03656307129798903, 0.03125, &#39;squared_error = 0.08\nsamples = 2\nvalue = 13.588&#39;),
 Text(0.038557420641515704, 0.21875, &#39;X[50] &lt;= 0.875\nsquared_error = 12.499\nsamples = 7\nvalue = 10.199&#39;),
 Text(0.03822502908426126, 0.15625, &#39;X[46] &lt;= 0.747\nsquared_error = 0.235\nsamples = 6\nvalue = 11.631&#39;),
 Text(0.03756024596975237, 0.09375, &#39;X[46] &lt;= 0.588\nsquared_error = 0.067\nsamples = 3\nvalue = 12.052&#39;),
 Text(0.03722785441249792, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.704&#39;),
 Text(0.03789263752700681, 0.03125, &#39;squared_error = 0.01\nsamples = 2\nvalue = 12.226&#39;),
 Text(0.03888981219877015, 0.09375, &#39;X[60] &lt;= 0.319\nsquared_error = 0.049\nsamples = 3\nvalue = 11.21&#39;),
 Text(0.038557420641515704, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.915&#39;),
 Text(0.039222203756024594, 0.03125, &#39;squared_error = 0.009\nsamples = 2\nvalue = 11.358&#39;),
 Text(0.03888981219877015, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 1.609&#39;),
 Text(0.04179823832474655, 0.34375, &#39;X[44] &lt;= 0.096\nsquared_error = 0.373\nsamples = 256\nvalue = 11.305&#39;),
 Text(0.04055176998504238, 0.28125, &#39;X[49] &lt;= 0.857\nsquared_error = 0.819\nsamples = 22\nvalue = 10.857&#39;),
 Text(0.04021937842778794, 0.21875, &#39;X[45] &lt;= 0.053\nsquared_error = 0.585\nsamples = 21\nvalue = 10.969&#39;),
 Text(0.03988698687053349, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 8.923&#39;),
 Text(0.04055176998504238, 0.15625, &#39;X[53] &lt;= 0.897\nsquared_error = 0.395\nsamples = 20\nvalue = 11.071&#39;),
 Text(0.04021937842778794, 0.09375, &#39;X[49] &lt;= 0.119\nsquared_error = 0.243\nsamples = 19\nvalue = 11.164&#39;),
 Text(0.03988698687053349, 0.03125, &#39;squared_error = 0.066\nsamples = 4\nvalue = 10.55&#39;),
 Text(0.04055176998504238, 0.03125, &#39;squared_error = 0.163\nsamples = 15\nvalue = 11.328&#39;),
 Text(0.04088416154229683, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.306&#39;),
 Text(0.04088416154229683, 0.21875, &#39;squared_error = -0.0\nsamples = 1\nvalue = 8.517&#39;),
 Text(0.043044706664450726, 0.28125, &#39;X[56] &lt;= 0.003\nsquared_error = 0.31\nsamples = 234\nvalue = 11.347&#39;),
 Text(0.04271231510719628, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.21&#39;),
 Text(0.04337709822170517, 0.21875, &#39;X[0] &lt;= 94000.0\nsquared_error = 0.292\nsamples = 233\nvalue = 11.356&#39;),
 Text(0.04221372777131461, 0.15625, &#39;X[59] &lt;= 0.535\nsquared_error = 0.278\nsamples = 218\nvalue = 11.323&#39;),
 Text(0.04154894465680572, 0.09375, &#39;X[50] &lt;= 0.745\nsquared_error = 0.232\nsamples = 114\nvalue = 11.425&#39;),
 Text(0.04121655309955127, 0.03125, &#39;squared_error = 0.194\nsamples = 87\nvalue = 11.518&#39;),
 Text(0.041881336214060164, 0.03125, &#39;squared_error = 0.237\nsamples = 27\nvalue = 11.126&#39;),
 Text(0.0428785108858235, 0.09375, &#39;X[55] &lt;= 0.989\nsquared_error = 0.304\nsamples = 104\nvalue = 11.211&#39;),
 Text(0.042546119328569054, 0.03125, &#39;squared_error = 0.282\nsamples = 103\nvalue = 11.226&#39;),
 Text(0.043210902443077945, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 9.616&#39;),
 Text(0.044540468672095726, 0.15625, &#39;X[48] &lt;= 0.939\nsquared_error = 0.246\nsamples = 15\nvalue = 11.834&#39;),
 Text(0.04420807711484128, 0.09375, &#39;X[57] &lt;= 0.38\nsquared_error = 0.137\nsamples = 14\nvalue = 11.742&#39;),
 Text(0.043875685557586835, 0.03125, &#39;squared_error = 0.028\nsamples = 2\nvalue = 10.988&#39;),
 Text(0.044540468672095726, 0.03125, &#39;squared_error = 0.044\nsamples = 12\nvalue = 11.868&#39;),
 Text(0.04487286022935017, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 13.122&#39;),
 Text(0.05772187136446734, 0.46875, &#39;X[1] &lt;= 1504.5\nsquared_error = 0.185\nsamples = 387\nvalue = 11.735&#39;),
 Text(0.05287103207578527, 0.40625, &#39;X[50] &lt;= 0.989\nsquared_error = 0.209\nsamples = 135\nvalue = 11.623&#39;),
 Text(0.050897457204587, 0.34375, &#39;X[56] &lt;= 0.129\nsquared_error = 0.173\nsamples = 132\nvalue = 11.602&#39;),
 Text(0.0486953631377763, 0.28125, &#39;X[58] &lt;= 0.958\nsquared_error = 0.44\nsamples = 19\nvalue = 11.289&#39;),
 Text(0.04753199268738574, 0.21875, &#39;X[61] &lt;= 0.555\nsquared_error = 0.116\nsamples = 17\nvalue = 11.488&#39;),
 Text(0.04620242645836796, 0.15625, &#39;X[0] &lt;= 40284.602\nsquared_error = 0.03\nsamples = 9\nvalue = 11.285&#39;),
 Text(0.04553764334385907, 0.09375, &#39;X[43] &lt;= 0.386\nsquared_error = 0.012\nsamples = 7\nvalue = 11.356&#39;),
 Text(0.045205251786604624, 0.03125, &#39;squared_error = 0.002\nsamples = 3\nvalue = 11.457&#39;),
 Text(0.045870034901113514, 0.03125, &#39;squared_error = 0.006\nsamples = 4\nvalue = 11.281&#39;),
 Text(0.04686720957287685, 0.09375, &#39;X[61] &lt;= 0.464\nsquared_error = 0.015\nsamples = 2\nvalue = 11.036&#39;),
 Text(0.046534818015622405, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.156&#39;),
 Text(0.047199601130131295, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.915&#39;),
 Text(0.04886155891640352, 0.15625, &#39;X[48] &lt;= 0.221\nsquared_error = 0.115\nsamples = 8\nvalue = 11.716&#39;),
 Text(0.04819677580189463, 0.09375, &#39;X[45] &lt;= 0.294\nsquared_error = 0.005\nsamples = 2\nvalue = 11.222&#39;),
 Text(0.047864384244640186, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.29&#39;),
 Text(0.04852916735914908, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.155&#39;),
 Text(0.04952634203091241, 0.09375, &#39;X[6] &lt;= 4.5\nsquared_error = 0.043\nsamples = 6\nvalue = 11.881&#39;),
 Text(0.04919395047365797, 0.03125, &#39;squared_error = 0.01\nsamples = 3\nvalue = 12.049&#39;),
 Text(0.04985873358816686, 0.03125, &#39;squared_error = 0.021\nsamples = 3\nvalue = 11.713&#39;),
 Text(0.04985873358816686, 0.21875, &#39;X[41] &lt;= 0.5\nsquared_error = 0.0\nsamples = 2\nvalue = 9.599&#39;),
 Text(0.04952634203091241, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.616&#39;),
 Text(0.0501911251454213, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.582&#39;),
 Text(0.053099551271397705, 0.28125, &#39;X[27] &lt;= 0.5\nsquared_error = 0.109\nsamples = 113\nvalue = 11.654&#39;),
 Text(0.052019278710320756, 0.21875, &#39;X[0] &lt;= 302841.5\nsquared_error = 0.089\nsamples = 110\nvalue = 11.636&#39;),
 Text(0.051188299817184646, 0.15625, &#39;X[36] &lt;= 1.761\nsquared_error = 0.077\nsamples = 106\nvalue = 11.652&#39;),
 Text(0.0508559082599302, 0.09375, &#39;X[51] &lt;= 0.961\nsquared_error = 0.071\nsamples = 105\nvalue = 11.66&#39;),
 Text(0.050523516702675755, 0.03125, &#39;squared_error = 0.067\nsamples = 102\nvalue = 11.647&#39;),
 Text(0.051188299817184646, 0.03125, &#39;squared_error = 0.016\nsamples = 3\nvalue = 12.093&#39;),
 Text(0.05152069137443909, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.82&#39;),
 Text(0.05285025760345687, 0.15625, &#39;X[50] &lt;= 0.538\nsquared_error = 0.225\nsamples = 4\nvalue = 11.204&#39;),
 Text(0.05218547448894798, 0.09375, &#39;X[57] &lt;= 0.427\nsquared_error = 0.023\nsamples = 2\nvalue = 11.663&#39;),
 Text(0.05185308293169354, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.813&#39;),
 Text(0.05251786604620243, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.513&#39;),
 Text(0.05351504071796576, 0.09375, &#39;X[48] &lt;= 0.165\nsquared_error = 0.006\nsamples = 2\nvalue = 10.744&#39;),
 Text(0.05318264916071132, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.669&#39;),
 Text(0.05384743227522021, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.82&#39;),
 Text(0.054179823832474654, 0.21875, &#39;X[57] &lt;= 0.149\nsquared_error = 0.367\nsamples = 3\nvalue = 12.327&#39;),
 Text(0.05384743227522021, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 13.122&#39;),
 Text(0.0545122153897291, 0.15625, &#39;X[9] &lt;= 1.5\nsquared_error = 0.077\nsamples = 2\nvalue = 11.929&#39;),
 Text(0.054179823832474654, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.206&#39;),
 Text(0.054844606946983544, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.653&#39;),
 Text(0.054844606946983544, 0.34375, &#39;X[61] &lt;= 0.303\nsquared_error = 0.837\nsamples = 3\nvalue = 12.58&#39;),
 Text(0.0545122153897291, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 13.872&#39;),
 Text(0.05517699850423799, 0.28125, &#39;X[54] &lt;= 0.414\nsquared_error = 0.002\nsamples = 2\nvalue = 11.934&#39;),
 Text(0.054844606946983544, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.983&#39;),
 Text(0.055509390061492435, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.884&#39;),
 Text(0.06257271065314941, 0.40625, &#39;X[0] &lt;= 10900.0\nsquared_error = 0.161\nsamples = 252\nvalue = 11.795&#39;),
 Text(0.05916569719129134, 0.34375, &#39;X[51] &lt;= 0.006\nsquared_error = 0.131\nsamples = 92\nvalue = 11.692&#39;),
 Text(0.058833305634036895, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.309&#39;),
 Text(0.059498088748545785, 0.28125, &#39;X[8] &lt;= 2.5\nsquared_error = 0.111\nsamples = 91\nvalue = 11.707&#39;),
 Text(0.05700515206913744, 0.21875, &#39;X[50] &lt;= 0.042\nsquared_error = 0.086\nsamples = 79\nvalue = 11.66&#39;),
 Text(0.05584178161874689, 0.15625, &#39;X[47] &lt;= 0.283\nsquared_error = 0.046\nsamples = 4\nvalue = 11.163&#39;),
 Text(0.055509390061492435, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.82&#39;),
 Text(0.05617417317600133, 0.09375, &#39;X[62] &lt;= 0.875\nsquared_error = 0.009\nsamples = 3\nvalue = 11.277&#39;),
 Text(0.05584178161874689, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 11.212&#39;),
 Text(0.05650656473325578, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.408&#39;),
 Text(0.058168522519528004, 0.15625, &#39;X[44] &lt;= 0.911\nsquared_error = 0.074\nsamples = 75\nvalue = 11.687&#39;),
 Text(0.057503739405019114, 0.09375, &#39;X[50] &lt;= 0.987\nsquared_error = 0.069\nsamples = 65\nvalue = 11.723&#39;),
 Text(0.05717134784776467, 0.03125, &#39;squared_error = 0.063\nsamples = 64\nvalue = 11.713&#39;),
 Text(0.05783613096227356, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.388&#39;),
 Text(0.058833305634036895, 0.09375, &#39;X[50] &lt;= 0.878\nsquared_error = 0.048\nsamples = 10\nvalue = 11.453&#39;),
 Text(0.05850091407678245, 0.03125, &#39;squared_error = 0.006\nsamples = 5\nvalue = 11.289&#39;),
 Text(0.05916569719129134, 0.03125, &#39;squared_error = 0.037\nsamples = 5\nvalue = 11.616&#39;),
 Text(0.06199102542795413, 0.21875, &#39;X[2] &lt;= 1955.0\nsquared_error = 0.17\nsamples = 12\nvalue = 12.011&#39;),
 Text(0.06082765497756357, 0.15625, &#39;X[43] &lt;= 0.94\nsquared_error = 0.033\nsamples = 8\nvalue = 12.27&#39;),
 Text(0.060162871863054676, 0.09375, &#39;X[48] &lt;= 0.219\nsquared_error = 0.007\nsamples = 5\nvalue = 12.146&#39;),
 Text(0.05983048030580023, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.301&#39;),
 Text(0.06049526342030912, 0.03125, &#39;squared_error = 0.001\nsamples = 4\nvalue = 12.107&#39;),
 Text(0.061492438092072464, 0.09375, &#39;X[1] &lt;= 2125.5\nsquared_error = 0.009\nsamples = 3\nvalue = 12.476&#39;),
 Text(0.06116004653481802, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 12.409&#39;),
 Text(0.06182482964932691, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 12.612&#39;),
 Text(0.06315439587834469, 0.15625, &#39;X[53] &lt;= 0.984\nsquared_error = 0.042\nsamples = 4\nvalue = 11.493&#39;),
 Text(0.06282200432109024, 0.09375, &#39;X[6] &lt;= 4.0\nsquared_error = 0.006\nsamples = 3\nvalue = 11.605&#39;),
 Text(0.0624896127638358, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.513&#39;),
 Text(0.06315439587834469, 0.03125, &#39;squared_error = 0.002\nsamples = 2\nvalue = 11.652&#39;),
 Text(0.06348678743559913, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.156&#39;),
 Text(0.06597972411500748, 0.34375, &#39;X[45] &lt;= 0.005\nsquared_error = 0.169\nsamples = 160\nvalue = 11.854&#39;),
 Text(0.06564733255775303, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.597&#39;),
 Text(0.06631211567226192, 0.28125, &#39;X[47] &lt;= 0.988\nsquared_error = 0.16\nsamples = 159\nvalue = 11.862&#39;),
 Text(0.06597972411500748, 0.21875, &#39;X[2] &lt;= 1935.0\nsquared_error = 0.151\nsamples = 158\nvalue = 11.87&#39;),
 Text(0.06481635366461692, 0.15625, &#39;X[1] &lt;= 2250.0\nsquared_error = 0.326\nsamples = 12\nvalue = 12.171&#39;),
 Text(0.06415157055010803, 0.09375, &#39;X[44] &lt;= 0.152\nsquared_error = 0.155\nsamples = 8\nvalue = 11.869&#39;),
 Text(0.06381917899285358, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.002&#39;),
 Text(0.06448396210736247, 0.03125, &#39;squared_error = 0.054\nsamples = 7\nvalue = 11.993&#39;),
 Text(0.06548113677912581, 0.09375, &#39;X[46] &lt;= 0.72\nsquared_error = 0.121\nsamples = 4\nvalue = 12.776&#39;),
 Text(0.06514874522187136, 0.03125, &#39;squared_error = 0.015\nsamples = 2\nvalue = 13.095&#39;),
 Text(0.06581352833638025, 0.03125, &#39;squared_error = 0.024\nsamples = 2\nvalue = 12.456&#39;),
 Text(0.06714309456539803, 0.15625, &#39;X[0] &lt;= 545401.0\nsquared_error = 0.129\nsamples = 146\nvalue = 11.845&#39;),
 Text(0.0668107030081436, 0.09375, &#39;X[56] &lt;= 0.156\nsquared_error = 0.122\nsamples = 145\nvalue = 11.853&#39;),
 Text(0.06647831145088914, 0.03125, &#39;squared_error = 0.124\nsamples = 29\nvalue = 11.69&#39;),
 Text(0.06714309456539803, 0.03125, &#39;squared_error = 0.114\nsamples = 116\nvalue = 11.893&#39;),
 Text(0.06747548612265249, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.82&#39;),
 Text(0.06664450722951637, 0.21875, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.597&#39;),
 Text(0.07728103706165863, 0.53125, &#39;X[53] &lt;= 0.022\nsquared_error = 0.472\nsamples = 217\nvalue = 11.868&#39;),
 Text(0.06963603124480638, 0.46875, &#39;X[47] &lt;= 0.184\nsquared_error = 8.407\nsamples = 3\nvalue = 9.706&#39;),
 Text(0.06930363968755193, 0.40625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 5.617&#39;),
 Text(0.06996842280206082, 0.40625, &#39;X[0] &lt;= 26534.602\nsquared_error = 0.069\nsamples = 2\nvalue = 11.751&#39;),
 Text(0.06963603124480638, 0.34375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.014&#39;),
 Text(0.07030081435931528, 0.34375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.488&#39;),
 Text(0.08492604287851088, 0.46875, &#39;X[1] &lt;= 1525.0\nsquared_error = 0.295\nsamples = 214\nvalue = 11.899&#39;),
 Text(0.07570217716470001, 0.40625, &#39;X[22] &lt;= 1.5\nsquared_error = 0.279\nsamples = 134\nvalue = 11.728&#39;),
 Text(0.07096559747382417, 0.34375, &#39;X[43] &lt;= 0.013\nsquared_error = 0.241\nsamples = 100\nvalue = 11.854&#39;),
 Text(0.07063320591656971, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.463&#39;),
 Text(0.0712979890310786, 0.28125, &#39;X[44] &lt;= 0.074\nsquared_error = 0.224\nsamples = 99\nvalue = 11.868&#39;),
 Text(0.06913744390892472, 0.21875, &#39;X[46] &lt;= 0.388\nsquared_error = 0.305\nsamples = 7\nvalue = 11.36&#39;),
 Text(0.06847266079441582, 0.15625, &#39;X[54] &lt;= 0.436\nsquared_error = 0.07\nsamples = 2\nvalue = 10.574&#39;),
 Text(0.06814026923716138, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.84&#39;),
 Text(0.06880505235167027, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.309&#39;),
 Text(0.06980222702343361, 0.15625, &#39;X[47] &lt;= 0.771\nsquared_error = 0.053\nsamples = 5\nvalue = 11.674&#39;),
 Text(0.06946983546617916, 0.09375, &#39;squared_error = 0.0\nsamples = 2\nvalue = 11.408&#39;),
 Text(0.07013461858068805, 0.09375, &#39;X[46] &lt;= 0.823\nsquared_error = 0.009\nsamples = 3\nvalue = 11.852&#39;),
 Text(0.06980222702343361, 0.03125, &#39;squared_error = 0.001\nsamples = 2\nvalue = 11.786&#39;),
 Text(0.0704670101379425, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.983&#39;),
 Text(0.0734585341532325, 0.21875, &#39;X[51] &lt;= 0.929\nsquared_error = 0.196\nsamples = 92\nvalue = 11.907&#39;),
 Text(0.07212896792421472, 0.15625, &#39;X[54] &lt;= 0.059\nsquared_error = 0.19\nsamples = 86\nvalue = 11.941&#39;),
 Text(0.07146418480970583, 0.09375, &#39;X[61] &lt;= 0.254\nsquared_error = 0.05\nsamples = 6\nvalue = 12.462&#39;),
 Text(0.07113179325245139, 0.03125, &#39;squared_error = 0.008\nsamples = 2\nvalue = 12.703&#39;),
 Text(0.07179657636696028, 0.03125, &#39;squared_error = 0.027\nsamples = 4\nvalue = 12.342&#39;),
 Text(0.07279375103872361, 0.09375, &#39;X[54] &lt;= 0.094\nsquared_error = 0.178\nsamples = 80\nvalue = 11.902&#39;),
 Text(0.07246135948146917, 0.03125, &#39;squared_error = 0.108\nsamples = 3\nvalue = 11.259&#39;),
 Text(0.07312614259597806, 0.03125, &#39;squared_error = 0.164\nsamples = 77\nvalue = 11.927&#39;),
 Text(0.0747881003822503, 0.15625, &#39;X[53] &lt;= 0.44\nsquared_error = 0.037\nsamples = 6\nvalue = 11.417&#39;),
 Text(0.07412331726774139, 0.09375, &#39;X[53] &lt;= 0.267\nsquared_error = 0.011\nsamples = 2\nvalue = 11.186&#39;),
 Text(0.07379092571048695, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.29&#39;),
 Text(0.07445570882499584, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.082&#39;),
 Text(0.07545288349675919, 0.09375, &#39;X[49] &lt;= 0.129\nsquared_error = 0.011\nsamples = 4\nvalue = 11.532&#39;),
 Text(0.07512049193950474, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.695&#39;),
 Text(0.07578527505401363, 0.03125, &#39;squared_error = 0.002\nsamples = 3\nvalue = 11.478&#39;),
 Text(0.08043875685557587, 0.34375, &#39;X[59] &lt;= 0.536\nsquared_error = 0.206\nsamples = 34\nvalue = 11.358&#39;),
 Text(0.07736413495097225, 0.28125, &#39;X[0] &lt;= 13750.0\nsquared_error = 0.164\nsamples = 12\nvalue = 10.999&#39;),
 Text(0.07611766661126808, 0.21875, &#39;X[53] &lt;= 0.323\nsquared_error = 0.049\nsamples = 3\nvalue = 11.527&#39;),
 Text(0.07578527505401363, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.225&#39;),
 Text(0.07645005816852252, 0.15625, &#39;X[46] &lt;= 0.686\nsquared_error = 0.004\nsamples = 2\nvalue = 11.678&#39;),
 Text(0.07611766661126808, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.744&#39;),
 Text(0.07678244972577697, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.613&#39;),
 Text(0.07861060329067641, 0.21875, &#39;X[46] &lt;= 0.38\nsquared_error = 0.079\nsamples = 9\nvalue = 10.824&#39;),
 Text(0.0777796243975403, 0.15625, &#39;X[20] &lt;= 1.5\nsquared_error = 0.035\nsamples = 2\nvalue = 10.409&#39;),
 Text(0.07744723284028586, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.222&#39;),
 Text(0.07811201595479475, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.597&#39;),
 Text(0.07944158218381253, 0.15625, &#39;X[2] &lt;= 1972.5\nsquared_error = 0.029\nsamples = 7\nvalue = 10.942&#39;),
 Text(0.07877679906930364, 0.09375, &#39;X[57] &lt;= 0.741\nsquared_error = 0.011\nsamples = 2\nvalue = 11.186&#39;),
 Text(0.07844440751204919, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.29&#39;),
 Text(0.07910919062655808, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.082&#39;),
 Text(0.08010636529832142, 0.09375, &#39;X[38] &lt;= 1.5\nsquared_error = 0.002\nsamples = 5\nvalue = 10.844&#39;),
 Text(0.07977397374106698, 0.03125, &#39;squared_error = 0.001\nsamples = 3\nvalue = 10.878&#39;),
 Text(0.08043875685557587, 0.03125, &#39;squared_error = 0.001\nsamples = 2\nvalue = 10.794&#39;),
 Text(0.08351337876017949, 0.28125, &#39;X[57] &lt;= 0.216\nsquared_error = 0.121\nsamples = 22\nvalue = 11.554&#39;),
 Text(0.08176832308459366, 0.21875, &#39;X[48] &lt;= 0.528\nsquared_error = 0.144\nsamples = 4\nvalue = 11.127&#39;),
 Text(0.08110353997008476, 0.15625, &#39;X[59] &lt;= 0.71\nsquared_error = 0.046\nsamples = 2\nvalue = 11.439&#39;),
 Text(0.08077114841283031, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.225&#39;),
 Text(0.0814359315273392, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.653&#39;),
 Text(0.08243310619910255, 0.15625, &#39;X[60] &lt;= 0.718\nsquared_error = 0.048\nsamples = 2\nvalue = 10.816&#39;),
 Text(0.0821007146418481, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.597&#39;),
 Text(0.08276549775635698, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.035&#39;),
 Text(0.08525843443576533, 0.21875, &#39;X[46] &lt;= 0.793\nsquared_error = 0.066\nsamples = 18\nvalue = 11.649&#39;),
 Text(0.08409506398537477, 0.15625, &#39;X[51] &lt;= 0.547\nsquared_error = 0.045\nsamples = 15\nvalue = 11.723&#39;),
 Text(0.08343028087086587, 0.09375, &#39;X[54] &lt;= 0.426\nsquared_error = 0.029\nsamples = 10\nvalue = 11.613&#39;),
 Text(0.08309788931361144, 0.03125, &#39;squared_error = 0.012\nsamples = 5\nvalue = 11.468&#39;),
 Text(0.08376267242812033, 0.03125, &#39;squared_error = 0.004\nsamples = 5\nvalue = 11.758&#39;),
 Text(0.08475984709988366, 0.09375, &#39;X[60] &lt;= 0.424\nsquared_error = 0.003\nsamples = 5\nvalue = 11.943&#39;),
 Text(0.08442745554262922, 0.03125, &#39;squared_error = 0.001\nsamples = 3\nvalue = 11.983&#39;),
 Text(0.08509223865713811, 0.03125, &#39;squared_error = 0.001\nsamples = 2\nvalue = 11.884&#39;),
 Text(0.08642180488615589, 0.15625, &#39;X[50] &lt;= 0.412\nsquared_error = 0.009\nsamples = 3\nvalue = 11.277&#39;),
 Text(0.08608941332890145, 0.09375, &#39;X[62] &lt;= 0.869\nsquared_error = 0.002\nsamples = 2\nvalue = 11.337&#39;),
 Text(0.085757021771647, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.29&#39;),
 Text(0.08642180488615589, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.385&#39;),
 Text(0.08675419644341034, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.156&#39;),
 Text(0.09414990859232175, 0.40625, &#39;X[2] &lt;= 1935.0\nsquared_error = 0.191\nsamples = 80\nvalue = 12.185&#39;),
 Text(0.09016120990526841, 0.34375, &#39;X[53] &lt;= 0.561\nsquared_error = 0.096\nsamples = 11\nvalue = 12.626&#39;),
 Text(0.08924713312281868, 0.28125, &#39;X[51] &lt;= 0.872\nsquared_error = 0.059\nsamples = 8\nvalue = 12.489&#39;),
 Text(0.08841615422968256, 0.21875, &#39;X[46] &lt;= 0.631\nsquared_error = 0.017\nsamples = 6\nvalue = 12.614&#39;),
 Text(0.08775137111517367, 0.15625, &#39;X[58] &lt;= 0.778\nsquared_error = 0.0\nsamples = 2\nvalue = 12.78&#39;),
 Text(0.08741897955791923, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.794&#39;),
 Text(0.08808376267242812, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 12.766&#39;),
 Text(0.08908093734419145, 0.15625, &#39;X[43] &lt;= 0.585\nsquared_error = 0.004\nsamples = 4\nvalue = 12.531&#39;),
 Text(0.08874854578693701, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.429&#39;),
 Text(0.0894133289014459, 0.09375, &#39;X[45] &lt;= 0.569\nsquared_error = 0.001\nsamples = 3\nvalue = 12.566&#39;),
 Text(0.08908093734419145, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 12.543&#39;),
 Text(0.08974572045870034, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 12.612&#39;),
 Text(0.0900781120159548, 0.21875, &#39;X[46] &lt;= 0.576\nsquared_error = 0.0\nsamples = 2\nvalue = 12.114&#39;),
 Text(0.08974572045870034, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.101&#39;),
 Text(0.09041050357320925, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 12.128&#39;),
 Text(0.09107528668771814, 0.28125, &#39;X[50] &lt;= 0.556\nsquared_error = 0.011\nsamples = 3\nvalue = 12.99&#39;),
 Text(0.09074289513046369, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.848&#39;),
 Text(0.09140767824497258, 0.21875, &#39;X[41] &lt;= 0.5\nsquared_error = 0.002\nsamples = 2\nvalue = 13.062&#39;),
 Text(0.09107528668771814, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 13.106&#39;),
 Text(0.09174006980222703, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 13.017&#39;),
 Text(0.09813860727937511, 0.34375, &#39;X[29] &lt;= 1.5\nsquared_error = 0.17\nsamples = 69\nvalue = 12.114&#39;),
 Text(0.09489778959614426, 0.28125, &#39;X[46] &lt;= 0.442\nsquared_error = 0.149\nsamples = 58\nvalue = 12.039&#39;),
 Text(0.09323583180987204, 0.21875, &#39;X[51] &lt;= 0.879\nsquared_error = 0.119\nsamples = 26\nvalue = 11.852&#39;),
 Text(0.09240485291673592, 0.15625, &#39;X[58] &lt;= 0.604\nsquared_error = 0.091\nsamples = 24\nvalue = 11.905&#39;),
 Text(0.09174006980222703, 0.09375, &#39;X[59] &lt;= 0.565\nsquared_error = 0.03\nsamples = 12\nvalue = 12.078&#39;),
 Text(0.09140767824497258, 0.03125, &#39;squared_error = 0.009\nsamples = 5\nvalue = 11.914&#39;),
 Text(0.09207246135948147, 0.03125, &#39;squared_error = 0.012\nsamples = 7\nvalue = 12.195&#39;),
 Text(0.09306963603124481, 0.09375, &#39;X[55] &lt;= 0.739\nsquared_error = 0.093\nsamples = 12\nvalue = 11.733&#39;),
 Text(0.09273724447399036, 0.03125, &#39;squared_error = 0.052\nsamples = 10\nvalue = 11.635&#39;),
 Text(0.09340202758849925, 0.03125, &#39;squared_error = 0.011\nsamples = 2\nvalue = 12.221&#39;),
 Text(0.09406681070300814, 0.15625, &#39;X[4] &lt;= 2.5\nsquared_error = 0.018\nsamples = 2\nvalue = 11.216&#39;),
 Text(0.0937344191457537, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.082&#39;),
 Text(0.09439920226026259, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.35&#39;),
 Text(0.09655974738241649, 0.21875, &#39;X[0] &lt;= 12100.0\nsquared_error = 0.122\nsamples = 32\nvalue = 12.19&#39;),
 Text(0.09539637693202592, 0.15625, &#39;X[49] &lt;= 0.187\nsquared_error = 0.118\nsamples = 21\nvalue = 12.064&#39;),
 Text(0.09506398537477148, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.948&#39;),
 Text(0.09572876848928037, 0.09375, &#39;X[45] &lt;= 0.989\nsquared_error = 0.083\nsamples = 20\nvalue = 12.02&#39;),
 Text(0.09539637693202592, 0.03125, &#39;squared_error = 0.052\nsamples = 19\nvalue = 12.062&#39;),
 Text(0.09606116004653482, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.225&#39;),
 Text(0.09772311783280704, 0.15625, &#39;X[48] &lt;= 0.336\nsquared_error = 0.04\nsamples = 11\nvalue = 12.432&#39;),
 Text(0.09705833471829815, 0.09375, &#39;X[42] &lt;= -2.0\nsquared_error = 0.001\nsamples = 2\nvalue = 12.066&#39;),
 Text(0.09672594316104371, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.101&#39;),
 Text(0.0973907262755526, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 12.032&#39;),
 Text(0.09838790094731593, 0.09375, &#39;X[50] &lt;= 0.41\nsquared_error = 0.012\nsamples = 9\nvalue = 12.513&#39;),
 Text(0.0980555093900615, 0.03125, &#39;squared_error = 0.003\nsamples = 5\nvalue = 12.422&#39;),
 Text(0.09872029250457039, 0.03125, &#39;squared_error = 0.001\nsamples = 4\nvalue = 12.627&#39;),
 Text(0.10137942496260595, 0.28125, &#39;X[50] &lt;= 0.558\nsquared_error = 0.093\nsamples = 11\nvalue = 12.512&#39;),
 Text(0.10004985873358817, 0.21875, &#39;X[46] &lt;= 0.319\nsquared_error = 0.008\nsamples = 5\nvalue = 12.26&#39;),
 Text(0.09938507561907928, 0.15625, &#39;X[41] &lt;= 0.5\nsquared_error = 0.0\nsamples = 2\nvalue = 12.357&#39;),
 Text(0.09905268406182482, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.346&#39;),
 Text(0.09971746717633372, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.367&#39;),
 Text(0.10071464184809706, 0.15625, &#39;X[32] &lt;= 0.5\nsquared_error = 0.003\nsamples = 3\nvalue = 12.196&#39;),
 Text(0.1003822502908426, 0.09375, &#39;X[56] &lt;= 0.603\nsquared_error = 0.001\nsamples = 2\nvalue = 12.23&#39;),
 Text(0.10004985873358817, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.255&#39;),
 Text(0.10071464184809706, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.206&#39;),
 Text(0.10104703340535151, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.128&#39;),
 Text(0.10270899119162373, 0.21875, &#39;X[32] &lt;= 0.5\nsquared_error = 0.067\nsamples = 6\nvalue = 12.721&#39;),
 Text(0.10204420807711484, 0.15625, &#39;X[56] &lt;= 0.08\nsquared_error = 0.006\nsamples = 4\nvalue = 12.897&#39;),
 Text(0.1017118165198604, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.766&#39;),
 Text(0.10237659963436929, 0.09375, &#39;X[52] &lt;= 0.175\nsquared_error = 0.001\nsamples = 3\nvalue = 12.941&#39;),
 Text(0.10204420807711484, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.899&#39;),
 Text(0.10270899119162373, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 12.962&#39;),
 Text(0.10337377430613262, 0.15625, &#39;X[36] &lt;= 0.761\nsquared_error = 0.002\nsamples = 2\nvalue = 12.368&#39;),
 Text(0.10304138274887818, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.324&#39;),
 Text(0.10370616586338707, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.413&#39;),
 Text(0.13297350216054513, 0.71875, &#39;X[30] &lt;= 0.5\nsquared_error = 0.754\nsamples = 2413\nvalue = 11.219&#39;),
 Text(0.13264111060329067, 0.65625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 0.0&#39;),
 Text(0.13330589371779958, 0.65625, &#39;X[1] &lt;= 707.5\nsquared_error = 0.702\nsamples = 2412\nvalue = 11.223&#39;),
 Text(0.10819345188632208, 0.59375, &#39;X[57] &lt;= 0.011\nsquared_error = 3.204\nsamples = 98\nvalue = 10.523&#39;),
 Text(0.10786106032906764, 0.53125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 0.0&#39;),
 Text(0.10852584344357653, 0.53125, &#39;X[60] &lt;= 0.97\nsquared_error = 2.083\nsamples = 97\nvalue = 10.631&#39;),
 Text(0.10786106032906764, 0.46875, &#39;X[55] &lt;= 0.037\nsquared_error = 0.924\nsamples = 95\nvalue = 10.745&#39;),
 Text(0.10752866877181319, 0.40625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 6.215&#39;),
 Text(0.10819345188632208, 0.40625, &#39;X[49] &lt;= 0.067\nsquared_error = 0.714\nsamples = 94\nvalue = 10.793&#39;),
 Text(0.10437094897789596, 0.34375, &#39;X[35] &lt;= -2.5\nsquared_error = 3.498\nsamples = 4\nvalue = 9.189&#39;),
 Text(0.10403855742064151, 0.28125, &#39;X[50] &lt;= 0.184\nsquared_error = 0.73\nsamples = 3\nvalue = 10.181&#39;),
 Text(0.10370616586338707, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.29&#39;),
 Text(0.10437094897789596, 0.21875, &#39;X[45] &lt;= 0.611\nsquared_error = 0.173\nsamples = 2\nvalue = 9.627&#39;),
 Text(0.10403855742064151, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.043&#39;),
 Text(0.1047033405351504, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 9.21&#39;),
 Text(0.1047033405351504, 0.28125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 6.215&#39;),
 Text(0.11201595479474821, 0.34375, &#39;X[50] &lt;= 0.556\nsquared_error = 0.471\nsamples = 90\nvalue = 10.864&#39;),
 Text(0.1082765497756357, 0.28125, &#39;X[1] &lt;= 315.0\nsquared_error = 0.317\nsamples = 62\nvalue = 11.077&#39;),
 Text(0.10636529832142264, 0.21875, &#39;X[41] &lt;= 0.5\nsquared_error = 0.237\nsamples = 9\nvalue = 11.66&#39;),
 Text(0.10536812364965929, 0.15625, &#39;X[8] &lt;= 2.5\nsquared_error = 0.123\nsamples = 4\nvalue = 12.091&#39;),
 Text(0.10503573209240485, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.513&#39;),
 Text(0.10570051520691374, 0.09375, &#39;X[2] &lt;= 1970.0\nsquared_error = 0.015\nsamples = 3\nvalue = 12.283&#39;),
 Text(0.10536812364965929, 0.03125, &#39;squared_error = 0.005\nsamples = 2\nvalue = 12.361&#39;),
 Text(0.10603290676416818, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.128&#39;),
 Text(0.10736247299318598, 0.15625, &#39;X[9] &lt;= 5.5\nsquared_error = 0.061\nsamples = 5\nvalue = 11.315&#39;),
 Text(0.10703008143593153, 0.09375, &#39;X[0] &lt;= 2900.0\nsquared_error = 0.01\nsamples = 4\nvalue = 11.2&#39;),
 Text(0.10669768987867709, 0.03125, &#39;squared_error = 0.001\nsamples = 2\nvalue = 11.112&#39;),
 Text(0.10736247299318598, 0.03125, &#39;squared_error = 0.004\nsamples = 2\nvalue = 11.288&#39;),
 Text(0.10769486455044042, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.775&#39;),
 Text(0.11018780122984877, 0.21875, &#39;X[34] &lt;= 2.654\nsquared_error = 0.263\nsamples = 53\nvalue = 10.978&#39;),
 Text(0.1090244307794582, 0.15625, &#39;X[56] &lt;= 0.039\nsquared_error = 0.223\nsamples = 50\nvalue = 11.028&#39;),
 Text(0.10835964766494931, 0.09375, &#39;X[52] &lt;= 0.863\nsquared_error = 0.035\nsamples = 2\nvalue = 10.089&#39;),
 Text(0.10802725610769487, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.903&#39;),
 Text(0.10869203922220376, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.275&#39;),
 Text(0.10968921389396709, 0.09375, &#39;X[61] &lt;= 0.115\nsquared_error = 0.193\nsamples = 48\nvalue = 11.067&#39;),
 Text(0.10935682233671265, 0.03125, &#39;squared_error = 0.027\nsamples = 7\nvalue = 10.649&#39;),
 Text(0.11002160545122154, 0.03125, &#39;squared_error = 0.186\nsamples = 41\nvalue = 11.138&#39;),
 Text(0.11135117168023932, 0.15625, &#39;X[43] &lt;= 0.437\nsquared_error = 0.201\nsamples = 3\nvalue = 10.152&#39;),
 Text(0.11101878012298487, 0.09375, &#39;X[45] &lt;= 0.462\nsquared_error = 0.065\nsamples = 2\nvalue = 9.871&#39;),
 Text(0.11068638856573043, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.127&#39;),
 Text(0.11135117168023932, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.616&#39;),
 Text(0.11168356323749377, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.714&#39;),
 Text(0.11575535981386073, 0.28125, &#39;X[2] &lt;= 1955.0\nsquared_error = 0.488\nsamples = 28\nvalue = 10.392&#39;),
 Text(0.11417649991690211, 0.21875, &#39;X[58] &lt;= 0.821\nsquared_error = 0.178\nsamples = 22\nvalue = 10.633&#39;),
 Text(0.11301312946651156, 0.15625, &#39;X[55] &lt;= 0.893\nsquared_error = 0.101\nsamples = 18\nvalue = 10.509&#39;),
 Text(0.11234834635200266, 0.09375, &#39;X[50] &lt;= 0.891\nsquared_error = 0.068\nsamples = 15\nvalue = 10.426&#39;),
 Text(0.11201595479474821, 0.03125, &#39;squared_error = 0.044\nsamples = 12\nvalue = 10.342&#39;),
 Text(0.1126807379092571, 0.03125, &#39;squared_error = 0.024\nsamples = 3\nvalue = 10.76&#39;),
 Text(0.11367791258102045, 0.09375, &#39;X[46] &lt;= 0.382\nsquared_error = 0.06\nsamples = 3\nvalue = 10.922&#39;),
 Text(0.113345521023766, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.597&#39;),
 Text(0.11401030413827488, 0.03125, &#39;squared_error = 0.01\nsamples = 2\nvalue = 11.085&#39;),
 Text(0.11533987036729267, 0.15625, &#39;X[52] &lt;= 0.344\nsquared_error = 0.14\nsamples = 4\nvalue = 11.195&#39;),
 Text(0.11500747881003823, 0.09375, &#39;X[62] &lt;= 0.472\nsquared_error = 0.028\nsamples = 3\nvalue = 11.394&#39;),
 Text(0.11467508725278378, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 11.513&#39;),
 Text(0.11533987036729267, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.156&#39;),
 Text(0.11567226192454712, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.597&#39;),
 Text(0.11733421971081935, 0.21875, &#39;X[55] &lt;= 0.795\nsquared_error = 0.632\nsamples = 6\nvalue = 9.509&#39;),
 Text(0.11666943659631045, 0.15625, &#39;X[2] &lt;= 1989.5\nsquared_error = 0.029\nsamples = 4\nvalue = 10.061&#39;),
 Text(0.11633704503905601, 0.09375, &#39;squared_error = 0.0\nsamples = 2\nvalue = 9.903&#39;),
 Text(0.1170018281535649, 0.09375, &#39;X[48] &lt;= 0.375\nsquared_error = 0.008\nsamples = 2\nvalue = 10.218&#39;),
 Text(0.11666943659631045, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.309&#39;),
 Text(0.11733421971081935, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.127&#39;),
 Text(0.11799900282532824, 0.15625, &#39;X[50] &lt;= 0.685\nsquared_error = 0.012\nsamples = 2\nvalue = 8.406&#39;),
 Text(0.11766661126807379, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 8.294&#39;),
 Text(0.11833139438258268, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 8.517&#39;),
 Text(0.10919062655808542, 0.46875, &#39;X[44] &lt;= 0.391\nsquared_error = 27.369\nsamples = 2\nvalue = 5.232&#39;),
 Text(0.10885823500083099, 0.40625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 0.0&#39;),
 Text(0.10952301811533988, 0.40625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.463&#39;),
 Text(0.15841833554927706, 0.59375, &#39;X[6] &lt;= 1.5\nsquared_error = 0.575\nsamples = 2314\nvalue = 11.253&#39;),
 Text(0.1429854994183148, 0.53125, &#39;X[8] &lt;= 1.5\nsquared_error = 0.462\nsamples = 1051\nvalue = 11.107&#39;),
 Text(0.12922760511882997, 0.46875, &#39;X[34] &lt;= 2.154\nsquared_error = 0.75\nsamples = 71\nvalue = 10.484&#39;),
 Text(0.12310952301811534, 0.40625, &#39;X[55] &lt;= 0.677\nsquared_error = 1.038\nsamples = 21\nvalue = 9.894&#39;),
 Text(0.12057503739405019, 0.34375, &#39;X[49] &lt;= 0.274\nsquared_error = 0.717\nsamples = 15\nvalue = 9.46&#39;),
 Text(0.11866378593983713, 0.28125, &#39;X[51] &lt;= 0.454\nsquared_error = 0.172\nsamples = 4\nvalue = 10.309&#39;),
 Text(0.11833139438258268, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.002&#39;),
 Text(0.11899617749709157, 0.21875, &#39;X[58] &lt;= 0.737\nsquared_error = 0.016\nsamples = 3\nvalue = 10.078&#39;),
 Text(0.11866378593983713, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.903&#39;),
 Text(0.11932856905434602, 0.15625, &#39;X[61] &lt;= 0.583\nsquared_error = 0.001\nsamples = 2\nvalue = 10.165&#39;),
 Text(0.11899617749709157, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.204&#39;),
 Text(0.11966096061160046, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.127&#39;),
 Text(0.12248628884826325, 0.28125, &#39;X[20] &lt;= 1.5\nsquared_error = 0.558\nsamples = 11\nvalue = 9.152&#39;),
 Text(0.1213229183978727, 0.21875, &#39;X[61] &lt;= 0.9\nsquared_error = 0.232\nsamples = 7\nvalue = 9.578&#39;),
 Text(0.1206581352833638, 0.15625, &#39;X[47] &lt;= 0.904\nsquared_error = 0.026\nsamples = 5\nvalue = 9.291&#39;),
 Text(0.12032574372610935, 0.09375, &#39;squared_error = 0.0\nsamples = 4\nvalue = 9.21&#39;),
 Text(0.12099052684061824, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 9.616&#39;),
 Text(0.12198770151238159, 0.15625, &#39;X[61] &lt;= 0.923\nsquared_error = 0.028\nsamples = 2\nvalue = 10.295&#39;),
 Text(0.12165530995512713, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.463&#39;),
 Text(0.12232009306963604, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.127&#39;),
 Text(0.12364965929865382, 0.21875, &#39;X[54] &lt;= 0.655\nsquared_error = 0.253\nsamples = 4\nvalue = 8.406&#39;),
 Text(0.12331726774139937, 0.15625, &#39;X[62] &lt;= 0.259\nsquared_error = 0.049\nsamples = 3\nvalue = 8.137&#39;),
 Text(0.12298487618414493, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 7.824&#39;),
 Text(0.12364965929865382, 0.09375, &#39;squared_error = 0.0\nsamples = 2\nvalue = 8.294&#39;),
 Text(0.12398205085590826, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 9.21&#39;),
 Text(0.12564400864218048, 0.34375, &#39;X[62] &lt;= 0.629\nsquared_error = 0.196\nsamples = 6\nvalue = 10.978&#39;),
 Text(0.12464683397041715, 0.28125, &#39;X[41] &lt;= 0.5\nsquared_error = 0.024\nsamples = 3\nvalue = 11.38&#39;),
 Text(0.12431444241316271, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.599&#39;),
 Text(0.1249792255276716, 0.21875, &#39;X[62] &lt;= 0.294\nsquared_error = 0.0\nsamples = 2\nvalue = 11.271&#39;),
 Text(0.12464683397041715, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.252&#39;),
 Text(0.12531161708492605, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.29&#39;),
 Text(0.12664118331394383, 0.28125, &#39;X[43] &lt;= 0.758\nsquared_error = 0.044\nsamples = 3\nvalue = 10.575&#39;),
 Text(0.12630879175668938, 0.21875, &#39;X[2] &lt;= 1930.0\nsquared_error = 0.012\nsamples = 2\nvalue = 10.708&#39;),
 Text(0.12597640019943493, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.597&#39;),
 Text(0.12664118331394383, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.82&#39;),
 Text(0.12697357487119826, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.309&#39;),
 Text(0.13534568721954462, 0.40625, &#39;X[56] &lt;= 0.57\nsquared_error = 0.422\nsamples = 50\nvalue = 10.732&#39;),
 Text(0.1315855077281037, 0.34375, &#39;X[45] &lt;= 0.71\nsquared_error = 0.454\nsamples = 26\nvalue = 10.473&#39;),
 Text(0.12954960943992022, 0.28125, &#39;X[50] &lt;= 0.499\nsquared_error = 0.328\nsamples = 20\nvalue = 10.676&#39;),
 Text(0.12797074954296161, 0.21875, &#39;X[59] &lt;= 0.143\nsquared_error = 0.233\nsamples = 10\nvalue = 10.328&#39;),
 Text(0.1273059664284527, 0.15625, &#39;X[42] &lt;= -2.0\nsquared_error = 0.12\nsamples = 2\nvalue = 9.557&#39;),
 Text(0.12697357487119826, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.21&#39;),
 Text(0.12763835798570716, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 9.903&#39;),
 Text(0.1286355326574705, 0.15625, &#39;X[49] &lt;= 0.942\nsquared_error = 0.075\nsamples = 8\nvalue = 10.521&#39;),
 Text(0.12830314110021607, 0.09375, &#39;X[0] &lt;= 2300.0\nsquared_error = 0.024\nsamples = 7\nvalue = 10.609&#39;),
 Text(0.12797074954296161, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 10.82&#39;),
 Text(0.1286355326574705, 0.03125, &#39;squared_error = 0.008\nsamples = 5\nvalue = 10.525&#39;),
 Text(0.12896792421472494, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 9.903&#39;),
 Text(0.13112846933687886, 0.21875, &#39;X[29] &lt;= 0.5\nsquared_error = 0.181\nsamples = 10\nvalue = 11.023&#39;),
 Text(0.13029749044374273, 0.15625, &#39;X[41] &lt;= 0.5\nsquared_error = 0.066\nsamples = 8\nvalue = 10.85&#39;),
 Text(0.12963270732923385, 0.09375, &#39;X[54] &lt;= 0.248\nsquared_error = 0.01\nsamples = 6\nvalue = 10.983&#39;),
 Text(0.1293003157719794, 0.03125, &#39;squared_error = 0.002\nsamples = 2\nvalue = 10.867&#39;),
 Text(0.12996509888648827, 0.03125, &#39;squared_error = 0.004\nsamples = 4\nvalue = 11.041&#39;),
 Text(0.13096227355825163, 0.09375, &#39;X[52] &lt;= 0.517\nsquared_error = 0.021\nsamples = 2\nvalue = 10.453&#39;),
 Text(0.13062988200099718, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.309&#39;),
 Text(0.13129466511550605, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.597&#39;),
 Text(0.13195944823001496, 0.15625, &#39;X[55] &lt;= 0.927\nsquared_error = 0.041\nsamples = 2\nvalue = 11.716&#39;),
 Text(0.1316270566727605, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.918&#39;),
 Text(0.1322918397872694, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.513&#39;),
 Text(0.1336214060162872, 0.28125, &#39;X[54] &lt;= 0.587\nsquared_error = 0.28\nsamples = 6\nvalue = 9.797&#39;),
 Text(0.1329566229017783, 0.21875, &#39;X[60] &lt;= 0.341\nsquared_error = 0.037\nsamples = 3\nvalue = 9.345&#39;),
 Text(0.13262423134452384, 0.15625, &#39;squared_error = 0.0\nsamples = 2\nvalue = 9.21&#39;),
 Text(0.13328901445903274, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 9.616&#39;),
 Text(0.13428618913079607, 0.21875, &#39;X[0] &lt;= 1175.0\nsquared_error = 0.117\nsamples = 3\nvalue = 10.248&#39;),
 Text(0.13395379757354164, 0.15625, &#39;X[15] &lt;= 1.5\nsquared_error = 0.012\nsamples = 2\nvalue = 10.015&#39;),
 Text(0.1336214060162872, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.903&#39;),
 Text(0.13428618913079607, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.127&#39;),
 Text(0.13461858068805052, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.714&#39;),
 Text(0.13910586671098554, 0.34375, &#39;X[52] &lt;= 0.438\nsquared_error = 0.235\nsamples = 24\nvalue = 11.012&#39;),
 Text(0.13644673425294998, 0.28125, &#39;X[60] &lt;= 0.261\nsquared_error = 0.187\nsamples = 8\nvalue = 10.602&#39;),
 Text(0.13561575535981385, 0.21875, &#39;X[55] &lt;= 0.192\nsquared_error = 0.005\nsamples = 3\nvalue = 11.085&#39;),
 Text(0.13528336380255943, 0.15625, &#39;X[4] &lt;= 3.5\nsquared_error = 0.002\nsamples = 2\nvalue = 11.042&#39;),
 Text(0.13495097224530497, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.002&#39;),
 Text(0.13561575535981385, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.082&#39;),
 Text(0.1359481469170683, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.17&#39;),
 Text(0.13727771314608608, 0.21875, &#39;X[56] &lt;= 0.821\nsquared_error = 0.073\nsamples = 5\nvalue = 10.313&#39;),
 Text(0.1366129300315772, 0.15625, &#39;X[46] &lt;= 0.291\nsquared_error = 0.012\nsamples = 2\nvalue = 10.015&#39;),
 Text(0.13628053847432275, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.127&#39;),
 Text(0.13694532158883163, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.903&#39;),
 Text(0.137942496260595, 0.15625, &#39;X[60] &lt;= 0.34\nsquared_error = 0.014\nsamples = 3\nvalue = 10.512&#39;),
 Text(0.13761010470334054, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.342&#39;),
 Text(0.13827488781784944, 0.09375, &#39;squared_error = -0.0\nsamples = 2\nvalue = 10.597&#39;),
 Text(0.1417649991690211, 0.28125, &#39;X[55] &lt;= 0.249\nsquared_error = 0.134\nsamples = 16\nvalue = 11.217&#39;),
 Text(0.13993684560412165, 0.21875, &#39;X[51] &lt;= 0.644\nsquared_error = 0.075\nsamples = 5\nvalue = 11.612&#39;),
 Text(0.13927206248961277, 0.15625, &#39;X[45] &lt;= 0.818\nsquared_error = 0.038\nsamples = 3\nvalue = 11.426&#39;),
 Text(0.13893967093235832, 0.09375, &#39;X[15] &lt;= 1.5\nsquared_error = 0.002\nsamples = 2\nvalue = 11.561&#39;),
 Text(0.13860727937510386, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.608&#39;),
 Text(0.13927206248961277, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.513&#39;),
 Text(0.13960445404686722, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.156&#39;),
 Text(0.14060162871863055, 0.15625, &#39;X[42] &lt;= -2.0\nsquared_error = 0.0\nsamples = 2\nvalue = 11.891&#39;),
 Text(0.1402692371613761, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.871&#39;),
 Text(0.140934020275885, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.912&#39;),
 Text(0.14359315273392056, 0.21875, &#39;X[45] &lt;= 0.327\nsquared_error = 0.058\nsamples = 11\nvalue = 11.038&#39;),
 Text(0.14226358650490278, 0.15625, &#39;X[1] &lt;= 2066.402\nsquared_error = 0.018\nsamples = 4\nvalue = 11.31&#39;),
 Text(0.14159880339039388, 0.09375, &#39;X[29] &lt;= 0.5\nsquared_error = 0.006\nsamples = 2\nvalue = 11.429&#39;),
 Text(0.14126641183313943, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.508&#39;),
 Text(0.14193119494764833, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.35&#39;),
 Text(0.14292836961941166, 0.09375, &#39;X[51] &lt;= 0.677\nsquared_error = 0.001\nsamples = 2\nvalue = 11.191&#39;),
 Text(0.1425959780621572, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.225&#39;),
 Text(0.1432607611766661, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.156&#39;),
 Text(0.14492271896293835, 0.15625, &#39;X[49] &lt;= 0.443\nsquared_error = 0.014\nsamples = 7\nvalue = 10.882&#39;),
 Text(0.14425793584842944, 0.09375, &#39;X[54] &lt;= 0.072\nsquared_error = 0.002\nsamples = 4\nvalue = 10.793&#39;),
 Text(0.14392554429117502, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.714&#39;),
 Text(0.1445903274056839, 0.03125, &#39;squared_error = 0.0\nsamples = 3\nvalue = 10.82&#39;),
 Text(0.14558750207744722, 0.09375, &#39;X[54] &lt;= 0.167\nsquared_error = 0.005\nsamples = 3\nvalue = 11.0&#39;),
 Text(0.1452551105201928, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.915&#39;),
 Text(0.14591989363470168, 0.03125, &#39;squared_error = 0.002\nsamples = 2\nvalue = 11.042&#39;),
 Text(0.15674339371779958, 0.46875, &#39;X[54] &lt;= 0.001\nsquared_error = 0.411\nsamples = 980\nvalue = 11.153&#39;),
 Text(0.15641100216054513, 0.40625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 6.908&#39;),
 Text(0.157075785275054, 0.40625, &#39;X[42] &lt;= -2.5\nsquared_error = 0.393\nsamples = 979\nvalue = 11.157&#39;),
 Text(0.15244307794582018, 0.34375, &#39;X[8] &lt;= 3.5\nsquared_error = 0.465\nsamples = 419\nvalue = 11.025&#39;),
 Text(0.1496592986538142, 0.28125, &#39;X[4] &lt;= 4.5\nsquared_error = 0.437\nsamples = 392\nvalue = 10.983&#39;),
 Text(0.1480804387568556, 0.21875, &#39;X[0] &lt;= 4100.0\nsquared_error = 0.412\nsamples = 384\nvalue = 10.966&#39;),
 Text(0.14724945986371946, 0.15625, &#39;X[46] &lt;= 0.984\nsquared_error = 0.398\nsamples = 77\nvalue = 11.169&#39;),
 Text(0.146917068306465, 0.09375, &#39;X[50] &lt;= 0.959\nsquared_error = 0.309\nsamples = 76\nvalue = 11.204&#39;),
 Text(0.14658467674921058, 0.03125, &#39;squared_error = 0.283\nsamples = 74\nvalue = 11.174&#39;),
 Text(0.14724945986371946, 0.03125, &#39;squared_error = 0.012\nsamples = 2\nvalue = 12.318&#39;),
 Text(0.1475818514209739, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 8.517&#39;),
 Text(0.1489114176499917, 0.15625, &#39;X[52] &lt;= 0.141\nsquared_error = 0.402\nsamples = 307\nvalue = 10.915&#39;),
 Text(0.14824663453548279, 0.09375, &#39;X[2] &lt;= 1919.5\nsquared_error = 0.524\nsamples = 42\nvalue = 10.606&#39;),
 Text(0.14791424297822836, 0.03125, &#39;squared_error = 0.352\nsamples = 4\nvalue = 9.544&#39;),
 Text(0.14857902609273724, 0.03125, &#39;squared_error = 0.411\nsamples = 38\nvalue = 10.717&#39;),
 Text(0.1495762007645006, 0.09375, &#39;X[4] &lt;= 2.5\nsquared_error = 0.365\nsamples = 265\nvalue = 10.963&#39;),
 Text(0.14924380920724614, 0.03125, &#39;squared_error = 0.348\nsamples = 116\nvalue = 10.8&#39;),
 Text(0.14990859232175502, 0.03125, &#39;squared_error = 0.341\nsamples = 149\nvalue = 11.091&#39;),
 Text(0.1512381585507728, 0.21875, &#39;X[57] &lt;= 0.097\nsquared_error = 0.985\nsamples = 8\nvalue = 11.799&#39;),
 Text(0.15090576699351838, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 14.036&#39;),
 Text(0.15157055010802725, 0.15625, &#39;X[47] &lt;= 0.54\nsquared_error = 0.309\nsamples = 7\nvalue = 11.48&#39;),
 Text(0.15090576699351838, 0.09375, &#39;X[50] &lt;= 0.554\nsquared_error = 0.051\nsamples = 5\nvalue = 11.81&#39;),
 Text(0.15057337543626392, 0.03125, &#39;squared_error = 0.015\nsamples = 3\nvalue = 11.976&#39;),
 Text(0.1512381585507728, 0.03125, &#39;squared_error = 0.002\nsamples = 2\nvalue = 11.561&#39;),
 Text(0.15223533322253616, 0.09375, &#39;X[62] &lt;= 0.57\nsquared_error = 0.003\nsamples = 2\nvalue = 10.656&#39;),
 Text(0.1519029416652817, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.597&#39;),
 Text(0.15256772477979058, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.714&#39;),
 Text(0.15522685723782617, 0.28125, &#39;X[54] &lt;= 0.858\nsquared_error = 0.458\nsamples = 27\nvalue = 11.641&#39;),
 Text(0.15456207412331727, 0.21875, &#39;X[1] &lt;= 2316.402\nsquared_error = 0.298\nsamples = 24\nvalue = 11.784&#39;),
 Text(0.15422968256606281, 0.15625, &#39;X[2] &lt;= 1955.0\nsquared_error = 0.212\nsamples = 23\nvalue = 11.848&#39;),
 Text(0.15356489945155394, 0.09375, &#39;X[47] &lt;= 0.309\nsquared_error = 0.138\nsamples = 20\nvalue = 11.946&#39;),
 Text(0.15323250789429949, 0.03125, &#39;squared_error = 0.047\nsamples = 4\nvalue = 11.505&#39;),
 Text(0.1538972910088084, 0.03125, &#39;squared_error = 0.1\nsamples = 16\nvalue = 12.057&#39;),
 Text(0.15489446568057172, 0.09375, &#39;X[46] &lt;= 0.148\nsquared_error = 0.216\nsamples = 3\nvalue = 11.195&#39;),
 Text(0.15456207412331727, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.849&#39;),
 Text(0.15522685723782617, 0.03125, &#39;squared_error = 0.002\nsamples = 2\nvalue = 10.867&#39;),
 Text(0.15489446568057172, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.309&#39;),
 Text(0.15589164035233505, 0.21875, &#39;X[48] &lt;= 0.254\nsquared_error = 0.264\nsamples = 3\nvalue = 10.498&#39;),
 Text(0.1555592487950806, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.156&#39;),
 Text(0.1562240319095895, 0.15625, &#39;X[62] &lt;= 0.338\nsquared_error = 0.07\nsamples = 2\nvalue = 10.169&#39;),
 Text(0.15589164035233505, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.434&#39;),
 Text(0.15655642346684395, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 9.903&#39;),
 Text(0.16170849260428785, 0.34375, &#39;X[29] &lt;= 1.5\nsquared_error = 0.316\nsamples = 560\nvalue = 11.256&#39;),
 Text(0.15888316436762506, 0.28125, &#39;X[52] &lt;= 0.004\nsquared_error = 0.298\nsamples = 555\nvalue = 11.248&#39;),
 Text(0.15722120658135283, 0.21875, &#39;X[45] &lt;= 0.26\nsquared_error = 0.758\nsamples = 3\nvalue = 10.152&#39;),
 Text(0.15688881502409838, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 8.923&#39;),
 Text(0.15755359813860728, 0.15625, &#39;X[45] &lt;= 0.52\nsquared_error = 0.003\nsamples = 2\nvalue = 10.767&#39;),
 Text(0.15722120658135283, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.714&#39;),
 Text(0.15788598969586173, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.82&#39;),
 Text(0.1605451221538973, 0.21875, &#39;X[0] &lt;= 4850.0\nsquared_error = 0.289\nsamples = 552\nvalue = 11.254&#39;),
 Text(0.15921555592487951, 0.15625, &#39;X[43] &lt;= 0.768\nsquared_error = 0.283\nsamples = 116\nvalue = 11.403&#39;),
 Text(0.1585507728103706, 0.09375, &#39;X[60] &lt;= 0.935\nsquared_error = 0.252\nsamples = 89\nvalue = 11.313&#39;),
 Text(0.15821838125311616, 0.03125, &#39;squared_error = 0.238\nsamples = 86\nvalue = 11.285&#39;),
 Text(0.15888316436762506, 0.03125, &#39;squared_error = 0.019\nsamples = 3\nvalue = 12.091&#39;),
 Text(0.1598803390393884, 0.09375, &#39;X[48] &lt;= 0.501\nsquared_error = 0.27\nsamples = 27\nvalue = 11.7&#39;),
 Text(0.15954794748213397, 0.03125, &#39;squared_error = 0.123\nsamples = 12\nvalue = 11.378&#39;),
 Text(0.16021273059664284, 0.03125, &#39;squared_error = 0.238\nsamples = 15\nvalue = 11.957&#39;),
 Text(0.16187468838291508, 0.15625, &#39;X[62] &lt;= 0.985\nsquared_error = 0.283\nsamples = 436\nvalue = 11.214&#39;),
 Text(0.16120990526840617, 0.09375, &#39;X[52] &lt;= 0.961\nsquared_error = 0.277\nsamples = 430\nvalue = 11.205&#39;),
 Text(0.16087751371115175, 0.03125, &#39;squared_error = 0.263\nsamples = 415\nvalue = 11.219&#39;),
 Text(0.16154229682566063, 0.03125, &#39;squared_error = 0.47\nsamples = 15\nvalue = 10.792&#39;),
 Text(0.16253947149742395, 0.09375, &#39;X[50] &lt;= 0.463\nsquared_error = 0.282\nsamples = 6\nvalue = 11.891&#39;),
 Text(0.16220707994016953, 0.03125, &#39;squared_error = 0.019\nsamples = 4\nvalue = 11.548&#39;),
 Text(0.1628718630546784, 0.03125, &#39;squared_error = 0.104\nsamples = 2\nvalue = 12.577&#39;),
 Text(0.16453382084095064, 0.28125, &#39;X[58] &lt;= 0.855\nsquared_error = 1.531\nsamples = 5\nvalue = 12.143&#39;),
 Text(0.1642014292836962, 0.21875, &#39;X[0] &lt;= 15400.0\nsquared_error = 0.557\nsamples = 4\nvalue = 11.622&#39;),
 Text(0.1635366461691873, 0.15625, &#39;X[62] &lt;= 0.779\nsquared_error = 0.003\nsamples = 2\nvalue = 10.876&#39;),
 Text(0.16320425461193286, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.933&#39;),
 Text(0.16386903772644174, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.82&#39;),
 Text(0.1648662123982051, 0.15625, &#39;X[53] &lt;= 0.657\nsquared_error = 0.0\nsamples = 2\nvalue = 12.367&#39;),
 Text(0.16453382084095064, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.346&#39;),
 Text(0.16519860395545954, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.388&#39;),
 Text(0.1648662123982051, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 14.226&#39;),
 Text(0.1738511716802393, 0.53125, &#39;X[55] &lt;= 0.003\nsquared_error = 0.636\nsamples = 1263\nvalue = 11.374&#39;),
 Text(0.1694365963104537, 0.46875, &#39;X[44] &lt;= 0.838\nsquared_error = 23.012\nsamples = 4\nvalue = 8.307&#39;),
 Text(0.16910420475319926, 0.40625, &#39;X[52] &lt;= 0.298\nsquared_error = 0.011\nsamples = 3\nvalue = 11.076&#39;),
 Text(0.16877181319594484, 0.34375, &#39;squared_error = -0.0\nsamples = 2\nvalue = 11.002&#39;),
 Text(0.1694365963104537, 0.34375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.225&#39;),
 Text(0.16976898786770817, 0.40625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 0.0&#39;),
 Text(0.17826574705002493, 0.46875, &#39;X[29] &lt;= 0.5\nsquared_error = 0.535\nsamples = 1259\nvalue = 11.384&#39;),
 Text(0.17354994183147748, 0.40625, &#39;X[51] &lt;= 0.984\nsquared_error = 0.609\nsamples = 854\nvalue = 11.297&#39;),
 Text(0.17010137942496262, 0.34375, &#39;X[4] &lt;= 2.5\nsquared_error = 0.453\nsamples = 844\nvalue = 11.313&#39;),
 Text(0.16802393219212233, 0.28125, &#39;X[51] &lt;= 0.967\nsquared_error = 0.637\nsamples = 354\nvalue = 11.197&#39;),
 Text(0.16686056174173175, 0.21875, &#39;X[49] &lt;= 0.995\nsquared_error = 0.505\nsamples = 349\nvalue = 11.217&#39;),
 Text(0.16652817018447733, 0.15625, &#39;X[0] &lt;= 6225.0\nsquared_error = 0.483\nsamples = 348\nvalue = 11.208&#39;),
 Text(0.16586338706996842, 0.09375, &#39;X[31] &lt;= 0.5\nsquared_error = 0.644\nsamples = 99\nvalue = 11.003&#39;),
 Text(0.16553099551271397, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 5.106&#39;),
 Text(0.16619577862722287, 0.03125, &#39;squared_error = 0.292\nsamples = 98\nvalue = 11.063&#39;),
 Text(0.1671929532989862, 0.09375, &#39;X[43] &lt;= 0.909\nsquared_error = 0.396\nsamples = 249\nvalue = 11.29&#39;),
 Text(0.16686056174173175, 0.03125, &#39;squared_error = 0.342\nsamples = 226\nvalue = 11.241&#39;),
 Text(0.16752534485624065, 0.03125, &#39;squared_error = 0.671\nsamples = 23\nvalue = 11.77&#39;),
 Text(0.1671929532989862, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 14.063&#39;),
 Text(0.1691873026425129, 0.21875, &#39;X[60] &lt;= 0.91\nsquared_error = 7.913\nsamples = 5\nvalue = 9.831&#39;),
 Text(0.16885491108525844, 0.15625, &#39;X[51] &lt;= 0.982\nsquared_error = 0.153\nsamples = 4\nvalue = 11.227&#39;),
 Text(0.16852251952800398, 0.09375, &#39;X[54] &lt;= 0.213\nsquared_error = 0.011\nsamples = 3\nvalue = 11.007&#39;),
 Text(0.16819012797074953, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.156&#39;),
 Text(0.16885491108525844, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 10.933&#39;),
 Text(0.1691873026425129, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.884&#39;),
 Text(0.1695196941997673, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 4.248&#39;),
 Text(0.1721788266578029, 0.28125, &#39;X[0] &lt;= 126000.0\nsquared_error = 0.303\nsamples = 490\nvalue = 11.397&#39;),
 Text(0.17084926042878512, 0.21875, &#39;X[51] &lt;= 0.124\nsquared_error = 0.255\nsamples = 455\nvalue = 11.37&#39;),
 Text(0.17018447731427622, 0.15625, &#39;X[45] &lt;= 0.962\nsquared_error = 0.318\nsamples = 57\nvalue = 11.16&#39;),
 Text(0.16985208575702176, 0.09375, &#39;X[2] &lt;= 1990.0\nsquared_error = 0.258\nsamples = 56\nvalue = 11.194&#39;),
 Text(0.1695196941997673, 0.03125, &#39;squared_error = 0.213\nsamples = 54\nvalue = 11.238&#39;),
 Text(0.17018447731427622, 0.03125, &#39;squared_error = 0.012\nsamples = 2\nvalue = 10.015&#39;),
 Text(0.17051686887153067, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.259&#39;),
 Text(0.171514043543294, 0.15625, &#39;X[59] &lt;= 0.996\nsquared_error = 0.239\nsamples = 398\nvalue = 11.4&#39;),
 Text(0.17118165198603955, 0.09375, &#39;X[46] &lt;= 0.105\nsquared_error = 0.221\nsamples = 397\nvalue = 11.393&#39;),
 Text(0.17084926042878512, 0.03125, &#39;squared_error = 0.186\nsamples = 45\nvalue = 11.123&#39;),
 Text(0.171514043543294, 0.03125, &#39;squared_error = 0.215\nsamples = 352\nvalue = 11.428&#39;),
 Text(0.17184643510054845, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 14.063&#39;),
 Text(0.17350839288682068, 0.21875, &#39;X[7] &lt;= 1.5\nsquared_error = 0.799\nsamples = 35\nvalue = 11.752&#39;),
 Text(0.17284360977231178, 0.15625, &#39;X[58] &lt;= 0.853\nsquared_error = 0.678\nsamples = 3\nvalue = 10.248&#39;),
 Text(0.17251121821505733, 0.09375, &#39;X[46] &lt;= 0.456\nsquared_error = 0.21\nsamples = 2\nvalue = 10.767&#39;),
 Text(0.1721788266578029, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.309&#39;),
 Text(0.17284360977231178, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.225&#39;),
 Text(0.17317600132956623, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.21&#39;),
 Text(0.17417317600132956, 0.15625, &#39;X[57] &lt;= 0.97\nsquared_error = 0.579\nsamples = 32\nvalue = 11.894&#39;),
 Text(0.1738407844440751, 0.09375, &#39;X[43] &lt;= 0.612\nsquared_error = 0.416\nsamples = 31\nvalue = 11.818&#39;),
 Text(0.17350839288682068, 0.03125, &#39;squared_error = 0.369\nsamples = 19\nvalue = 11.571&#39;),
 Text(0.17417317600132956, 0.03125, &#39;squared_error = 0.241\nsamples = 12\nvalue = 12.209&#39;),
 Text(0.174505567558584, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 14.226&#39;),
 Text(0.17699850423799235, 0.34375, &#39;X[55] &lt;= 0.872\nsquared_error = 11.808\nsamples = 10\nvalue = 9.911&#39;),
 Text(0.1766661126807379, 0.28125, &#39;X[47] &lt;= 0.912\nsquared_error = 0.992\nsamples = 9\nvalue = 11.012&#39;),
 Text(0.17633372112348347, 0.21875, &#39;X[51] &lt;= 0.992\nsquared_error = 0.241\nsamples = 8\nvalue = 11.324&#39;),
 Text(0.17550274223034734, 0.15625, &#39;X[61] &lt;= 0.768\nsquared_error = 0.088\nsamples = 3\nvalue = 10.793&#39;),
 Text(0.17517035067309292, 0.09375, &#39;X[60] &lt;= 0.778\nsquared_error = 0.0\nsamples = 2\nvalue = 11.002&#39;),
 Text(0.17483795911583846, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.002&#39;),
 Text(0.17550274223034734, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.002&#39;),
 Text(0.1758351337876018, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.373&#39;),
 Text(0.17716470001661958, 0.15625, &#39;X[52] &lt;= 0.582\nsquared_error = 0.061\nsamples = 5\nvalue = 11.643&#39;),
 Text(0.1764999169021107, 0.09375, &#39;X[52] &lt;= 0.459\nsquared_error = 0.02\nsamples = 3\nvalue = 11.462&#39;),
 Text(0.17616752534485625, 0.03125, &#39;squared_error = 0.007\nsamples = 2\nvalue = 11.376&#39;),
 Text(0.17683230845936512, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.635&#39;),
 Text(0.17782948313112848, 0.09375, &#39;X[52] &lt;= 0.847\nsquared_error = 0.0\nsamples = 2\nvalue = 11.915&#39;),
 Text(0.17749709157387403, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.912&#39;),
 Text(0.1781618746883829, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.918&#39;),
 Text(0.17699850423799235, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 8.517&#39;),
 Text(0.1773308957952468, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 0.0&#39;),
 Text(0.18298155226857238, 0.40625, &#39;X[13] &lt;= -7.5\nsquared_error = 0.33\nsamples = 405\nvalue = 11.568&#39;),
 Text(0.18264916071131793, 0.34375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 5.106&#39;),
 Text(0.18331394382582683, 0.34375, &#39;X[9] &lt;= 1.5\nsquared_error = 0.227\nsamples = 404\nvalue = 11.584&#39;),
 Text(0.17999002825328236, 0.28125, &#39;X[58] &lt;= 0.964\nsquared_error = 0.299\nsamples = 34\nvalue = 12.039&#39;),
 Text(0.1796576366960279, 0.21875, &#39;X[50] &lt;= 0.081\nsquared_error = 0.142\nsamples = 33\nvalue = 11.969&#39;),
 Text(0.1788266578028918, 0.15625, &#39;X[2] &lt;= 1965.0\nsquared_error = 0.025\nsamples = 2\nvalue = 11.239&#39;),
 Text(0.17849426624563736, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.396&#39;),
 Text(0.17915904936014626, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.082&#39;),
 Text(0.18048861558916404, 0.15625, &#39;X[49] &lt;= 0.138\nsquared_error = 0.114\nsamples = 31\nvalue = 12.016&#39;),
 Text(0.17982383247465514, 0.09375, &#39;X[62] &lt;= 0.715\nsquared_error = 0.13\nsamples = 8\nvalue = 11.722&#39;),
 Text(0.17949144091740069, 0.03125, &#39;squared_error = 0.031\nsamples = 5\nvalue = 11.468&#39;),
 Text(0.1801562240319096, 0.03125, &#39;squared_error = 0.007\nsamples = 3\nvalue = 12.146&#39;),
 Text(0.18115339870367292, 0.09375, &#39;X[59] &lt;= 0.286\nsquared_error = 0.067\nsamples = 23\nvalue = 12.118&#39;),
 Text(0.1808210071464185, 0.03125, &#39;squared_error = 0.044\nsamples = 4\nvalue = 12.431&#39;),
 Text(0.18148579026092737, 0.03125, &#39;squared_error = 0.048\nsamples = 19\nvalue = 12.052&#39;),
 Text(0.18032241981053682, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 14.344&#39;),
 Text(0.18663785939837127, 0.28125, &#39;X[8] &lt;= 3.5\nsquared_error = 0.199\nsamples = 370\nvalue = 11.542&#39;),
 Text(0.18447731427621739, 0.21875, &#39;X[2] &lt;= 1945.0\nsquared_error = 0.189\nsamples = 358\nvalue = 11.523&#39;),
 Text(0.1831477480471996, 0.15625, &#39;X[46] &lt;= 0.134\nsquared_error = 0.268\nsamples = 95\nvalue = 11.375&#39;),
 Text(0.1824829649326907, 0.09375, &#39;X[62] &lt;= 0.593\nsquared_error = 0.472\nsamples = 11\nvalue = 11.861&#39;),
 Text(0.18215057337543628, 0.03125, &#39;squared_error = 0.149\nsamples = 6\nvalue = 11.408&#39;),
 Text(0.18281535648994515, 0.03125, &#39;squared_error = 0.32\nsamples = 5\nvalue = 12.403&#39;),
 Text(0.18381253116170848, 0.09375, &#39;X[53] &lt;= 0.996\nsquared_error = 0.206\nsamples = 84\nvalue = 11.311&#39;),
 Text(0.18348013960445406, 0.03125, &#39;squared_error = 0.184\nsamples = 83\nvalue = 11.328&#39;),
 Text(0.18414492271896293, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 9.903&#39;),
 Text(0.18580688050523517, 0.15625, &#39;X[0] &lt;= 747657.0\nsquared_error = 0.15\nsamples = 263\nvalue = 11.577&#39;),
 Text(0.18514209739072626, 0.09375, &#39;X[0] &lt;= 688910.0\nsquared_error = 0.142\nsamples = 261\nvalue = 11.569&#39;),
 Text(0.18480970583347184, 0.03125, &#39;squared_error = 0.132\nsamples = 259\nvalue = 11.577&#39;),
 Text(0.18547448894798071, 0.03125, &#39;squared_error = 0.392\nsamples = 2\nvalue = 10.53&#39;),
 Text(0.18647166361974407, 0.09375, &#39;X[58] &lt;= 0.537\nsquared_error = 0.006\nsamples = 2\nvalue = 12.689&#39;),
 Text(0.18613927206248962, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.766&#39;),
 Text(0.1868040551769985, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 12.612&#39;),
 Text(0.18879840452052518, 0.21875, &#39;X[52] &lt;= 0.789\nsquared_error = 0.188\nsamples = 12\nvalue = 12.09&#39;),
 Text(0.18846601296327073, 0.15625, &#39;X[51] &lt;= 0.529\nsquared_error = 0.088\nsamples = 11\nvalue = 12.189&#39;),
 Text(0.18780122984876185, 0.09375, &#39;X[54] &lt;= 0.509\nsquared_error = 0.059\nsamples = 6\nvalue = 12.377&#39;),
 Text(0.1874688382915074, 0.03125, &#39;squared_error = 0.006\nsamples = 4\nvalue = 12.544&#39;),
 Text(0.18813362140601628, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 12.044&#39;),
 Text(0.18913079607777963, 0.09375, &#39;X[56] &lt;= 0.461\nsquared_error = 0.028\nsamples = 5\nvalue = 11.964&#39;),
 Text(0.18879840452052518, 0.03125, &#39;squared_error = 0.006\nsamples = 3\nvalue = 12.092&#39;),
 Text(0.18946318763503406, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 11.771&#39;),
 Text(0.18913079607777963, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.002&#39;),
 Text(0.30971557150573376, 0.78125, &#39;X[1] &lt;= 1689.5\nsquared_error = 0.533\nsamples = 8841\nvalue = 11.854&#39;),
 Text(0.24529782802891806, 0.71875, &#39;X[62] &lt;= 0.0\nsquared_error = 0.524\nsamples = 3241\nvalue = 11.677&#39;),
 Text(0.2278102667442247, 0.65625, &#39;X[60] &lt;= 0.96\nsquared_error = 35.31\nsamples = 2\nvalue = 5.942&#39;),
 Text(0.22747787518697024, 0.59375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 0.0&#39;),
 Text(0.22814265830147915, 0.59375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.884&#39;),
 Text(0.26278538931361145, 0.65625, &#39;X[19] &lt;= 1.5\nsquared_error = 0.483\nsamples = 3239\nvalue = 11.68&#39;),
 Text(0.22880744141598802, 0.59375, &#39;X[29] &lt;= 0.5\nsquared_error = 0.444\nsamples = 2852\nvalue = 11.711&#39;),
 Text(0.21402900116337045, 0.53125, &#39;X[1] &lt;= 1383.0\nsquared_error = 0.421\nsamples = 2281\nvalue = 11.68&#39;),
 Text(0.20296659464849592, 0.46875, &#39;X[3] &lt;= 2.5\nsquared_error = 0.404\nsamples = 984\nvalue = 11.597&#39;),
 Text(0.19627721455875022, 0.40625, &#39;X[52] &lt;= 0.961\nsquared_error = 0.398\nsamples = 950\nvalue = 11.58&#39;),
 Text(0.19208077114841282, 0.34375, &#39;X[45] &lt;= 0.003\nsquared_error = 0.287\nsamples = 913\nvalue = 11.597&#39;),
 Text(0.1902941665281702, 0.28125, &#39;X[62] &lt;= 0.613\nsquared_error = 0.48\nsamples = 2\nvalue = 9.903&#39;),
 Text(0.18996177497091574, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.597&#39;),
 Text(0.19062655808542464, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.21&#39;),
 Text(0.19386737576865548, 0.28125, &#39;X[2] &lt;= 1925.0\nsquared_error = 0.28\nsamples = 911\nvalue = 11.601&#39;),
 Text(0.19129134119993352, 0.21875, &#39;X[9] &lt;= 1.5\nsquared_error = 0.242\nsamples = 35\nvalue = 11.936&#39;),
 Text(0.19012797074954296, 0.15625, &#39;X[59] &lt;= 0.03\nsquared_error = 0.143\nsamples = 19\nvalue = 12.249&#39;),
 Text(0.1897955791922885, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.225&#39;),
 Text(0.19046036230679741, 0.09375, &#39;X[60] &lt;= 0.114\nsquared_error = 0.09\nsamples = 18\nvalue = 12.305&#39;),
 Text(0.19012797074954296, 0.03125, &#39;squared_error = 0.039\nsamples = 3\nvalue = 12.755&#39;),
 Text(0.19079275386405184, 0.03125, &#39;squared_error = 0.051\nsamples = 15\nvalue = 12.215&#39;),
 Text(0.19245471165032407, 0.15625, &#39;X[45] &lt;= 0.767\nsquared_error = 0.104\nsamples = 16\nvalue = 11.564&#39;),
 Text(0.1917899285358152, 0.09375, &#39;X[44] &lt;= 0.339\nsquared_error = 0.053\nsamples = 11\nvalue = 11.705&#39;),
 Text(0.19145753697856074, 0.03125, &#39;squared_error = 0.011\nsamples = 3\nvalue = 11.439&#39;),
 Text(0.19212232009306965, 0.03125, &#39;squared_error = 0.032\nsamples = 8\nvalue = 11.805&#39;),
 Text(0.19311949476483298, 0.09375, &#39;X[57] &lt;= 0.493\nsquared_error = 0.076\nsamples = 5\nvalue = 11.253&#39;),
 Text(0.19278710320757853, 0.03125, &#39;squared_error = 0.014\nsamples = 3\nvalue = 11.455&#39;),
 Text(0.19345188632208743, 0.03125, &#39;squared_error = 0.017\nsamples = 2\nvalue = 10.951&#39;),
 Text(0.19644341033737744, 0.21875, &#39;X[34] &lt;= 2.154\nsquared_error = 0.277\nsamples = 876\nvalue = 11.588&#39;),
 Text(0.19511384410835964, 0.15625, &#39;X[49] &lt;= 0.834\nsquared_error = 0.344\nsamples = 87\nvalue = 11.401&#39;),
 Text(0.19444906099385076, 0.09375, &#39;X[21] &lt;= 1.5\nsquared_error = 0.325\nsamples = 72\nvalue = 11.313&#39;),
 Text(0.1941166694365963, 0.03125, &#39;squared_error = 0.237\nsamples = 67\nvalue = 11.389&#39;),
 Text(0.1947814525511052, 0.03125, &#39;squared_error = 0.384\nsamples = 5\nvalue = 10.296&#39;),
 Text(0.19577862722286854, 0.09375, &#39;X[61] &lt;= 0.884\nsquared_error = 0.223\nsamples = 15\nvalue = 11.824&#39;),
 Text(0.1954462356656141, 0.03125, &#39;squared_error = 0.117\nsamples = 12\nvalue = 11.997&#39;),
 Text(0.196111018780123, 0.03125, &#39;squared_error = 0.053\nsamples = 3\nvalue = 11.136&#39;),
 Text(0.19777297656639523, 0.15625, &#39;X[0] &lt;= 1225.0\nsquared_error = 0.265\nsamples = 789\nvalue = 11.608&#39;),
 Text(0.19710819345188632, 0.09375, &#39;X[46] &lt;= 0.13\nsquared_error = 1.289\nsamples = 10\nvalue = 11.097&#39;),
 Text(0.19677580189463187, 0.03125, &#39;squared_error = 0.12\nsamples = 2\nvalue = 8.864&#39;),
 Text(0.19744058500914077, 0.03125, &#39;squared_error = 0.023\nsamples = 8\nvalue = 11.655&#39;),
 Text(0.1984377596809041, 0.09375, &#39;X[50] &lt;= 0.026\nsquared_error = 0.249\nsamples = 779\nvalue = 11.615&#39;),
 Text(0.19810536812364965, 0.03125, &#39;squared_error = 0.56\nsamples = 19\nvalue = 11.244&#39;),
 Text(0.19877015123815855, 0.03125, &#39;squared_error = 0.238\nsamples = 760\nvalue = 11.624&#39;),
 Text(0.2004736579690876, 0.34375, &#39;X[52] &lt;= 0.965\nsquared_error = 2.96\nsamples = 37\nvalue = 11.169&#39;),
 Text(0.199102542795413, 0.28125, &#39;X[60] &lt;= 0.43\nsquared_error = 16.709\nsamples = 3\nvalue = 7.348&#39;),
 Text(0.19877015123815855, 0.21875, &#39;X[44] &lt;= 0.292\nsquared_error = 0.362\nsamples = 2\nvalue = 10.218&#39;),
 Text(0.1984377596809041, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.82&#39;),
 Text(0.199102542795413, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 9.616&#39;),
 Text(0.19943493435266743, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 1.609&#39;),
 Text(0.20184477314276217, 0.28125, &#39;X[56] &lt;= 0.147\nsquared_error = 0.346\nsamples = 34\nvalue = 11.506&#39;),
 Text(0.2004321090244308, 0.21875, &#39;X[5] &lt;= 0.5\nsquared_error = 0.747\nsamples = 5\nvalue = 10.811&#39;),
 Text(0.19976732590992188, 0.15625, &#39;X[50] &lt;= 0.261\nsquared_error = 0.25\nsamples = 3\nvalue = 11.442&#39;),
 Text(0.19943493435266743, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.82&#39;),
 Text(0.20009971746717634, 0.09375, &#39;X[1] &lt;= 1100.0\nsquared_error = 0.085\nsamples = 2\nvalue = 11.753&#39;),
 Text(0.19976732590992188, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.044&#39;),
 Text(0.2004321090244308, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 11.462&#39;),
 Text(0.20109689213893966, 0.15625, &#39;X[15] &lt;= 1.5\nsquared_error = 0.002\nsamples = 2\nvalue = 9.865&#39;),
 Text(0.2007645005816852, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.903&#39;),
 Text(0.20142928369619412, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.826&#39;),
 Text(0.20325743726109358, 0.21875, &#39;X[52] &lt;= 0.966\nsquared_error = 0.179\nsamples = 29\nvalue = 11.626&#39;),
 Text(0.20242645836795745, 0.15625, &#39;X[62] &lt;= 0.6\nsquared_error = 0.13\nsamples = 2\nvalue = 12.461&#39;),
 Text(0.20209406681070302, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.101&#39;),
 Text(0.2027588499252119, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.821&#39;),
 Text(0.20408841615422968, 0.15625, &#39;X[60] &lt;= 0.379\nsquared_error = 0.127\nsamples = 27\nvalue = 11.564&#39;),
 Text(0.2034236330397208, 0.09375, &#39;X[53] &lt;= 0.455\nsquared_error = 0.03\nsamples = 10\nvalue = 11.309&#39;),
 Text(0.20309124148246635, 0.03125, &#39;squared_error = 0.017\nsamples = 4\nvalue = 11.461&#39;),
 Text(0.20375602459697523, 0.03125, &#39;squared_error = 0.012\nsamples = 6\nvalue = 11.208&#39;),
 Text(0.20475319926873858, 0.09375, &#39;X[61] &lt;= 0.147\nsquared_error = 0.123\nsamples = 17\nvalue = 11.714&#39;),
 Text(0.20442080771148413, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.82&#39;),
 Text(0.205085590825993, 0.03125, &#39;squared_error = 0.078\nsamples = 16\nvalue = 11.77&#39;),
 Text(0.20965597473824166, 0.40625, &#39;X[42] &lt;= 1.5\nsquared_error = 0.349\nsamples = 34\nvalue = 12.051&#39;),
 Text(0.20757852750540137, 0.34375, &#39;X[56] &lt;= 0.673\nsquared_error = 0.412\nsamples = 8\nvalue = 12.677&#39;),
 Text(0.20674754861226524, 0.28125, &#39;X[46] &lt;= 0.769\nsquared_error = 0.138\nsamples = 6\nvalue = 12.383&#39;),
 Text(0.20608276549775636, 0.21875, &#39;X[47] &lt;= 0.075\nsquared_error = 0.07\nsamples = 4\nvalue = 12.173&#39;),
 Text(0.2057503739405019, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.736&#39;),
 Text(0.2064151570550108, 0.15625, &#39;X[49] &lt;= 0.788\nsquared_error = 0.009\nsamples = 3\nvalue = 12.318&#39;),
 Text(0.20608276549775636, 0.09375, &#39;X[42] &lt;= -2.5\nsquared_error = 0.003\nsamples = 2\nvalue = 12.377&#39;),
 Text(0.2057503739405019, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.324&#39;),
 Text(0.2064151570550108, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.429&#39;),
 Text(0.20674754861226524, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.201&#39;),
 Text(0.20741233172677415, 0.21875, &#39;X[6] &lt;= 4.0\nsquared_error = 0.009\nsamples = 2\nvalue = 12.803&#39;),
 Text(0.2070799401695197, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.707&#39;),
 Text(0.2077447232840286, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.899&#39;),
 Text(0.20840950639853748, 0.28125, &#39;X[45] &lt;= 0.47\nsquared_error = 0.192\nsamples = 2\nvalue = 13.561&#39;),
 Text(0.20807711484128302, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 13.999&#39;),
 Text(0.20874189795579193, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 13.122&#39;),
 Text(0.21173342197108194, 0.34375, &#39;X[45] &lt;= 0.165\nsquared_error = 0.171\nsamples = 26\nvalue = 11.859&#39;),
 Text(0.2100714641848097, 0.28125, &#39;X[59] &lt;= 0.568\nsquared_error = 0.101\nsamples = 5\nvalue = 11.375&#39;),
 Text(0.2094066810703008, 0.21875, &#39;X[56] &lt;= 0.237\nsquared_error = 0.028\nsamples = 3\nvalue = 11.613&#39;),
 Text(0.20907428951304638, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.385&#39;),
 Text(0.20973907262755526, 0.15625, &#39;X[50] &lt;= 0.457\nsquared_error = 0.002\nsamples = 2\nvalue = 11.727&#39;),
 Text(0.2094066810703008, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.775&#39;),
 Text(0.2100714641848097, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.678&#39;),
 Text(0.21073624729931859, 0.21875, &#39;X[57] &lt;= 0.703\nsquared_error = 0.0\nsamples = 2\nvalue = 11.018&#39;),
 Text(0.21040385574206416, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.035&#39;),
 Text(0.21106863885657304, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.002&#39;),
 Text(0.21339537975735418, 0.28125, &#39;X[32] &lt;= 1.5\nsquared_error = 0.119\nsamples = 21\nvalue = 11.974&#39;),
 Text(0.21306298820009972, 0.21875, &#39;X[55] &lt;= 0.604\nsquared_error = 0.08\nsamples = 20\nvalue = 11.928&#39;),
 Text(0.21173342197108194, 0.15625, &#39;X[47] &lt;= 0.342\nsquared_error = 0.042\nsamples = 12\nvalue = 12.085&#39;),
 Text(0.21106863885657304, 0.09375, &#39;X[4] &lt;= 3.5\nsquared_error = 0.013\nsamples = 5\nvalue = 12.249&#39;),
 Text(0.21073624729931859, 0.03125, &#39;squared_error = 0.002\nsamples = 2\nvalue = 12.114&#39;),
 Text(0.2114010304138275, 0.03125, &#39;squared_error = 0.0\nsamples = 3\nvalue = 12.339&#39;),
 Text(0.21239820508559082, 0.09375, &#39;X[49] &lt;= 0.618\nsquared_error = 0.03\nsamples = 7\nvalue = 11.968&#39;),
 Text(0.21206581352833637, 0.03125, &#39;squared_error = 0.009\nsamples = 3\nvalue = 11.784&#39;),
 Text(0.21273059664284527, 0.03125, &#39;squared_error = 0.002\nsamples = 4\nvalue = 12.107&#39;),
 Text(0.2143925544291175, 0.15625, &#39;X[43] &lt;= 0.657\nsquared_error = 0.044\nsamples = 8\nvalue = 11.691&#39;),
 Text(0.2137277713146086, 0.09375, &#39;X[60] &lt;= 0.128\nsquared_error = 0.013\nsamples = 5\nvalue = 11.836&#39;),
 Text(0.21339537975735418, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 11.716&#39;),
 Text(0.21406016287186305, 0.03125, &#39;squared_error = 0.005\nsamples = 3\nvalue = 11.916&#39;),
 Text(0.21505733754362638, 0.09375, &#39;X[62] &lt;= 0.4\nsquared_error = 0.004\nsamples = 3\nvalue = 11.45&#39;),
 Text(0.21472494598637196, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.535&#39;),
 Text(0.21538972910088083, 0.03125, &#39;squared_error = -0.0\nsamples = 2\nvalue = 11.408&#39;),
 Text(0.2137277713146086, 0.21875, &#39;squared_error = -0.0\nsamples = 1\nvalue = 12.899&#39;),
 Text(0.22509140767824498, 0.46875, &#39;X[53] &lt;= 0.989\nsquared_error = 0.425\nsamples = 1297\nvalue = 11.743&#39;),
 Text(0.22216220707994017, 0.40625, &#39;X[59] &lt;= 0.905\nsquared_error = 0.319\nsamples = 1281\nvalue = 11.753&#39;),
 Text(0.21979391723450226, 0.34375, &#39;X[47] &lt;= 0.078\nsquared_error = 0.278\nsamples = 1164\nvalue = 11.774&#39;),
 Text(0.21821505733754362, 0.28125, &#39;X[1] &lt;= 1660.0\nsquared_error = 1.127\nsamples = 85\nvalue = 11.538&#39;),
 Text(0.21788266578028917, 0.21875, &#39;X[51] &lt;= 0.976\nsquared_error = 0.704\nsamples = 84\nvalue = 11.609&#39;),
 Text(0.21705168688715307, 0.15625, &#39;X[48] &lt;= 0.85\nsquared_error = 0.253\nsamples = 82\nvalue = 11.671&#39;),
 Text(0.21638690377264416, 0.09375, &#39;X[53] &lt;= 0.98\nsquared_error = 0.169\nsamples = 68\nvalue = 11.762&#39;),
 Text(0.21605451221538974, 0.03125, &#39;squared_error = 0.131\nsamples = 67\nvalue = 11.786&#39;),
 Text(0.21671929532989861, 0.03125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 10.127&#39;),
 Text(0.21771647000166197, 0.09375, &#39;X[56] &lt;= 0.554\nsquared_error = 0.432\nsamples = 14\nvalue = 11.232&#39;),
 Text(0.21738407844440752, 0.03125, &#39;squared_error = 0.057\nsamples = 8\nvalue = 11.706&#39;),
 Text(0.2180488615589164, 0.03125, &#39;squared_error = 0.235\nsamples = 6\nvalue = 10.601&#39;),
 Text(0.2187136446734253, 0.15625, &#39;X[62] &lt;= 0.515\nsquared_error = 12.567\nsamples = 2\nvalue = 9.066&#39;),
 Text(0.21838125311617085, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.612&#39;),
 Text(0.21904603623067975, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 5.521&#39;),
 Text(0.21854744889479807, 0.21875, &#39;squared_error = -0.0\nsamples = 1\nvalue = 5.521&#39;),
 Text(0.22137277713146086, 0.28125, &#39;X[49] &lt;= 0.997\nsquared_error = 0.206\nsamples = 1079\nvalue = 11.793&#39;),
 Text(0.22070799401695196, 0.21875, &#39;X[52] &lt;= 0.997\nsquared_error = 0.2\nsamples = 1077\nvalue = 11.79&#39;),
 Text(0.22037560245969753, 0.15625, &#39;X[8] &lt;= 3.5\nsquared_error = 0.196\nsamples = 1076\nvalue = 11.788&#39;),
 Text(0.21971081934518863, 0.09375, &#39;X[6] &lt;= 1.5\nsquared_error = 0.188\nsamples = 944\nvalue = 11.768&#39;),
 Text(0.21937842778793418, 0.03125, &#39;squared_error = 0.209\nsamples = 163\nvalue = 11.644&#39;),
 Text(0.22004321090244308, 0.03125, &#39;squared_error = 0.179\nsamples = 781\nvalue = 11.794&#39;),
 Text(0.2210403855742064, 0.09375, &#39;X[51] &lt;= 0.116\nsquared_error = 0.235\nsamples = 132\nvalue = 11.934&#39;),
 Text(0.22070799401695196, 0.03125, &#39;squared_error = 0.282\nsamples = 12\nvalue = 12.425&#39;),
 Text(0.22137277713146086, 0.03125, &#39;squared_error = 0.204\nsamples = 120\nvalue = 11.884&#39;),
 Text(0.2210403855742064, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 13.755&#39;),
 Text(0.22203756024596974, 0.21875, &#39;X[47] &lt;= 0.57\nsquared_error = 1.464\nsamples = 2\nvalue = 13.224&#39;),
 Text(0.22170516868871532, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.014&#39;),
 Text(0.2223699518032242, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 14.433&#39;),
 Text(0.2245304969253781, 0.34375, &#39;X[28] &lt;= 1.5\nsquared_error = 0.683\nsamples = 117\nvalue = 11.54&#39;),
 Text(0.22419810536812365, 0.28125, &#39;X[54] &lt;= 0.997\nsquared_error = 0.373\nsamples = 116\nvalue = 11.592&#39;),
 Text(0.2238657138108692, 0.21875, &#39;X[44] &lt;= 0.938\nsquared_error = 0.294\nsamples = 115\nvalue = 11.619&#39;),
 Text(0.2230347349177331, 0.15625, &#39;X[54] &lt;= 0.089\nsquared_error = 0.234\nsamples = 113\nvalue = 11.652&#39;),
 Text(0.2223699518032242, 0.09375, &#39;X[61] &lt;= 0.443\nsquared_error = 0.227\nsamples = 10\nvalue = 12.125&#39;),
 Text(0.22203756024596974, 0.03125, &#39;squared_error = 0.108\nsamples = 7\nvalue = 11.875&#39;),
 Text(0.22270234336047864, 0.03125, &#39;squared_error = 0.018\nsamples = 3\nvalue = 12.708&#39;),
 Text(0.22369951803224197, 0.09375, &#39;X[46] &lt;= 0.942\nsquared_error = 0.211\nsamples = 103\nvalue = 11.606&#39;),
 Text(0.22336712647498755, 0.03125, &#39;squared_error = 0.193\nsamples = 97\nvalue = 11.57&#39;),
 Text(0.22403190958949643, 0.03125, &#39;squared_error = 0.14\nsamples = 6\nvalue = 12.187&#39;),
 Text(0.22469669270400533, 0.15625, &#39;X[59] &lt;= 0.933\nsquared_error = 0.135\nsamples = 2\nvalue = 9.76&#39;),
 Text(0.22436430114675088, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 10.127&#39;),
 Text(0.22502908426125975, 0.09375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 9.393&#39;),
 Text(0.2245304969253781, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 8.517&#39;),
 Text(0.22486288848263253, 0.28125, &#39;squared_error = -0.0\nsamples = 1\nvalue = 5.521&#39;),
 Text(0.22802060827654977, 0.40625, &#39;X[53] &lt;= 0.99\nsquared_error = 8.246\nsamples = 16\nvalue = 10.945&#39;),
 Text(0.22768821671929532, 0.34375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 0.0&#39;),
 Text(0.22835299983380422, 0.34375, &#39;X[43] &lt;= 0.462\nsquared_error = 0.277\nsamples = 15\nvalue = 11.675&#39;),
 Text(0.22702343360478644, 0.28125, &#39;X[59] &lt;= 0.468\nsquared_error = 0.091\nsamples = 6\nvalue = 12.191&#39;),
 Text(0.22635865049027754, 0.21875, &#39;X[47] &lt;= 0.918\nsquared_error = 0.035\nsamples = 4\nvalue = 12.374&#39;),
 Text(0.2260262589330231, 0.15625, &#39;X[54] &lt;= 0.317\nsquared_error = 0.002\nsamples = 3\nvalue = 12.268&#39;),
 Text(0.22569386737576866, 0.09375, &#39;X[46] &lt;= 0.534\nsquared_error = 0.0\nsamples = 2\nvalue = 12.24&#39;),
 Text(0.2253614758185142, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.255&#39;),
 Text(0.2260262589330231, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.226&#39;),
 Text(0.22635865049027754, 0.09375, &#39;squared_error = -0.0\nsamples = 1\nvalue = 12.324&#39;),
 Text(0.226691042047532, 0.15625, &#39;squared_error = -0.0\nsamples = 1\nvalue = 12.692&#39;),
 Text(0.22768821671929532, 0.21875, &#39;X[48] &lt;= 0.446\nsquared_error = 0.003\nsamples = 2\nvalue = 11.826&#39;),
 Text(0.2273558251620409, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.884&#39;),
 Text(0.22802060827654977, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.768&#39;),
 Text(0.229682566062822, 0.28125, &#39;X[52] &lt;= 0.115\nsquared_error = 0.105\nsamples = 9\nvalue = 11.331&#39;),
 Text(0.22901778294831313, 0.21875, &#39;X[6] &lt;= 4.0\nsquared_error = 0.042\nsamples = 2\nvalue = 11.839&#39;),
 Text(0.22868539139105867, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.044&#39;),
 Text(0.22935017450556755, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.635&#39;),
 Text(0.2303473491773309, 0.21875, &#39;X[42] &lt;= -2.0\nsquared_error = 0.028\nsamples = 7\nvalue = 11.186&#39;),
 Text(0.23001495762007645, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.513&#39;),
 Text(0.23067974073458533, 0.15625, &#39;X[58] &lt;= 0.197\nsquared_error = 0.011\nsamples = 6\nvalue = 11.131&#39;),
 Text(0.23001495762007645, 0.09375, &#39;X[1] &lt;= 1552.5\nsquared_error = 0.0\nsamples = 3\nvalue = 11.234&#39;),
 Text(0.229682566062822, 0.03125, &#39;squared_error = 0.0\nsamples = 2\nvalue = 11.219&#39;),
 Text(0.2303473491773309, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.264&#39;),
 Text(0.23134452384909424, 0.09375, &#39;X[50] &lt;= 0.374\nsquared_error = 0.001\nsamples = 3\nvalue = 11.029&#39;),
 Text(0.23101213229183978, 0.03125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.082&#39;),
 Text(0.2316769154063487, 0.03125, &#39;squared_error = -0.0\nsamples = 2\nvalue = 11.002&#39;),
 Text(0.24358588166860562, 0.53125, &#39;X[60] &lt;= 0.028\nsquared_error = 0.515\nsamples = 571\nvalue = 11.836&#39;),
 Text(0.23205085590825994, 0.46875, &#39;X[44] &lt;= 0.203\nsquared_error = 6.361\nsamples = 14\nvalue = 10.979&#39;),
 Text(0.2303473491773309, 0.40625, &#39;X[0] &lt;= 18375.0\nsquared_error = 10.398\nsamples = 3\nvalue = 7.141&#39;),
 Text(0.23001495762007645, 0.34375, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.695&#39;),
 Text(0.23067974073458533, 0.34375, &#39;X[55] &lt;= 0.333\nsquared_error = 0.044\nsamples = 2\nvalue = 4.865&#39;),
 Text(0.2303473491773309, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 4.654&#39;),
 Text(0.23101213229183978, 0.28125, &#39;squared_error = 0.0\nsamples = 1\nvalue = 5.075&#39;),
 Text(0.23375436263918897, 0.40625, &#39;X[8] &lt;= 2.5\nsquared_error = 0.147\nsamples = 11\nvalue = 12.026&#39;),
 Text(0.23234169852085756, 0.34375, &#39;X[51] &lt;= 0.833\nsquared_error = 0.053\nsamples = 4\nvalue = 11.647&#39;),
 Text(0.2316769154063487, 0.28125, &#39;X[60] &lt;= 0.005\nsquared_error = 0.015\nsamples = 2\nvalue = 11.859&#39;),
 Text(0.23134452384909424, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.983&#39;),
 Text(0.2320093069636031, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.736&#39;),
 Text(0.23300648163536647, 0.28125, &#39;X[62] &lt;= 0.783\nsquared_error = 0.001\nsamples = 2\nvalue = 11.435&#39;),
 Text(0.23267409007811202, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.462&#39;),
 Text(0.2333388731926209, 0.21875, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.408&#39;),
 Text(0.23516702675752035, 0.34375, &#39;X[45] &lt;= 0.258\nsquared_error = 0.072\nsamples = 7\nvalue = 12.243&#39;),
 Text(0.23433604786438425, 0.28125, &#39;X[58] &lt;= 0.4\nsquared_error = 0.006\nsamples = 3\nvalue = 12.528&#39;),
 Text(0.2340036563071298, 0.21875, &#39;X[53] &lt;= 0.561\nsquared_error = 0.001\nsamples = 2\nvalue = 12.577&#39;),
 Text(0.23367126474987535, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.612&#39;),
 Text(0.23433604786438425, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 12.543&#39;),
 Text(0.2346684394216387, 0.21875, &#39;squared_error = -0.0\nsamples = 1\nvalue = 12.429&#39;),
 Text(0.23599800565065648, 0.28125, &#39;X[47] &lt;= 0.392\nsquared_error = 0.015\nsamples = 4\nvalue = 12.029&#39;),
 Text(0.23533322253614758, 0.21875, &#39;X[51] &lt;= 0.712\nsquared_error = 0.001\nsamples = 2\nvalue = 11.918&#39;),
 Text(0.23500083097889313, 0.15625, &#39;squared_error = 0.0\nsamples = 1\nvalue = 11.884&#39;),
 ...]
</pre></div>
</div>
<img alt="../_images/2_introduction_to_Machine_Learning_72_1.png" src="../_images/2_introduction_to_Machine_Learning_72_1.png" />
</div>
</div>
<p>To reduce the complexity of the tree, we <strong>prune</strong> the tree: we collapse its leaves, permitting bias to increase but forcing variance to decrease until the desired trade-off is achieved. In <code class="docutils literal notranslate"><span class="pre">rpart</span></code>, this is done by considering a modified loss function that takes into account the number of terminal nodes (i.e., the number of regions in which the original data was partitioned). Somewhat heuristically, if we denote tree predictions by <span class="math notranslate nohighlight">\(T(x)\)</span> and its number of terminal nodes by  <span class="math notranslate nohighlight">\(|T|\)</span>, the modified regression problem can be written as:</p>
<div class="math notranslate nohighlight" id="equation-pruned-tree">
<span class="eqno">(2.4)<a class="headerlink" href="#equation-pruned-tree" title="Permalink to this equation">¶</a></span>\[
  \widehat{T} = \arg\min_{T} \sum_{i=1}^m \left( T(X_i) - Y_i \right)^2 + c_p |T|
\]</div>
<p>The complexity of the tree is controlled by the scalar parameter <span class="math notranslate nohighlight">\(c_p\)</span>, denoted as <code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> in <code class="docutils literal notranslate"><span class="pre">sklearn.tree.DecisionTreeRegressor</span></code>. For each value of <span class="math notranslate nohighlight">\(c_p\)</span>, we find the subtree that solves <a class="reference internal" href="#equation-pruned-tree">(2.4)</a>. Large values of <span class="math notranslate nohighlight">\(c_p\)</span> lead to aggressively pruned trees, which have more bias and less variance. Small values of <span class="math notranslate nohighlight">\(c_p\)</span> allow for deeper trees whose predictions can vary more wildly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">max_depth</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">mse_gini</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">30</span><span class="p">):</span>
    <span class="n">dtree</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span> <span class="n">max_depth</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">dtree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">dtree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
    <span class="n">mse_gini</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">))</span>
    <span class="n">max_depth</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

<span class="n">d1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;acc_gini&#39;</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">mse_gini</span><span class="p">),</span><span class="s1">&#39;max_depth&#39;</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">max_depth</span><span class="p">)})</span>

<span class="c1"># visualizing changes in parameters</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="s1">&#39;max_depth&#39;</span><span class="p">,</span><span class="s1">&#39;acc_gini&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">d1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;max_depth&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;mse&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x1825a929760&gt;
</pre></div>
</div>
<img alt="../_images/2_introduction_to_Machine_Learning_74_1.png" src="../_images/2_introduction_to_Machine_Learning_74_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">cost_complexity_pruning_path</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">alphas_dt</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">path</span><span class="p">[</span><span class="s1">&#39;ccp_alphas&#39;</span><span class="p">],</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;alphas&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># A function with a manual cross validation</span>
<span class="k">def</span> <span class="nf">run_cross_validation_on_trees</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">tree_ccp</span><span class="p">,</span> <span class="n">nfold</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">cv_scores_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">cv_scores_mean</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">cv_scores_std</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">cp_table</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">cp_table_error</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">cp_table_std</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">cp_table_rel_error</span> <span class="o">=</span> <span class="p">[]</span>
   
     <span class="c1"># Num ob observations</span>
    <span class="n">nobs</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># Define folds indices </span>
    <span class="n">list_1</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nfold</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span><span class="o">*</span><span class="n">nobs</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">nobs</span><span class="p">,</span><span class="n">nobs</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">foldid</span> <span class="o">=</span> <span class="p">[</span><span class="n">list_1</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">sample</span><span class="p">]</span>

    <span class="c1"># Create split function(similar to R)</span>
    <span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>
        <span class="n">count</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span> <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">compress</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">el</span> <span class="o">==</span> <span class="n">i</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">f</span><span class="p">)))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">count</span><span class="p">)</span> <span class="p">)</span> 

    <span class="c1"># Split observation indices into folds </span>
    <span class="n">list_2</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nobs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
    <span class="n">I</span> <span class="o">=</span> <span class="n">split</span><span class="p">(</span><span class="n">list_2</span><span class="p">,</span> <span class="n">foldid</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tree_ccp</span><span class="p">:</span>
        <span class="n">dtree</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span> <span class="n">ccp_alpha</span><span class="o">=</span> <span class="n">i</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
        
    <span class="c1"># loop to save results</span>
        <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">I</span><span class="p">)):</span>
            <span class="c1"># Split data - index to keep are in mask as booleans</span>
            <span class="n">include_idx</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">I</span><span class="p">[</span><span class="n">b</span><span class="p">])</span>  <span class="c1">#Here should go I[b] Set is more efficient, but doesn&#39;t reorder your elements if that is desireable</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([(</span><span class="n">a</span> <span class="ow">in</span> <span class="n">include_idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))])</span>
            
            <span class="n">dtree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">])</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">dtree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span>
            <span class="n">xerror_fold</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">-</span> <span class="n">pred</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
            <span class="n">rel_error_fold</span> <span class="o">=</span> <span class="mi">1</span><span class="o">-</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">Y</span><span class="p">[</span><span class="n">mask</span><span class="p">],</span> <span class="n">pred</span><span class="p">)</span>
            <span class="n">cv_scores_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xerror_fold</span><span class="p">)</span>
            <span class="n">rel_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rel_error_fold</span><span class="p">)</span>
            <span class="n">xerror</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cv_scores_list</span><span class="p">)</span>
            <span class="n">xstd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">cv_scores_list</span><span class="p">)</span>

        <span class="n">cp_table_rel_error</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rel_error</span><span class="p">)</span>
        <span class="n">cp_table_error</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xerror</span><span class="p">)</span>
        <span class="n">cp_table_std</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">xstd</span><span class="p">)</span>
    <span class="n">cp_table</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">alphas_dt</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;cp&quot;</span><span class="p">),</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">cp_table_rel_error</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;rel error&quot;</span><span class="p">)</span>
                         <span class="p">,</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">cp_table_error</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;xerror&quot;</span><span class="p">),</span>
                         <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">cp_table_std</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;xstd&quot;</span><span class="p">)])</span><span class="o">.</span><span class="n">T</span>    
    <span class="k">return</span> <span class="n">cp_table</span> 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sm_tree_ccp</span> <span class="o">=</span> <span class="n">alphas_dt</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>
<span class="n">cp_table</span> <span class="o">=</span> <span class="n">run_cross_validation_on_trees</span><span class="p">(</span><span class="n">XX</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">sm_tree_ccp</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cp_table</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>cp</th>
      <th>rel error</th>
      <th>xerror</th>
      <th>xstd</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.000000e+00</td>
      <td>1.301711</td>
      <td>1.304097</td>
      <td>0.091112</td>
    </tr>
    <tr>
      <th>1</th>
      <td>7.067264e-19</td>
      <td>1.301711</td>
      <td>1.304097</td>
      <td>0.091112</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.060090e-18</td>
      <td>1.301711</td>
      <td>1.304097</td>
      <td>0.091112</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.413453e-18</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.413453e-18</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">run_cross_validation_on_trees</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">tree_ccp</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">):</span>
    <span class="n">cv_scores_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">cv_scores_std</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">cv_scores_mean</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">MSE_scores</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">ccp</span> <span class="ow">in</span> <span class="n">tree_ccp</span><span class="p">:</span>
        <span class="n">tree_model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">ccp_alpha</span><span class="o">=</span> <span class="n">ccp</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">cv_scores</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">tree_model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span> <span class="n">scoring</span><span class="p">)</span>
        <span class="n">cv_scores_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cv_scores</span><span class="p">)</span>
        <span class="n">cv_scores_mean</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cv_scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
        <span class="n">cv_scores_std</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cv_scores</span><span class="o">.</span><span class="n">std</span><span class="p">())</span>
        
    <span class="c1"># MSE_scores.append(tree_model.fit(X, y).score(X, y))</span>
    <span class="n">cv_scores_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">cv_scores_mean</span><span class="p">)</span>
    <span class="n">cv_scores_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">cv_scores_std</span><span class="p">)</span>
    
    <span class="c1"># MSE_scores = np.array(MSE_scores)</span>
    <span class="k">return</span> <span class="n">cv_scores_mean</span><span class="p">,</span> <span class="n">cv_scores_std</span>

<span class="c1"># fitting trees</span>
<span class="n">sm_tree_ccp</span> <span class="o">=</span> <span class="n">alphas_dt</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span> <span class="c1">#it should run all alphas, but it takes too long</span>
<span class="n">sm_cv_scores_mean</span><span class="p">,</span> <span class="n">sm_cv_scores_std</span> <span class="o">=</span> <span class="n">run_cross_validation_on_trees</span><span class="p">(</span><span class="n">XX</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">sm_tree_ccp</span><span class="p">)</span>
<span class="n">sm_cv_scores_mean</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([1.32904969, 1.32904969, 1.32904969, 1.32904969, 1.32904969,
       1.32904969, 1.32904969, 1.32904969, 1.32904969, 1.32904969])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cp_table</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">alphas_dt</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;cp&quot;</span><span class="p">),</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">sm_cv_scores_mean</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;MSE&quot;</span><span class="p">),</span>
              <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">sm_cv_scores_std</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;xstd&quot;</span><span class="p">)])</span><span class="o">.</span><span class="n">T</span>
<span class="n">cp_table</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>cp</th>
      <th>MSE</th>
      <th>xstd</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.000000e+00</td>
      <td>1.32905</td>
      <td>0.035923</td>
    </tr>
    <tr>
      <th>1</th>
      <td>7.067264e-19</td>
      <td>1.32905</td>
      <td>0.035923</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.060090e-18</td>
      <td>1.32905</td>
      <td>0.035923</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.413453e-18</td>
      <td>1.32905</td>
      <td>0.035923</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.413453e-18</td>
      <td>1.32905</td>
      <td>0.035923</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>1725</th>
      <td>1.361723e-02</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1726</th>
      <td>1.645619e-02</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1727</th>
      <td>3.639536e-02</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1728</th>
      <td>9.906067e-02</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1729</th>
      <td>1.475424e-01</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>1730 rows × 3 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mse_gini</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">alphas_dt</span><span class="p">:</span>
    <span class="n">dtree</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span> <span class="n">ccp_alpha</span><span class="o">=</span><span class="n">i</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">dtree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">dtree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
    <span class="n">mse_gini</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">d2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;acc_gini&#39;</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">mse_gini</span><span class="p">),</span><span class="s1">&#39;ccp_alphas&#39;</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">alphas_dt</span><span class="p">)})</span>

<span class="c1">#plt.style.context(&quot;dark_background&quot;)</span>

<span class="c1"># visualizing changes in parameters</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">facecolor</span> <span class="o">=</span> <span class="s2">&quot;white&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="s1">&#39;ccp_alphas&#39;</span><span class="p">,</span><span class="s1">&#39;acc_gini&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">d2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="c1">#plt.gca().invert_xaxis()</span>


<span class="c1">#plt.xticks(np.arange(0, 0.15, step=0.01))  # Set label locations.</span>
<span class="c1">#plt.yticks(np.arange(0.5, 1.5, step=0.1))  # Set label locations.</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">labelrotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">labelrotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;ccp_alphas&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x1825da220d0&gt;
</pre></div>
</div>
<img alt="../_images/2_introduction_to_Machine_Learning_82_1.png" src="../_images/2_introduction_to_Machine_Learning_82_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mse_dt</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">mse_gini</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;mse&quot;</span><span class="p">)</span>
<span class="n">filter_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span> <span class="p">[</span><span class="n">alphas_dt</span><span class="p">,</span> <span class="n">mse_dt</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
</div>
<p>The following code retrieves the optimal parameter and prunes the tree. Here, instead of choosing the parameter that minimizes the mean-squared-error, we’re following another common heuristic: we will choose the most regularized model whose error is within one standard error of the minimum error.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">best_max_depth</span> <span class="o">=</span> <span class="n">d1</span><span class="p">[</span><span class="n">d1</span><span class="p">[</span><span class="s2">&quot;acc_gini&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">d1</span><span class="p">[</span><span class="s2">&quot;acc_gini&quot;</span><span class="p">])]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">best_ccp</span> <span class="o">=</span> <span class="n">filter_df</span><span class="p">[</span><span class="n">filter_df</span><span class="p">[</span><span class="s2">&quot;mse&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">filter_df</span><span class="p">[</span><span class="s2">&quot;mse&quot;</span><span class="p">])</span> <span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Prune the tree</span>
<span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">ccp_alpha</span><span class="o">=</span> <span class="n">best_ccp</span> <span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span> <span class="n">best_max_depth</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tree1</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Plotting the pruned tree. See also the package <a class="reference external" href="http://www.milbo.org/rpart-plot/prp.pdf">rpart.plot</a> for more advanced plotting capabilities.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">tree</span><span class="o">.</span><span class="n">plot_tree</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rounded</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Text(0.6764705882352942, 0.9, &#39;X[22] &lt;= 3.5\nsquared_error = 0.955\nsamples = 20108\nvalue = 11.816&#39;),
 Text(0.47058823529411764, 0.7, &#39;X[1] &lt;= 2436.5\nsquared_error = 0.77\nsamples = 19388\nvalue = 11.887&#39;),
 Text(0.23529411764705882, 0.5, &#39;X[3] &lt;= 1.5\nsquared_error = 0.643\nsamples = 13926\nvalue = 11.687&#39;),
 Text(0.11764705882352941, 0.3, &#39;X[19] &lt;= 1.5\nsquared_error = 0.713\nsamples = 5053\nvalue = 11.387&#39;),
 Text(0.058823529411764705, 0.1, &#39;squared_error = 0.622\nsamples = 2644\nvalue = 11.544&#39;),
 Text(0.17647058823529413, 0.1, &#39;squared_error = 0.755\nsamples = 2409\nvalue = 11.214&#39;),
 Text(0.35294117647058826, 0.3, &#39;X[1] &lt;= 1691.5\nsquared_error = 0.523\nsamples = 8873\nvalue = 11.858&#39;),
 Text(0.29411764705882354, 0.1, &#39;squared_error = 0.496\nsamples = 3243\nvalue = 11.687&#39;),
 Text(0.4117647058823529, 0.1, &#39;squared_error = 0.512\nsamples = 5630\nvalue = 11.957&#39;),
 Text(0.7058823529411765, 0.5, &#39;X[3] &lt;= 2.5\nsquared_error = 0.729\nsamples = 5462\nvalue = 12.398&#39;),
 Text(0.5882352941176471, 0.3, &#39;X[3] &lt;= 1.5\nsquared_error = 0.665\nsamples = 2848\nvalue = 12.152&#39;),
 Text(0.5294117647058824, 0.1, &#39;squared_error = 1.095\nsamples = 340\nvalue = 11.594&#39;),
 Text(0.6470588235294118, 0.1, &#39;squared_error = 0.559\nsamples = 2508\nvalue = 12.228&#39;),
 Text(0.8235294117647058, 0.3, &#39;X[1] &lt;= 3999.0\nsquared_error = 0.662\nsamples = 2614\nvalue = 12.665&#39;),
 Text(0.7647058823529411, 0.1, &#39;squared_error = 0.561\nsamples = 1666\nvalue = 12.497&#39;),
 Text(0.8823529411764706, 0.1, &#39;squared_error = 0.704\nsamples = 948\nvalue = 12.96&#39;),
 Text(0.8823529411764706, 0.7, &#39;X[12] &lt;= 1.5\nsquared_error = 2.138\nsamples = 720\nvalue = 9.901&#39;),
 Text(0.8235294117647058, 0.5, &#39;squared_error = 2.31\nsamples = 429\nvalue = 9.423&#39;),
 Text(0.9411764705882353, 0.5, &#39;squared_error = 1.052\nsamples = 291\nvalue = 10.605&#39;)]
</pre></div>
</div>
<img alt="../_images/2_introduction_to_Machine_Learning_87_1.png" src="../_images/2_introduction_to_Machine_Learning_87_1.png" />
</div>
</div>
<p>Finally, here’s how to extract predictions and mse estimates from the pruned tree.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Retrieve predictions from pruned tre</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">dt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>

<span class="c1"># Compute mse for pruned tree (using cross-validated predictions)</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tree MSE estimate:&quot;</span><span class="p">,</span> <span class="n">mse</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tree MSE estimate: 0.656562830536762
</pre></div>
</div>
</div>
</div>
<p>It’s often said that trees are “interpretable.” To some extent, that’s true – we can look at the tree and clearly visualize the mapping from inputs to prediction. This can be important in settings in which conveying how one got to a prediction is important. For example, if a decision tree were to be used for credit scoring, it would be easy to explain to a client how their credit was scored.</p>
<p>Beyond that, however, there are several reasons for not interpreting the obtained decision tree further. First, even though a tree may have used a particular variable for a split, that does not mean that it’s indeed an important variable: if two covariates are highly correlated, the tree may split on one variable but not the other, and there’s no guarantee which variables are relevant in the underlying data-generating process.</p>
<p>Similar to what we did for Lasso above, we can estimate the average value of each covariate per leaf. Although results are noisier here because there are many leaves, we see somewhat similar trends in that houses with higher predictions are also correlated with more bedrooms, bathrooms and room sizes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">Series</span>
<span class="kn">from</span> <span class="nn">simple_colors</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="nn">smf</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span>

<span class="c1"># Number of leaves should equal the number of distinct prediction values.</span>
<span class="c1"># This should be okay for most applications, but if an exact answer is needed use</span>
<span class="c1"># predict.rpart.leaves from package treeCluster</span>
<span class="n">num_leaves</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>

<span class="c1"># Leaf membership, ordered by increasing prediction value</span>
<span class="n">categ</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">categories</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)))</span>
<span class="n">leaf</span> <span class="o">=</span> <span class="n">categ</span><span class="o">.</span><span class="n">rename_categories</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">categ</span><span class="o">.</span><span class="n">categories</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># Looping over covariates</span>
<span class="n">data1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span> <span class="n">covariates</span><span class="p">)</span>
<span class="n">data1</span><span class="p">[</span><span class="s2">&quot;leaf&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">leaf</span>

<span class="k">for</span> <span class="n">var_name</span> <span class="ow">in</span> <span class="n">covariates</span><span class="p">:</span>
    <span class="c1"># Coefficients on linear regression of covariate on leaf </span>
    <span class="c1">#  are the average covariate value in each leaf.</span>
    <span class="c1"># covariate ~ leaf.1 + ... + leaf.L </span>
    <span class="n">form2</span> <span class="o">=</span> <span class="n">var_name</span> <span class="o">+</span> <span class="s2">&quot; ~ &quot;</span> <span class="o">+</span> <span class="s2">&quot;0&quot;</span> <span class="o">+</span> <span class="s2">&quot;+&quot;</span> <span class="o">+</span> <span class="s2">&quot;leaf&quot;</span>
    
    <span class="c1"># Heteroskedasticity-robust standard errors</span>
    <span class="n">ols</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="n">form2</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">cov_type</span> <span class="o">=</span> <span class="s1">&#39;HC2&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">summary2</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">red</span><span class="p">(</span><span class="n">var_name</span><span class="p">,</span> <span class="s1">&#39;bold&#39;</span><span class="p">),</span><span class="n">ols</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Red">LOT</span>                leaf[1]        leaf[2]       leaf[3]       leaf[4]  \
Coef.     62147.992348  156095.016394  37918.059380  37963.609724   
Std.Err.   9679.715527   18439.782886   3374.904533   3040.919277   

               leaf[5]       leaf[6]       leaf[7]       leaf[8]  \
Coef.     57460.902664  35800.336601  47309.306565  52027.215145   
Std.Err.  11958.583936   2572.586333   2453.410031   3692.568525   

               leaf[9]      leaf[10]  
Coef.     49146.276086  75986.183034  
Std.Err.   4299.266616   7573.937525   

<span class=" -Color -Color-Bold -Color-Bold-Red">UNITSF</span>               leaf[1]      leaf[2]      leaf[3]      leaf[4]      leaf[5]  \
Coef.     1497.891661  1888.676869  1579.970803  1537.358397  5201.335526   
Std.Err.   111.922106   130.153136    18.687591    15.776454   385.749500   

              leaf[6]      leaf[7]      leaf[8]      leaf[9]     leaf[10]  
Coef.     1360.202752  2075.217669  3612.324569  3129.419532  7927.140777  
Std.Err.     5.905033     5.117509    72.067705    14.816769   210.685167   

<span class=" -Color -Color-Bold -Color-Bold-Red">BUILT</span>               leaf[1]      leaf[2]      leaf[3]      leaf[4]      leaf[5]  \
Coef.     1981.613260  1987.591837  1948.011091  1957.886364  1952.697368   
Std.Err.     1.037346     1.085556     0.603229     0.660465     2.026017   

              leaf[6]      leaf[7]      leaf[8]      leaf[9]     leaf[10]  
Coef.     1977.029689  1979.056555  1982.184950  1987.643741  1988.274272  
Std.Err.     0.600677     0.469025     0.667451     0.754772     1.098346   

<span class=" -Color -Color-Bold -Color-Bold-Red">BATHS</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.558011  2.013605  0.987061  0.999091  0.993421  2.040550   
Std.Err.  0.039425  0.033421  0.003437  0.000909  0.006579  0.005408   

           leaf[7]       leaf[8]   leaf[9]  leaf[10]  
Coef.     2.143530  2.000000e+00  3.145805  3.674757  
Std.Err.  0.008028  1.337784e-16  0.014074  0.038914   

<span class=" -Color -Color-Bold -Color-Bold-Red">BEDRMS</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     2.502762  3.102041  2.705176  2.770909  3.171053  2.974656   
Std.Err.  0.041180  0.050838  0.022705  0.021643  0.072172  0.016488   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     3.337618  3.617407  4.063274  4.495146  
Std.Err.  0.013899  0.020535  0.026156  0.042161   

<span class=" -Color -Color-Bold -Color-Bold-Red">DINING</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.232044  0.489796  0.488909  0.516364  0.723684  0.472122   
Std.Err.  0.034266  0.042483  0.015702  0.015400  0.037569  0.013937   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.724936  0.853128  0.927098  1.004854  
Std.Err.  0.010190  0.013337  0.015150  0.017182   

<span class=" -Color -Color-Bold -Color-Bold-Red">METRO</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     6.309392  6.734694  4.272643  5.008182  4.361842  5.503983   
Std.Err.  0.139132  0.098600  0.090002  0.083896  0.239370  0.068088   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     5.713796  6.013599  6.152682  6.361650  
Std.Err.  0.049852  0.065963  0.075993  0.090116   

<span class=" -Color -Color-Bold -Color-Bold-Red">CRACKS</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.911602  1.938776  1.910351  1.944545  1.927632  1.946416   
Std.Err.  0.021159  0.019841  0.008689  0.006904  0.021085  0.006062   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     1.952871  1.962829  1.953232  1.970874  
Std.Err.  0.004387  0.005699  0.007836  0.008295   

<span class=" -Color -Color-Bold -Color-Bold-Red">REGION</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     2.867403  2.897959  2.459335  2.503636  2.519737  2.876901   
Std.Err.  0.046622  0.047030  0.019717  0.023699  0.061191  0.017219   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     2.760069  2.694470  2.858322  2.781553  
Std.Err.  0.013108  0.020404  0.023131  0.032994   

<span class=" -Color -Color-Bold -Color-Bold-Red">METRO3</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.878453  1.952381  1.641405  1.804545  1.598684  2.031861   
Std.Err.  0.024355  0.017625  0.031286  0.034764  0.063490  0.040993   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     2.054841  2.128740  2.074278  2.247573  
Std.Err.  0.030523  0.045094  0.048274  0.078698   

<span class=" -Color -Color-Bold -Color-Bold-Red">PHONE</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.541436  0.564626  0.707948  0.750909  0.592105  0.743664   
Std.Err.  0.143505  0.160974  0.047290  0.043139  0.145920  0.038788   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.682091  0.773345  0.669876  0.631068  
Std.Err.  0.033012  0.041261  0.060200  0.085295   

<span class=" -Color -Color-Bold -Color-Bold-Red">KITCHEN</span>                leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.000000e+00  1.020408  1.015712  1.007273  1.013158  1.006517   
Std.Err.  6.620091e-17  0.011702  0.003782  0.002563  0.009273  0.002166   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     1.001714  1.000907  1.001376  1.002427  
Std.Err.  0.000856  0.000907  0.001376  0.002427   

<span class=" -Color -Color-Bold -Color-Bold-Red">MOBILTYP</span>                leaf[1]       leaf[2]       leaf[3]       leaf[4]  \
Coef.     1.000000e+00  2.000000e+00 -1.000000e+00 -1.000000e+00   
Std.Err.  6.620091e-17  3.675308e-17  1.485765e-16  1.339588e-16   

               leaf[5]       leaf[6]       leaf[7]       leaf[8]  \
Coef.    -1.000000e+00 -1.000000e+00 -1.000000e+00 -1.000000e+00   
Std.Err.  1.806973e-17  1.314993e-16  1.333156e-16  6.688819e-17   

               leaf[9]      leaf[10]  
Coef.    -1.000000e+00 -1.000000e+00  
Std.Err.  1.318536e-16  1.095265e-17   

<span class=" -Color -Color-Bold -Color-Bold-Red">WINTEROVEN</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.917127  1.850340  1.931608  1.937273  1.921053  1.940623   
Std.Err.  0.051305  0.096221  0.017023  0.022029  0.066353  0.019016   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     1.949871  1.978241  1.986245  1.917476  
Std.Err.  0.013986  0.012938  0.012454  0.040895   

<span class=" -Color -Color-Bold -Color-Bold-Red">WINTERKESP</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.895028  1.809524  1.935305  1.926364  1.927632  1.934830   
Std.Err.  0.052268  0.097175  0.016936  0.022223  0.066074  0.019112   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     1.938303  1.970082  1.977992  1.924757  
Std.Err.  0.014143  0.013207  0.012887  0.040712   

<span class=" -Color -Color-Bold -Color-Bold-Red">WINTERELSP</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.723757  1.700680  1.743068  1.786364  1.723684  1.799421   
Std.Err.  0.057625  0.099113  0.020185  0.024180  0.072426  0.020912   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     1.801200  1.831369  1.828061  1.776699  
Std.Err.  0.015604  0.016592  0.018235  0.043706   

<span class=" -Color -Color-Bold -Color-Bold-Red">WINTERWOOD</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.944751  1.857143  1.971349  1.946364  1.934211  1.949312   
Std.Err.  0.049999  0.096050  0.016018  0.021863  0.065789  0.018868   

           leaf[7]   leaf[8]  leaf[9]  leaf[10]  
Coef.     1.952442  1.978241  1.98762  1.929612  
Std.Err.  0.013950  0.012938  0.01238  0.040588   

<span class=" -Color -Color-Bold -Color-Bold-Red">WINTERNONE</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.248619  1.081633  1.262477  1.130000  1.157895  1.125996   
Std.Err.  0.056989  0.094342  0.020246  0.023124  0.069295  0.020023   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     1.128963  1.148685  1.158184  1.101942  
Std.Err.  0.014945  0.016216  0.017884  0.041364   

<span class=" -Color -Color-Bold -Color-Bold-Red">NEWC</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.    -8.834254 -8.523810 -8.972274 -8.936364 -8.802632 -8.753802   
Std.Err.  0.095160  0.176246  0.015993  0.023987  0.113194  0.041715   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.    -8.558698 -8.546691 -8.284732 -8.344660  
Std.Err.  0.042522  0.062666  0.095642  0.122066   

<span class=" -Color -Color-Bold -Color-Bold-Red">DISH</span>            leaf[1]   leaf[2]       leaf[3]       leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.591160  1.204082  2.000000e+00  1.000000e+00  1.328947  1.126720   
Std.Err.  0.036643  0.033355  2.971530e-16  1.339588e-16  0.038234  0.008955   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     1.080977  1.047144  1.011004  1.004854  
Std.Err.  0.005648  0.006385  0.003872  0.003428   

<span class=" -Color -Color-Bold -Color-Bold-Red">WASH</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.082873  1.047619  1.073937  1.033636  1.019737  1.018827   
Std.Err.  0.020549  0.017625  0.007959  0.005438  0.011319  0.003659   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     1.003856  1.003626  1.004127  1.004854  
Std.Err.  0.001283  0.001811  0.002379  0.003428   

<span class=" -Color -Color-Bold -Color-Bold-Red">DRY</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.127072  1.047619  1.120148  1.037273  1.046053  1.028240   
Std.Err.  0.024824  0.017625  0.009889  0.005714  0.017057  0.004459   

           leaf[7]   leaf[8]   leaf[9]      leaf[10]  
Coef.     1.008997  1.005440  1.005502  1.000000e+00  
Std.Err.  0.001955  0.002216  0.002745  1.095265e-17   

<span class=" -Color -Color-Bold -Color-Bold-Red">NUNIT2</span>                leaf[1]       leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     4.000000e+00  4.000000e+00  1.097967  1.226364  1.026316  1.215062   
Std.Err.  2.648036e-16  7.350617e-17  0.012597  0.017651  0.016026  0.014508   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     1.074122  1.031732  1.038514  1.004854  
Std.Err.  0.006473  0.006407  0.009018  0.003428   

<span class=" -Color -Color-Bold -Color-Bold-Red">BURNER</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]       leaf[5]   leaf[6]  \
Coef.    -5.790055 -5.945578 -5.862292 -5.979091 -6.000000e+00 -5.977552   
Std.Err.  0.093039  0.054422  0.031363  0.012084  7.227893e-17  0.011229   

           leaf[7]   leaf[8]   leaf[9]      leaf[10]  
Coef.    -5.976864 -5.972801 -5.979367 -6.000000e+00  
Std.Err.  0.008748  0.013611  0.014612  8.762122e-17   

<span class=" -Color -Color-Bold -Color-Bold-Red">COOK</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]       leaf[5]   leaf[6]  \
Coef.     1.027624  1.006803  1.017560  1.002727  1.000000e+00  1.002896   
Std.Err.  0.012216  0.006803  0.003995  0.001573  1.806973e-17  0.001447   

           leaf[7]   leaf[8]   leaf[9]      leaf[10]  
Coef.     1.002999  1.003626  1.002751  1.000000e+00  
Std.Err.  0.001132  0.001811  0.001944  1.095265e-17   

<span class=" -Color -Color-Bold -Color-Bold-Red">OVEN</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]       leaf[5]  leaf[6]  \
Coef.    -5.883978 -5.952381 -5.893715 -5.987273 -6.000000e+00 -5.98407   
Std.Err.  0.066612  0.047619  0.026426  0.008995  7.227893e-17  0.00921   

           leaf[7]   leaf[8]   leaf[9]      leaf[10]  
Coef.    -5.985004 -5.987307 -5.990371 -6.000000e+00  
Std.Err.  0.006701  0.008971  0.009629  8.762122e-17   

<span class=" -Color -Color-Bold -Color-Bold-Red">REFR</span>                leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.000000e+00  1.013605  1.006470  1.002727  1.013158  1.003621   
Std.Err.  6.620091e-17  0.009587  0.002438  0.001573  0.009273  0.001617   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     1.001285  1.000907  1.001376  1.002427  
Std.Err.  0.000742  0.000907  0.001376  0.002427   

<span class=" -Color -Color-Bold -Color-Bold-Red">DENS</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.027624  0.183673  0.104436  0.098182  0.151316  0.099203   
Std.Err.  0.012216  0.032046  0.009393  0.009068  0.029163  0.008367   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.209940  0.277425  0.350757  0.509709  
Std.Err.  0.008877  0.013848  0.019542  0.029858   

<span class=" -Color -Color-Bold -Color-Bold-Red">FAMRM</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.011050  0.074830  0.085028  0.173636  0.217105  0.132513   
Std.Err.  0.007792  0.021776  0.008584  0.011712  0.036054  0.009466   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.272065  0.461469  0.500688  0.764563  
Std.Err.  0.009737  0.016530  0.022013  0.039197   

<span class=" -Color -Color-Bold -Color-Bold-Red">HALFB</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.165746  0.054422  0.303142  0.595455  0.763158  0.178856   
Std.Err.  0.059419  0.018774  0.014578  0.019056  0.051753  0.012225   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.446872  0.818676  0.566713  0.973301  
Std.Err.  0.011743  0.016320  0.021079  0.033909   

<span class=" -Color -Color-Bold -Color-Bold-Red">KITCH</span>                leaf[1]       leaf[2]   leaf[3]   leaf[4]       leaf[5]  \
Coef.     1.000000e+00  1.000000e+00  1.000924  1.001818  1.000000e+00   
Std.Err.  6.620091e-17  1.837654e-17  0.000924  0.001285  1.806973e-17   

           leaf[6]   leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     1.009413  1.011997  1.016319  1.028886  1.082524  
Std.Err.  0.002599  0.002334  0.003817  0.006514  0.013573   

<span class=" -Color -Color-Bold -Color-Bold-Red">LIVING</span>           leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]   leaf[7]  \
Coef.     0.98895  1.061224  1.000000  1.019091  1.052632  1.020999  1.056127   
Std.Err.  0.01105  0.022065  0.005231  0.005350  0.020429  0.005722  0.006322   

           leaf[8]   leaf[9]  leaf[10]  
Coef.     1.068903  1.148556  1.177184  
Std.Err.  0.010913  0.017409  0.026405   

<span class=" -Color -Color-Bold -Color-Bold-Red">OTHFN</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.016575  0.034014  0.055453  0.067273  0.144737  0.065170   
Std.Err.  0.009516  0.015002  0.007882  0.008581  0.032882  0.007102   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.117823  0.189483  0.229711  0.434466  
Std.Err.  0.007815  0.013798  0.021174  0.039427   

<span class=" -Color -Color-Bold -Color-Bold-Red">RECRM</span>           leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]   leaf[7]  \
Coef.         0.0  0.013605  0.023105  0.040909  0.052632  0.030413  0.069837   
Std.Err.      0.0  0.009587  0.004569  0.005975  0.018172  0.004623  0.005482   

           leaf[8]   leaf[9]  leaf[10]  
Coef.     0.153218  0.210454  0.361650  
Std.Err.  0.011076  0.015742  0.026522   

<span class=" -Color -Color-Bold -Color-Bold-Red">CLIMB</span>                leaf[1]       leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     2.308571e+00  2.308571e+00  2.259694  2.216400  2.293383  2.247804   
Std.Err.  3.310046e-17  1.102593e-16  0.013614  0.018583  0.015188  0.016434   

           leaf[7]   leaf[8]   leaf[9]      leaf[10]  
Coef.     2.286082  2.300894  2.301049  2.308571e+00  
Std.Err.  0.004575  0.003673  0.014276  1.971477e-16   

<span class=" -Color -Color-Bold -Color-Bold-Red">ELEV</span>                leaf[1]       leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.    -6.000000e+00 -6.000000e+00 -5.706100 -5.404545 -5.947368 -5.564084   
Std.Err.  1.986027e-16  2.940247e-16  0.042939  0.059522  0.052632  0.046176   

           leaf[7]   leaf[8]   leaf[9]      leaf[10]  
Coef.    -5.895887 -5.947416 -5.921596 -6.000000e+00  
Std.Err.  0.017751  0.018565  0.027616  8.762122e-17   

<span class=" -Color -Color-Bold -Color-Bold-Red">DIRAC</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.507923  1.469052  1.475979  1.452383  1.468251  1.452593   
Std.Err.  0.012786  0.028698  0.008995  0.010210  0.025063  0.009424   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     1.406490  1.317666  1.283246  1.246568  
Std.Err.  0.008602  0.015906  0.021207  0.028282   

<span class=" -Color -Color-Bold -Color-Bold-Red">PORCH</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]  leaf[6]   leaf[7]  \
Coef.     1.149171  1.088435  1.127542  1.077273  1.046053  1.06155  1.048843   
Std.Err.  0.026554  0.023498  0.010146  0.008055  0.017057  0.00647  0.004462   

           leaf[8]   leaf[9]  leaf[10]  
Coef.     1.043518  1.024759  1.021845  
Std.Err.  0.006146  0.005767  0.007210   

<span class=" -Color -Color-Bold -Color-Bold-Red">AIRSYS</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     1.397790  1.088435  1.401109  1.201818  1.309211  1.068067   
Std.Err.  0.036481  0.023498  0.014907  0.012107  0.037611  0.006780   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     1.064696  1.055304  1.042641  1.038835  
Std.Err.  0.005093  0.006885  0.007499  0.009530   

<span class=" -Color -Color-Bold -Color-Bold-Red">WELL</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.    -0.696133 -0.564626 -0.866913 -0.908182 -0.960526 -0.908762   
Std.Err.  0.054364  0.070323  0.015495  0.012724  0.022639  0.011511   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.    -0.871037 -0.844062 -0.887208 -0.808252  
Std.Err.  0.010464  0.016306  0.017123  0.029946   

<span class=" -Color -Color-Bold -Color-Bold-Red">WELDUS</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  leaf[7]  \
Coef.     4.469613  4.156463  4.720887  4.799091  4.815789  4.816799  4.73479   
Std.Err.  0.098342  0.136075  0.031023  0.026495  0.068228  0.022334  0.02060   

           leaf[8]   leaf[9]  leaf[10]  
Coef.     4.690843  4.768913  4.631068  
Std.Err.  0.032063  0.034306  0.056676   

<span class=" -Color -Color-Bold -Color-Bold-Red">STEAM</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.011050  0.081633 -0.054529 -0.079091  0.105263 -0.109341   
Std.Err.  0.105697  0.119215  0.042390  0.041720  0.117766  0.036897   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.008997 -0.012693  0.145805  0.031553  
Std.Err.  0.029344  0.042464  0.054061  0.070205   

<span class=" -Color -Color-Bold -Color-Bold-Red">OARSYS</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.    -1.209945  1.204082 -1.234750  0.340000 -0.500000  1.406951   
Std.Err.  0.290425  0.187171  0.118709  0.096368  0.299733  0.054194   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     1.389032  1.395286  1.314993  1.293689  
Std.Err.  0.040679  0.055021  0.059886  0.076132   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise1</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.474392  0.485675  0.491980  0.493226  0.529165  0.496986   
Std.Err.  0.020866  0.024247  0.008563  0.008569  0.023549  0.007735   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.506801  0.503722  0.485235  0.532615  
Std.Err.  0.005969  0.008567  0.010803  0.013891   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise2</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.499365  0.477223  0.491978  0.493990  0.505021  0.509057   
Std.Err.  0.020791  0.024888  0.008665  0.008633  0.022857  0.007600   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.494212  0.517874  0.508010  0.531369  
Std.Err.  0.005963  0.008677  0.010643  0.014090   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise3</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.485311  0.512201  0.504588  0.498832  0.496952  0.496812   
Std.Err.  0.021615  0.022380  0.008794  0.008724  0.023092  0.007969   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.502202  0.506226  0.501597  0.484022  
Std.Err.  0.005953  0.008489  0.010746  0.014592   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise4</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.504756  0.499641  0.479744  0.499257  0.471399  0.495866   
Std.Err.  0.021311  0.023497  0.008583  0.008814  0.024742  0.007789   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.493827  0.498341  0.496005  0.495855  
Std.Err.  0.005917  0.008545  0.010453  0.014358   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise5</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.491256  0.513972  0.493933  0.496906  0.466841  0.506005   
Std.Err.  0.021763  0.024659  0.008503  0.008690  0.022607  0.007937   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.497487  0.501580  0.506217  0.523169  
Std.Err.  0.006008  0.008698  0.010760  0.014213   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise6</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.491718  0.469613  0.496021  0.498028  0.553031  0.500049   
Std.Err.  0.020967  0.024782  0.008862  0.008590  0.022138  0.007662   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.502105  0.514641  0.498158  0.520025  
Std.Err.  0.005976  0.008848  0.010962  0.014162   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise7</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.488858  0.461354  0.501237  0.494184  0.507918  0.498332   
Std.Err.  0.021060  0.025089  0.008807  0.008804  0.023145  0.007686   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.501601  0.499010  0.494277  0.491689  
Std.Err.  0.005946  0.008495  0.010849  0.014116   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise8</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.505197  0.495414  0.509971  0.483280  0.471419  0.500312   
Std.Err.  0.021648  0.025133  0.008905  0.008613  0.022984  0.007681   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.498416  0.505052  0.513244  0.498875  
Std.Err.  0.005990  0.008730  0.010633  0.014676   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise9</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.503774  0.505684  0.509193  0.503724  0.495117  0.497724   
Std.Err.  0.021368  0.022083  0.008729  0.008895  0.024026  0.007790   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.502499  0.497624  0.482065  0.483302  
Std.Err.  0.005911  0.008694  0.010711  0.013752   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise10</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.493354  0.534488  0.496621  0.501796  0.480764  0.514013   
Std.Err.  0.022280  0.023470  0.008870  0.008602  0.023038  0.007875   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.486570  0.501135  0.486868  0.478504  
Std.Err.  0.006023  0.008704  0.010525  0.014174   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise11</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.478240  0.476699  0.492790  0.507017  0.479131  0.489874   
Std.Err.  0.021454  0.024575  0.009093  0.008602  0.023431  0.007664   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.507827  0.484728  0.482073  0.519721  
Std.Err.  0.005912  0.008692  0.010689  0.014121   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise12</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.519023  0.493302  0.495664  0.507416  0.492513  0.497138   
Std.Err.  0.021533  0.025782  0.008673  0.008740  0.023878  0.007687   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.501768  0.494392  0.505834  0.521542  
Std.Err.  0.005992  0.008870  0.011018  0.013904   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise13</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.446577  0.488432  0.512726  0.503292  0.505169  0.501416   
Std.Err.  0.021562  0.024128  0.008722  0.008580  0.022691  0.007791   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.500943  0.490835  0.475812  0.469562  
Std.Err.  0.006008  0.008713  0.010934  0.014113   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise14</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.501246  0.502539  0.481349  0.500402  0.504592  0.488694   
Std.Err.  0.021904  0.024281  0.008846  0.008779  0.023489  0.007735   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.496610  0.505539  0.490338  0.508695  
Std.Err.  0.006006  0.008917  0.010421  0.014288   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise15</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.469737  0.494470  0.487038  0.509433  0.506092  0.504696   
Std.Err.  0.021478  0.021279  0.008892  0.008622  0.023361  0.007789   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.513881  0.490818  0.507790  0.507370  
Std.Err.  0.005998  0.008588  0.011194  0.013814   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise16</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.524719  0.455748  0.502285  0.489549  0.537517  0.491817   
Std.Err.  0.021479  0.024267  0.008788  0.008720  0.023624  0.007640   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.501098  0.502460  0.513953  0.480556  
Std.Err.  0.005965  0.008707  0.011100  0.013685   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise17</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.499257  0.538940  0.505553  0.489215  0.484954  0.500511   
Std.Err.  0.021145  0.023288  0.008749  0.008696  0.024676  0.007744   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.496586  0.495329  0.494849  0.488306  
Std.Err.  0.006046  0.008622  0.010640  0.014218   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise18</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.487828  0.501180  0.479001  0.500020  0.554518  0.489017   
Std.Err.  0.021457  0.025033  0.008648  0.008765  0.022651  0.007716   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.494694  0.500506  0.499138  0.501016  
Std.Err.  0.006032  0.008678  0.010996  0.014382   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise19</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.492477  0.510517  0.497361  0.498034  0.496796  0.497254   
Std.Err.  0.021991  0.022717  0.008947  0.008679  0.022420  0.007749   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.506206  0.489874  0.487258  0.513952  
Std.Err.  0.006005  0.008683  0.010829  0.014231   

<span class=" -Color -Color-Bold -Color-Bold-Red">noise20</span>            leaf[1]   leaf[2]   leaf[3]   leaf[4]   leaf[5]   leaf[6]  \
Coef.     0.508315  0.525029  0.492421  0.500168  0.514199  0.515827   
Std.Err.  0.021264  0.023527  0.008743  0.008766  0.023021  0.007844   

           leaf[7]   leaf[8]   leaf[9]  leaf[10]  
Coef.     0.505397  0.493414  0.493779  0.501220  
Std.Err.  0.005925  0.008682  0.010648  0.014518   
</pre></div>
</div>
</div>
</div>
<p>Finally, as we did in the linear model case, we can use the same code for an annotated version of the same information. Again, we ordered the rows in decreasing order based on an estimate of the relative variance “explained” by leaf membership: <span class="math notranslate nohighlight">\(Var(E[X_i|L_i]) / Var(X_i)\)</span>, where <span class="math notranslate nohighlight">\(L_i\)</span> represents the leaf.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>

<span class="k">for</span> <span class="n">var_name</span> <span class="ow">in</span> <span class="n">covariates</span><span class="p">:</span>
    <span class="c1"># Looping over covariate names</span>
    <span class="c1"># Compute average covariate value per ranking (with correct standard errors)</span>
    <span class="n">form2</span> <span class="o">=</span> <span class="n">var_name</span> <span class="o">+</span> <span class="s2">&quot; ~ &quot;</span> <span class="o">+</span> <span class="s2">&quot;0&quot;</span> <span class="o">+</span> <span class="s2">&quot;+&quot;</span> <span class="o">+</span> <span class="s2">&quot;leaf&quot;</span>
    <span class="n">ols</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="n">form2</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">cov_type</span> <span class="o">=</span> <span class="s1">&#39;HC2&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">summary2</span><span class="p">()</span><span class="o">.</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>
    
    <span class="c1"># Retrieve results</span>
    <span class="n">toget_index</span> <span class="o">=</span> <span class="n">ols</span><span class="p">[</span><span class="s2">&quot;Coef.&quot;</span><span class="p">]</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">toget_index</span><span class="o">.</span><span class="n">index</span>
    <span class="n">cova1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">var_name</span><span class="p">,</span><span class="n">num_leaves</span><span class="p">),</span> <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;covariate&quot;</span><span class="p">)</span>
    <span class="n">avg</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">ols</span><span class="p">[</span><span class="s2">&quot;Coef.&quot;</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;avg&quot;</span><span class="p">)</span>
    <span class="n">stderr</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">ols</span><span class="p">[</span><span class="s2">&quot;Std.Err.&quot;</span><span class="p">],</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;stderr&quot;</span><span class="p">)</span>
    <span class="n">ranking</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">num_leaves</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;ranking&quot;</span><span class="p">)</span>
    <span class="n">scaling</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">((</span><span class="n">avg</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">avg</span><span class="p">))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">avg</span><span class="p">)),</span> <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;scaling&quot;</span><span class="p">)</span>
    <span class="n">data2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span> <span class="n">covariates</span><span class="p">)</span>
    <span class="n">variation1</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">avg</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">data2</span><span class="p">[</span><span class="n">var_name</span><span class="p">])</span>
    <span class="n">variation</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">variation1</span><span class="p">,</span> <span class="n">num_leaves</span><span class="p">),</span> <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;variation&quot;</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">avg</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;str&#39;</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="s2">&quot;(&quot;</span> <span class="o">+</span> <span class="nb">round</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;str&#39;</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;)&quot;</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;labels&quot;</span><span class="p">)</span>
    
    <span class="c1"># Tally up results</span>
    <span class="n">df1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">cova1</span><span class="p">,</span> <span class="n">avg</span><span class="p">,</span> <span class="n">stderr</span><span class="p">,</span> <span class="n">ranking</span><span class="p">,</span> <span class="n">scaling</span><span class="p">,</span> <span class="n">variation</span><span class="p">,</span> <span class="n">labels</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">df1</span><span class="p">)</span>

<span class="c1"># a small optional trick to ensure heatmap will be in decreasing order of &#39;variation&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;variation&quot;</span><span class="p">,</span> <span class="s2">&quot;covariate&quot;</span><span class="p">],</span> <span class="n">ascending</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">:(</span><span class="mi">8</span><span class="o">*</span><span class="n">num_leaves</span><span class="p">),</span> <span class="p">:]</span>
<span class="n">df1</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span><span class="n">index</span> <span class="o">=</span> <span class="s2">&quot;covariate&quot;</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="s2">&quot;ranking&quot;</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;scaling&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span>  <span class="n">df</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span><span class="n">index</span> <span class="o">=</span> <span class="s2">&quot;covariate&quot;</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="s2">&quot;ranking&quot;</span><span class="p">,</span> <span class="n">values</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="c1"># plot heatmap</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">df1</span><span class="p">,</span> 
                 <span class="n">annot</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
                 <span class="n">annot_kws</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span> <span class="s1">&#39;color&#39;</span><span class="p">:</span><span class="s2">&quot;k&quot;</span><span class="p">},</span>
                 <span class="n">fmt</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="p">,</span>
                 <span class="n">cmap</span> <span class="o">=</span> <span class="s2">&quot;terrain_r&quot;</span><span class="p">,</span>
                 <span class="n">linewidths</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">xticklabels</span> <span class="o">=</span> <span class="n">ranking</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">labelrotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">labelrotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Leaf (ordered by prediction, low to high)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Average covariate values within leaf&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span> <span class="n">fontweight</span> <span class="o">=</span> <span class="s2">&quot;bold&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Average covariate values within leaf&#39;)
</pre></div>
</div>
<img alt="../_images/2_introduction_to_Machine_Learning_94_1.png" src="../_images/2_introduction_to_Machine_Learning_94_1.png" />
</div>
</div>
</div>
<div class="section" id="forest">
<h3><span class="section-number">2.2.3. </span>Forest<a class="headerlink" href="#forest" title="Permalink to this headline">¶</a></h3>
<p>Forests are a type of <strong>ensemble</strong> estimators: they aggregate information about many decision trees to compute a new estimate that typically has much smaller variance.</p>
<p>At a high level, the process of fitting a (regression) forest consists of fitting many decision trees, each on a different subsample of the data. The forest prediction for a particular point <span class="math notranslate nohighlight">\(x\)</span> is the average of all tree predictions for that point.</p>
<p>One interesting aspect of forests and many other ensemble methods is that cross-validation can be built into the algorithm itself. Since each tree only uses a subset of the data, the remaining subset is effectively a test set for that tree. We call these observations <strong>out-of-bag</strong> (there were not in the “bag” of training observations). They can be used to evaluate the performance of that tree, and the average of out-of-bag evaluations is evidence of the performance of the forest itself.</p>
<p>For the example below, we’ll use the regression_forest function of the <code class="docutils literal notranslate"><span class="pre">R</span></code> package <code class="docutils literal notranslate"><span class="pre">grf</span></code>. The particular forest implementation in <code class="docutils literal notranslate"><span class="pre">grf</span></code> has interesting properties that are absent from most other packages. For example, trees are build using a certain sample-splitting scheme that ensures that predictions are approximately unbiased and normally distributed for large samples, which in turn allows us to compute valid confidence intervals around those predictions. We’ll have more to say about the importance of these features when we talk about causal estimates in future chapters. See also the grf website for more information.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fitting the forest</span>
<span class="c1"># We&#39;ll use few trees for speed here. </span>
<span class="c1"># In a practical application please use a higher number of trees.</span>
<span class="n">forest</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>

<span class="c1">#x_train, x_test, y_train, y_test = train_test_split(XX.to_numpy() , Y, test_size=.3)</span>
<span class="n">forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Retrieving forest predictions</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">forest</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>

<span class="c1"># Evaluation (out-of-bag mse)</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Forest MSE (out-of-bag):&quot;</span><span class="p">,</span> <span class="n">mse</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Forest MSE (out-of-bag): 0.6516070635940446
</pre></div>
</div>
</div>
</div>
<p>The fitted attribute <code class="docutils literal notranslate"><span class="pre">feature_importances_</span></code> computes the decrease in node impurity weighted by the probability of reaching that node. The node probability can be calculated by the number of samples that reach the node, divided by the total number of samples. The higher the value the more important the feature.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">sorted_idx</span> <span class="o">=</span> <span class="n">forest</span><span class="o">.</span><span class="n">feature_importances_</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[:</span><span class="mi">10</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">XX</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">sorted_idx</span><span class="p">],</span> <span class="n">forest</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">[</span><span class="n">sorted_idx</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">labelrotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">labelrotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Random Forest Feature Importance&quot;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">15</span><span class="p">,</span> <span class="n">fontweight</span> <span class="o">=</span> <span class="s2">&quot;bold&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Random Forest Feature Importance&#39;)
</pre></div>
</div>
<img alt="../_images/2_introduction_to_Machine_Learning_100_1.png" src="../_images/2_introduction_to_Machine_Learning_100_1.png" />
</div>
</div>
<p>All the caveats about interpretation that we mentioned above apply in a similar to forest output.</p>
</div>
</div>
<div class="section" id="further-reading">
<h2><span class="section-number">2.3. </span>Further reading<a class="headerlink" href="#further-reading" title="Permalink to this headline">¶</a></h2>
<p>In this tutorial we briefly reviewed some key concepts that we recur later in this tutorial. For readers who are entirely new to this field or interested in learning about it more depth, the first few chapters of the following textbook are an acccessible introduction:</p>
<p>James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). An introduction to statistical learning (Vol. 112, p. 18). New York: springer. Available for free at <a class="reference external" href="https://www.statlearning.com/">the authors’ website</a>.</p>
<p>Some of the discussion in the Lasso section in particular was drawn from <a class="reference external" href="https://www.aeaweb.org/articles?id=10.1257/jep.31.2.87">Mullainathan and Spiess (JEP, 2017)</a>, which contains a good discussion of the interpretability issues discussed here.</p>
<p>There has been a good deal of research on inference in high-dimensional models, Although we won’t be covering in depth it in this tutorial, we refer readers to <a class="reference external" href="http://www.mit.edu/~vchern/papers/JEP.pdf">Belloni, Chernozhukov and Hansen (JEP, 2014)</a>. Also check out the related <code class="docutils literal notranslate"><span class="pre">R</span></code> package <a class="reference external" href="https://cran.r-project.org/web/packages/hdm/hdm.pdf"><code class="docutils literal notranslate"><span class="pre">hdm</span></code></a>, developed by the same authors, along with Philipp Bach and Martin Spindler.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "d2cml-ai/mgtecon634_python",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="1_introduction_1.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">1. </span>Introduction</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="3_average_treatment_effect_1.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">3. </span>ATE I: Binary treatment</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Phd Susan Athey<br/>
    
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>